{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/notes/blob/main/MachineLearning/Recommenders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COYGNbvomdLn"
      },
      "source": [
        "# Recommenders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "cRJxEtXoOomo"
      },
      "source": [
        ">[Recommenders](#scrollTo=COYGNbvomdLn)\n",
        "\n",
        ">>[Mathematical Recommenders](#scrollTo=X7LG4JFYmfH9)\n",
        "\n",
        ">>>[Lift](#scrollTo=-NPdgLKNkwA2)\n",
        "\n",
        ">>>[Hacker News Formula](#scrollTo=bNo3EqUbmjRp)\n",
        "\n",
        ">>>[Reddit Formula](#scrollTo=OX9Eah-ppQoq)\n",
        "\n",
        ">>>[Google Page Rank](#scrollTo=cCDmUR3Fp0YC)\n",
        "\n",
        ">>>>[Markov Models](#scrollTo=bIU2zR8LtEfH)\n",
        "\n",
        ">>>>[Transition Probability Matrix](#scrollTo=UENDikH0tHHt)\n",
        "\n",
        ">>>>[Calculating Probabilities](#scrollTo=Cbn1ogLEtKl_)\n",
        "\n",
        ">>>>[Beta Posterior Mean](#scrollTo=-PvC0vn1xck2)\n",
        "\n",
        ">>>>[State Distribution](#scrollTo=pS0RySi_1Ecy)\n",
        "\n",
        ">>>>[PageRank](#scrollTo=Q640uQxg3_sJ)\n",
        "\n",
        ">>[Statistics](#scrollTo=Ivwe3u2lzHp3)\n",
        "\n",
        ">>>[Smoothing (Dampening)](#scrollTo=S34-RUx8zI_f)\n",
        "\n",
        ">>>[Explore-Exploit Dilemma](#scrollTo=gahD3CwG1XEv)\n",
        "\n",
        ">>>[Bayesian Method](#scrollTo=N4WrwUfO6MUp)\n",
        "\n",
        ">>[Collaborative Filtering](#scrollTo=biqlBm7295Kg)\n",
        "\n",
        ">>>[Sparsity](#scrollTo=VHSIzt7MLEya)\n",
        "\n",
        ">>>[Regression](#scrollTo=pmcJqzBwNcuy)\n",
        "\n",
        ">>>[User-User Collaborative Filtering](#scrollTo=XlrdEcfgxdmH)\n",
        "\n",
        ">>>>[Pearson Correlation Coefficient](#scrollTo=LJzwxoPl2ayu)\n",
        "\n",
        ">>>>[Cosine Similarity](#scrollTo=B2ZTwhaW4o-v)\n",
        "\n",
        ">>>>[Python Implementation](#scrollTo=7fXxflLq7YCc)\n",
        "\n",
        ">>>[Item-Item Collaborative Filtering](#scrollTo=RVJUmoIsVf11)\n",
        "\n",
        ">>>>[Python Implementation](#scrollTo=QSNW575oXYDB)\n",
        "\n",
        ">>>[Comparison](#scrollTo=-Hewxxx2C7sS)\n",
        "\n",
        ">>[Matrix Factorization](#scrollTo=LPgwQOSzPfEH)\n",
        "\n",
        ">>>[Sparse representation](#scrollTo=txxT6ymhSNL9)\n",
        "\n",
        ">>>[SVD](#scrollTo=tqVBGCMWStEn)\n",
        "\n",
        ">>>>[Dimensionality Reduction](#scrollTo=V3TKC_wIeg2E)\n",
        "\n",
        ">>>>[MF vs SVD](#scrollTo=xC-qr1Jrc0NZ)\n",
        "\n",
        ">>>[Training](#scrollTo=FYoCYIFskzXP)\n",
        "\n",
        ">>>>[Altering Least Squares](#scrollTo=ywYS29A_qfmu)\n",
        "\n",
        ">>>>[Bias Terms](#scrollTo=CBe12epTbcXS)\n",
        "\n",
        ">>>>[Regularization](#scrollTo=ruhEy0yMldCF)\n",
        "\n",
        ">>>>>[Regularization in Matrix Factorization](#scrollTo=1_18KD9CnEk6)\n",
        "\n",
        ">>>[Python Implementation](#scrollTo=3LVneg7_sZYm)\n",
        "\n",
        ">>>[Probabilistic Matrix Factorization](#scrollTo=K5DWDURXkhdy)\n",
        "\n",
        ">>>>[Maximum likelihood estimation](#scrollTo=eE01L_Zvg_2x)\n",
        "\n",
        ">>>>[MAP Estimation](#scrollTo=RQn4tCIiht_2)\n",
        "\n",
        ">>>[Bayesian Matrix Factorization](#scrollTo=-hHAwlHKpGgM)\n",
        "\n",
        ">>>[Keras Implementation](#scrollTo=2LkBa6UxyVPO)\n",
        "\n",
        ">>>>[Matrix Factorization Keras](#scrollTo=AVZxznol1nQj)\n",
        "\n",
        ">>>>[Deep Learning Keras - Dense](#scrollTo=LhA72a6G2Ao4)\n",
        "\n",
        ">>>>[Deep Learning Keras - Residuals](#scrollTo=-VG8MuVN4nRh)\n",
        "\n",
        ">>[AutoRec](#scrollTo=Q-2Mgca6V8gu)\n",
        "\n",
        ">>>[Calculating Cost](#scrollTo=cNr-nKVAaM61)\n",
        "\n",
        ">>>[Python Implementation](#scrollTo=W0jRn7Yuav_f)\n",
        "\n",
        ">>[RBMs](#scrollTo=cREtuwmsT8qY)\n",
        "\n",
        ">>>[Bernoulli RBMs](#scrollTo=gB2VvyGwWqtj)\n",
        "\n",
        ">>>>[Calculation RBM](#scrollTo=m48LycHPdNFb)\n",
        "\n",
        ">>>>[Boltzmann Machines](#scrollTo=r7vbYYjudTLk)\n",
        "\n",
        ">>>>>[Training a Boltzmann Machine](#scrollTo=PvydCuqfpzDc)\n",
        "\n",
        ">>>[Intractability](#scrollTo=jVwDYapJt8mU)\n",
        "\n",
        ">>>[RBM Formula](#scrollTo=a0KU2AadvMKU)\n",
        "\n",
        ">[Resources](#scrollTo=IiWhdb8CU7jE)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmytnrgSPqmg"
      },
      "outputs": [],
      "source": [
        "*# Adjust depending large directory location\n",
        "# https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset\n",
        "\n",
        "# LARGE_FILE_DIR  = \"MachineLearning/notes/large_files\"\n",
        "LARGE_FILE_DIR  = \"./\"\n",
        "!unrar x  ./data.rar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7LG4JFYmfH9"
      },
      "source": [
        "## Mathematical Recommenders\n",
        "\n",
        "These examples are non-personalized recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NPdgLKNkwA2"
      },
      "source": [
        "### Lift\n",
        "\n",
        "$$\n",
        "\\text{Lift} = \\frac{p(A,B)}{p(A)p(B)} = \\frac{p(A|B)}{p(A)} = \\frac{p(B|A)}{p(B)}\n",
        "$$\n",
        "\n",
        "- Symmetric\n",
        "- If A and B are independent, then $p(A|B) = p(A)$\n",
        "  - $p(A|B) / p(A) = 1$\n",
        "- if increasing the probability of B increases the probability of A, then Lift > 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNo3EqUbmjRp"
      },
      "source": [
        "### Hacker News Formula\n",
        "\n",
        "Balancing Popularirty with Age\n",
        "\n",
        "$$\n",
        "\\frac{f(\\text{popularity})}{g(\\text{age})} \\\\[1cm]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{score} = \\frac{(\\text{ups} - \\text{downs} -1 )^{0.8}}{(\\text{age}+2)^{\\text{gravity}}} \\times \\text{penalty}\n",
        "$$\n",
        "\n",
        "- gravity = 1.8\n",
        "- penalty = multiplier to implement \"business rules\" (e.g. penalize self-posts, \"controversial\" posts, + many more rules)\n",
        "\n",
        "\\\n",
        "\n",
        "age starts from 2, to prevent division by zero\n",
        "\n",
        "exponent of numerator is bigger than the exponent of the denominator, meaning denominator grows faster.\n",
        "\n",
        "Age always overtakes popularity\n",
        "\n",
        "\\\n",
        "\n",
        "exponent 0.8 causes sublinear growth.\n",
        "meaning 0 → 100 worth more than 1000 → 1100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX9Eah-ppQoq"
      },
      "source": [
        "### Reddit Formula\n",
        "\n",
        "$$\n",
        "\\text{score} = \\text{sign}(\\text{ups}-\\text{down}) \\times \\log \\{ \\max(1, |\\text{ups} - \\text{downs}|) \\} + \\frac{\\text{age}}{45000}\n",
        "$$\n",
        "\n",
        "- log of the absoulte value of net votes - sublinear curve - initial votes matter more - max since log 0 is not possible\n",
        "\n",
        "\\\n",
        "\n",
        "- can be positive or negative\n",
        "  - The more downvotes you get, the futher your score goes down\n",
        "\n",
        "\\\n",
        "\n",
        "- age is in seconds from inception of reddit\n",
        "- age is always positive\n",
        "- newer links → more score\n",
        "- reddit scores will forever increase linearly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDmUR3Fp0YC"
      },
      "source": [
        "### Google Page Rank\n",
        "\n",
        "Logic: The page rank of a page is the probability I would end up on that page if I surfed the internet randomly for an infinite amount of time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIU2zR8LtEfH"
      },
      "source": [
        "#### Markov Models\n",
        "\n",
        "- Simplest way to think about Markov Models are bigrams from NLP\n",
        "- Build a probablistic language model\n",
        "- Can ask \"what is the probability of the next word in the sentence 'love' give the previous word was 'I'?\" i.e., p(love | I )\n",
        "\n",
        "**Bigrams**\n",
        "\n",
        "- It's a bigram because we only consider 2 words at a time\n",
        "\n",
        "We don't have to think of each item as a word, just a generic state: $x(t)$\n",
        "\n",
        "\"Markov\" means $x(t)$ doesn't depend on any values 2 or more steps behind, only the immediate last value.\n",
        "$$\n",
        "p(x_t | x_{t-1}, x_{t-2}, \\cdots, x_1) = p(x_t | x_{t-1})\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UENDikH0tHHt"
      },
      "source": [
        "#### Transition Probability Matrix\n",
        "\n",
        "- $A(i,j)$ tells use the probability of going to state j from state i\n",
        "$$\n",
        "A(i,j) = p(x_t = j | x_{t-1} = i )\n",
        "$$\n",
        "\n",
        "- Key: rows must sum to 1\n",
        "    - Since it's a probability this must be true\n",
        "    - If true, A is called a \"stochastic matrix\" or \"Markov Matrix\"\n",
        "$$\n",
        "\\sum^M_{j=1}A(i,j) = \\sum^M_{j=1}p(x_t=j|x_{t-1}=i) =1\n",
        "$$\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "Example:\n",
        "\n",
        "Weather is\n",
        "- state 1 = Sunny\n",
        "- state 2 = Rainy\n",
        "\n",
        "Suppose:\n",
        "- p( sunny | sunny ) = 0.9\n",
        "- p( sunny | rainy ) = 0.1\n",
        "- p( rainy | sunny ) = 0.1\n",
        "- p( rainy | rainy ) = 0.9\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbn1ogLEtKl_"
      },
      "source": [
        "#### Calculating Probabilities\n",
        "\n",
        "$$\n",
        "p(\\text{rainy} | \\text{sunny}) = \\frac{\\text{count}(\\text{sunny} \\rightarrow \\text{rainy})}{\\text{count}(\\text{sunny})}\n",
        "$$\n",
        "\n",
        "Generalized\n",
        "\n",
        "$$\n",
        "p(B|A) = \\frac{\\text{count}(A \\rightarrow B)}{\\text{count}(A)}\n",
        "$$\n",
        "\n",
        "Now probability of a sentence would be\n",
        "\n",
        "$$\n",
        "p(x_1, \\cdots, x_T) = p(x_1) \\prod^T_{t=2} p(x_t | x_{t-1})\n",
        "$$\n",
        "\n",
        "Problem: If a bigram didn't apear in the train set, the probility would be 0 and anything × 0 is 0.\n",
        "\n",
        "**Add-1 Smoothing**\n",
        "\n",
        "$$\n",
        "p(x_t=j|x_{t-1}=i) = \\frac{\\text{count}(i \\rightarrow j)+\\epsilon}{\\text{count}(i)+\\epsilon V} \\\\\n",
        "$$\n",
        "\n",
        "- Add a \"fake count\" to every possible bigram\n",
        "  - ϵ can be any value, for example 1.\n",
        "- V = Vocabulary size = number of unique words in dataset\n",
        "- In this case, V=M (number of states) since each state is a word\n",
        "\n",
        "\n",
        "e.g. p(and | and) never occurs but would get positve probability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PvC0vn1xck2"
      },
      "source": [
        "#### Beta Posterior Mean\n",
        "\n",
        "In our case, the equation is the beta posterior mean instead of only 2 possible outcomes, V possible outcomts\n",
        "\n",
        "$$\n",
        "E(\\pi) = \\frac{\\alpha'}{\\alpha'+\\beta'} = \\frac{\\alpha+(\\sum_{i=1}^N X_i)}{\\alpha + \\beta + N} \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS0RySi_1Ecy"
      },
      "source": [
        "#### State Distribution\n",
        "\n",
        "$\\pi_t$ = state probability distribution at time t\n",
        "\n",
        "$\\pi(t)$ is a row vector by convention\n",
        "\n",
        "For the weather example,\n",
        "\n",
        "$$\n",
        "\\pi_t = [p(x_t = \\text{sunny}), p(x_t = \\text{rainy})]\n",
        "$$\n",
        "\n",
        "**Future State Distribution**\n",
        "\n",
        "Calculating the $\\pi(t+1)$ use Bayes rule\n",
        "\n",
        "$$\n",
        "p(x_{t+1} = j) = \\sum^M_{i=1} p(x_{t+1} = j, x_t =i) \\\\\n",
        "= \\sum^M_{i=1} p(x_{t+1}=j | x_t = i) p(x_t = i) \\\\\n",
        "= \\sum^M_{i=1} A(i,j)\\pi(i) \\\\\n",
        "= \\pi_{t+1}(j)\n",
        "$$\n",
        "\n",
        "Since A is a matrix and $\\pi(t)$ is a vector, we can express it in terms of matrix math\n",
        "$$\n",
        "\\pi_{x+1}(j) = \\sum^M_{i=1} A(i,j)\\pi_t(i) \\\\\n",
        "\\pi_{t+1} = \\pi_t A\n",
        "$$\n",
        "\n",
        "Further future\n",
        "\n",
        "$$\n",
        "\\pi_{t+2} = \\pi_t A^2 \\\\\n",
        "\\pi_{t+k} = \\pi_t A^k \\\\\n",
        "$$\n",
        "\n",
        "For infinity\n",
        "\n",
        "$$\n",
        "\\pi_\\infty = \\lim_{t\\rightarrow \\infty} \\pi_0 A^t \\\\\n",
        "\\pi_\\infty = \\pi_\\infty A\n",
        "$$\n",
        "\n",
        "This is just the eigenvalue problem\n",
        "  - Give matrix A, find a vector and a scalar s.t. multiplying the vector by A is equivalent to stretching it be the scalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q640uQxg3_sJ"
      },
      "source": [
        "#### PageRank\n",
        "\n",
        "Every page on the internet is a state in a Markov Model\n",
        "\n",
        "The transition probablity is distributed equally amongst all links on a page\n",
        "- p(dlc.com | lp.me ) = 0.5\n",
        "- p(yt.com | lp.me ) = 0.5\n",
        "\n",
        "In general, we can write the transition probability as:\n",
        "\n",
        "$$\n",
        "p(x_t =j | x_{x-1} = i) = \\frac{1}{n(i)}\n",
        "$$\n",
        "if $i$ links to $j$, $n(i) = $ number of links on page $i$, otherwise $0$.\n",
        "\n",
        "**Smoothing**\n",
        "\n",
        "$$\n",
        "G = 0.85A + 0.15U \\\\\n",
        "U(i,j) = \\frac{1}{M} \\\\\n",
        "\\forall i,j = 1 \\dots M\n",
        "$$\n",
        "\n",
        "Find the limiting distribution of G - yields a vector of length M - these probabilities are the respective PageRanks for ech page on the internet\n",
        "\n",
        "$$\n",
        "\\pi_\\infty = \\pi_\\infty G\n",
        "$$\n",
        "\n",
        "\n",
        "**Perron-Frobenius Theorem**:\n",
        "> If G is a valid Markov matrix and all its elements are positive then the stationary distribution and limiting distribution are the same\n",
        "- Limiting Distribution: state distribution you'd arrive at after transitioning by G an infinite number of times\n",
        "- Stationary Distribution: a state distribution that does not change after transitioning by G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivwe3u2lzHp3"
      },
      "source": [
        "## Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S34-RUx8zI_f"
      },
      "source": [
        "### Smoothing (Dampening)\n",
        "\n",
        "To resolve the issue of 0 in sample data when getting mean\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum^N_{i=1} X_i + \\lambda \\mu_0}{N+\\lambda}\n",
        "$$\n",
        "\n",
        "\n",
        "- $\\lambda$ some random small non-zero number\n",
        "- $\\mu_0$ the global avergage or just some middle value\n",
        "\n",
        "\\\n",
        "\n",
        "For example:\n",
        "- 1000 reviews of 4 star - μ = 3 - λ = 1 → 3.999\n",
        "- 5 reviews of 4 star - μ = 3 - λ = 1 → 3.83\n",
        "- 1 review of 4 star - μ = 3 - λ = 1 → 3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gahD3CwG1XEv"
      },
      "source": [
        "### Explore-Exploit Dilemma\n",
        "\n",
        "Example 1\n",
        "\n",
        "Imagine we want to find the slot machine with the highest win rate among 10 slot machines.\n",
        "\n",
        "Traditional statistical test can tell us whether or not there's a significant difference between win rates between machines.\n",
        "\n",
        "If playing each machine 100 times, meaning 1000 turns total, 900 (9/10) turns yielded a suboptimal reward.\n",
        "\n",
        "Hence the dilemma, Play more or play less!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Example 2\n",
        "\n",
        "Watching a bunach of YouTube videos on how to make eggs.\n",
        "\n",
        "Now your reccomendations are filled with videos about making eggs\n",
        "\n",
        "porbably suboptimal - once I've figured out how to make eggs, I don't want to watch more egg videos.\n",
        "\n",
        "YouTube is not exploiting the fact that I watched eggs video and not exploring other topics\n",
        "\n",
        "Should there be a stronger exploration component?\n",
        "\n",
        "Maybe I'd like to seE movie trailers or machine learning videos\n",
        "\n",
        "---\n",
        "\n",
        "How do we strike a balance between these 2 opposing forces?\n",
        "\n",
        "Smoothed average gives us one part of the solution\n",
        "\n",
        "Making good things look worse and bad things look better\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4WrwUfO6MUp"
      },
      "source": [
        "### Bayesian Method\n",
        "\n",
        "Bayesian method automatically balances need to explore and exploit\n",
        "\n",
        "- 2 fat distributions: explore both (totally random ranking)\n",
        "- 2 skinny distributions: exploit both (nearly deterministic ranking)\n",
        "- Mixed: explore and exploit co-exit\n",
        "\n",
        "\n",
        "Completely automatic - does not require A/B testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biqlBm7295Kg"
      },
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "Non-specific to any particular user, Score each item from 1 to M a number.\n",
        "\n",
        "Basic algorithm is to make s(j) the average rating for j\n",
        "\n",
        "$$\n",
        "s(j) = \\frac{\\sum_{i \\in \\Omega_j} r_{ij}}{|\\Omega_j|} \\\\\n",
        "$$\n",
        "\n",
        "- $\\Omega_j$ = set of all users who rated item j\n",
        "- $r_{ij}$ = rating user i gave item j\n",
        "\n",
        "Translates to, average rating for a product is the sum of rating divided by the number of the ratings.\n",
        "\n",
        "**Personalize the score**\n",
        "\n",
        "s(i,j) can depend both on user i and item j\n",
        "\n",
        "$$\n",
        "s(i,j) = \\frac{\\sum_{i' \\in \\Omega_j} r_{i'j}}{|\\Omega_j|} \\\\\n",
        "$$\n",
        "\n",
        "i' is just an index\n",
        "\n",
        "i = 1 … N, N = number of users\n",
        "\n",
        "j = 1 … M, M = number of items\n",
        "\n",
        "$R_{N\\times M}$ = user -item ratings matrix of size N × M\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "- User-item matrix is reminiscent of term-document.\n",
        "- X(t,d) = # of time term t appears in document d\n",
        "- In terms of recommender systems, can think of X(t,d) as \"how much does t like the item d\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHSIzt7MLEya"
      },
      "source": [
        "### Sparsity\n",
        "\n",
        "One characteristic of the user items matrix that makes it unique to recommender systems is that it's Sparse.\n",
        "\n",
        "- Term-document matrix is sparse because most entries are 0\n",
        "- User-item matrix is sparse because most entries are **empty**\n",
        "\n",
        "The average user does not interact with all items.\n",
        "\n",
        "\n",
        "**Goal of Collaborative Filtering**\n",
        "\n",
        "- Most of r(i,j) doesn't exist - this is good.\n",
        "\n",
        "If every user has seen every item, then there's nothing to recommend\n",
        "\n",
        "Goal:\n",
        "> We want to guess what you might rate an item you haven't seen yet\n",
        "\n",
        "$$\n",
        "s(i,j) = \\hat r (i,j) = \\text{ guess what user i might rate item j}\n",
        "$$\n",
        "\n",
        "E.g. if we think you might rate some move a 5, we definitely want you to watch that movie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcJqzBwNcuy"
      },
      "source": [
        "### Regression\n",
        "\n",
        "Since this is a regression probelm, the evaluation metric is going to be the mean squared error.\n",
        "\n",
        "Outline:\n",
        "- user-user collaborative filtering\n",
        "- item-item collaborative filtering\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{|\\Omega|} \\sum_{i,j \\in \\Omega}(r_{ij} - \\hat r_{ij})^2\n",
        "$$\n",
        "\n",
        "Ω = set of pairs (i,j) where user i has rated item j\n",
        "\n",
        "we're going to take our models predicted ratings, compare them to the actual ratings, square the difference and then take the average of those squared differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlrdEcfgxdmH"
      },
      "source": [
        "### User-User Collaborative Filtering\n",
        "\n",
        "||item 1 | item2 | ... | item n |\n",
        "|--|--|--|--|--|\n",
        "|user 1| score |score | ... |score |\n",
        "|user 2| score |score | ... |score |\n",
        "| ... | ... |... | ... |... |\n",
        "|user n| score |score | ... |score |\n",
        "\n",
        "Comparing rows together, if 2 rows are very similar, it can be concluded that they have similar taste\n",
        "\n",
        "\n",
        "Average Rating reminder\n",
        "$\n",
        "s(i,j) = \\frac{\\sum_{i'\\in \\Omega_j} r_{i'j}}{|\\Omega_j|}\n",
        "$\n",
        "\n",
        "It treats everyone's rating of the items equally\n",
        "\n",
        "User 1's s(i,j) equally depends on user 2 rating and user 3 rating. even though user 1 doesn't agree with user 2.\n",
        "\n",
        "\n",
        "**Weighting Ratings**\n",
        "\n",
        "To make it small for users who don't agree and large for users who do agree\n",
        "\n",
        "$$\n",
        "s(i,j) = \\frac{\\sum_{i'\\in \\Omega_j} w_{ii'} r_{i'j}}{\\sum_{i'\\in\\Omega_j} w_{ii'}} \\\\\n",
        "$$\n",
        "\n",
        "Users can be biased, optimistic or perstimistic.\n",
        "\n",
        "**Deviation**\n",
        "\n",
        "Don't care about your absolute rating, but how much it deviates from your own average.\n",
        "- if your average is 2.5, but you rate something 5, it must be really great\n",
        "- if you rate everything a 5, it's diffcult to know how those items compare\n",
        "\n",
        "$$\n",
        "\\text{dev}(i,j) = r(i,j) - \\bar r_i, \\text{ for a known rating}\n",
        "$$\n",
        "\n",
        "My predicted rating is my own average + predicted deviation\n",
        "\n",
        "$$\n",
        "\\hat{\\text{dev}}(i,j) = \\frac{1}{|\\Omega_j|}\\sum_{i'\\in \\Omega_j} r(i',j) - \\bar r_{i'} \\\\\n",
        "$$\n",
        "\n",
        "For a prediction from known ratings\n",
        "\n",
        "$$\n",
        "s(i,j) = \\bar r_i + \\frac{1}{|\\Omega_j|}\\sum_{i'\\in \\Omega_j} r(i',j) - \\bar r_{i'} \\\\\n",
        "= \\bar r_i + \\hat{\\text{dev}}(i,j) \\\\\n",
        "$$\n",
        "\n",
        "Note: In order to make recommendation, I don't need to add back the average, because it's the same over all items\n",
        "\n",
        "**Combine**\n",
        "\n",
        "Combine the idea of deviations with the idea of weightings to get our final formula\n",
        "\n",
        "$$\n",
        "s(i,j) = \\bar r_i + \\frac{\\sum_{i'\\in \\Omega_j} w_{ii'} \\{ r_{i'j} - \\bar r_{i'} \\}}{\\sum_{i'\\in \\Omega_j} |w_{ii'}|}  \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJzwxoPl2ayu"
      },
      "source": [
        "How to calculate wieghts\n",
        "\n",
        "#### Pearson Correlation Coefficient\n",
        "\n",
        "$$\n",
        "\\varrho_{xy} = \\frac\n",
        "{\\sum^N_{i=1} (x_i -\\bar x)(y_i - \\bar y)}\n",
        "{\n",
        "  \\sqrt{\\sum^N_{i=1} (x_i - \\bar x)^2 }\n",
        "  \\sqrt{\\sum^N_{i=1} (y_i - \\bar y)^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "Our data is sparse, meaning we have a lot of missing data.\n",
        "\n",
        "Update formula:\n",
        "\n",
        "$$\n",
        "w_{ii'} = \\frac\n",
        "{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar i_i)(r_{i'j} - \\bar r_{i'})}\n",
        "{\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar i_i)^2 }\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{i'j} -\\bar i_{i'})^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "- $\\Psi_i$ = set of items that user i has rated\n",
        "- $\\Psi_{ii'}$ = set of items that user i and i' have rated\n",
        "- $\\Psi_{ii'} = \\Psi_i \\cap \\Psi_{i'}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ZTwhaW4o-v"
      },
      "source": [
        "#### Cosine Similarity\n",
        "\n",
        "$$\n",
        "\\cos \\theta = \\frac{x^Ty}{|x| \\ |y|} = \\frac\n",
        "{\\sum_{i=1}^N x_iy_i}\n",
        "{\n",
        "  \\sqrt{\\sum_{i=1}^N x_i^2 }\n",
        "  \\sqrt{\\sum_{i=1}^N y_i^2 }\n",
        "} \\\\\n",
        "$$\n",
        "\n",
        "They are the same, execpt pearson is centered.\n",
        "\n",
        "We want to center them anyway because we're working with deviations, not absolute ratings\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "If 2 users have zero or very few items in common, we don't want to consider them in the calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1QrBRRp5mq_"
      },
      "source": [
        "**Neighborhood**\n",
        "\n",
        "In practice, don't sum over all users who rated item j (takes too long)\n",
        "\n",
        "- It can help to precompute weights beforehand\n",
        "- Instead of summing over all users, take the ones with highest weight\n",
        "  - E.g. use K nearest neighbors, K=25 upto 50\n",
        "\n",
        "\\\n",
        "\n",
        "In Summary,\n",
        "Discard users w/ no items in common, or few items. Keep only users whose weights are high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fXxflLq7YCc"
      },
      "source": [
        "#### Python Implementation\n",
        "\n",
        "Outline\n",
        "- Split data into train and test sets\n",
        "- Calculate weightsss using train set\n",
        "- Make a predict function, e.g. score ← predict(i,j)\n",
        "- Output MSE for train and test sets\n",
        "\n",
        "\n",
        "Data fetched from [MovieLens 20M Dataset](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset) - File `rating.csv`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing IDs"
      ],
      "metadata": {
        "id": "eEZTqNZgP_3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y01Or58a9kt_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(LARGE_FILE_DIR + \"/rating.csv\")\n",
        "\n",
        "# note:\n",
        "# user ids are ordered sequentially from 1..138493\n",
        "# with no missing numbers\n",
        "# movie ids are integers from 1..131262\n",
        "# NOT all movie ids appear\n",
        "# there are only 26744 movie ids\n",
        "\n",
        "# make the user ids go from 0...N-1\n",
        "df.userId = df.userId - 1\n",
        "\n",
        "# create a mapping for movie ids\n",
        "unique_movie_ids = set(df.movieId.values)\n",
        "movie2idx = {}\n",
        "count = 0\n",
        "for movie_id in unique_movie_ids:\n",
        "  movie2idx[movie_id] = count\n",
        "  count += 1\n",
        "\n",
        "# add them to the data frame\n",
        "# takes awhile\n",
        "df['movie_idx'] = df.apply(lambda row: movie2idx[row.movieId], axis=1)\n",
        "\n",
        "df = df.drop(columns=['timestamp'])\n",
        "\n",
        "df.to_csv(LARGE_FILE_DIR + '/edited_rating.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shrinking data size"
      ],
      "metadata": {
        "id": "0M40cyyvQBf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e-Fvw4ZPqmh",
        "outputId": "d2a39be8-ceb3-4f05-e75b-4810766fccc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original dataframe size: 20000263\n",
            "i: 10000\n",
            "j: 2000\n",
            "Setting new ids\n",
            "max user id: 9999\n",
            "max movie id: 1999\n",
            "small dataframe size: 5392025\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# load in the data\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/edited_rating.csv')\n",
        "print(\"original dataframe size:\", len(df))\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "user_ids_count = Counter(df.userId)\n",
        "movie_ids_count = Counter(df.movie_idx)\n",
        "\n",
        "# number of users and movies we would like to keep\n",
        "n = 10000\n",
        "m = 2000\n",
        "\n",
        "user_ids = [u for u, c in user_ids_count.most_common(n)]\n",
        "movie_ids = [m for m, c in movie_ids_count.most_common(m)]\n",
        "\n",
        "# make a copy, otherwise ids won't be overwritten\n",
        "df_small = df[df.userId.isin(user_ids) & df.movie_idx.isin(movie_ids)].copy()\n",
        "\n",
        "# need to remake user ids and movie ids since they are no longer sequential\n",
        "new_user_id_map = {}\n",
        "i = 0\n",
        "for old in user_ids:\n",
        "  new_user_id_map[old] = i\n",
        "  i += 1\n",
        "print(\"i:\", i)\n",
        "\n",
        "new_movie_id_map = {}\n",
        "j = 0\n",
        "for old in movie_ids:\n",
        "  new_movie_id_map[old] = j\n",
        "  j += 1\n",
        "print(\"j:\", j)\n",
        "\n",
        "print(\"Setting new ids\")\n",
        "df_small.loc[:, 'userId'] = df_small.apply(lambda row: new_user_id_map[row.userId], axis=1)\n",
        "df_small.loc[:, 'movie_idx'] = df_small.apply(lambda row: new_movie_id_map[row.movie_idx], axis=1)\n",
        "# df_small.drop(columns=['userId', 'movie_idx'])\n",
        "# df_small.rename(index=str, columns={'new_userId': 'userId', 'new_movie_idx': 'movie_idx'})\n",
        "print(\"max user id:\", df_small.userId.max())\n",
        "print(\"max movie id:\", df_small.movie_idx.max())\n",
        "\n",
        "print(\"small dataframe size:\", len(df_small))\n",
        "df_small.to_csv(LARGE_FILE_DIR + '/small_rating.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating user-movie-rating data structures"
      ],
      "metadata": {
        "id": "3zBimvPKQF0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZvEbyS7Pqmh",
        "outputId": "36694632-74fd-413f-bd07-82e20d0967be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling: update_user2movie_and_movie2user\n",
            "processed: 0.023\n",
            "processed: 0.046\n",
            "processed: 0.070\n",
            "processed: 0.093\n",
            "processed: 0.116\n",
            "processed: 0.139\n",
            "processed: 0.162\n",
            "processed: 0.185\n",
            "processed: 0.209\n",
            "processed: 0.232\n",
            "processed: 0.255\n",
            "processed: 0.278\n",
            "processed: 0.301\n",
            "processed: 0.325\n",
            "processed: 0.348\n",
            "processed: 0.371\n",
            "processed: 0.394\n",
            "processed: 0.417\n",
            "processed: 0.440\n",
            "processed: 0.464\n",
            "processed: 0.487\n",
            "processed: 0.510\n",
            "processed: 0.533\n",
            "processed: 0.556\n",
            "processed: 0.580\n",
            "processed: 0.603\n",
            "processed: 0.626\n",
            "processed: 0.649\n",
            "processed: 0.672\n",
            "processed: 0.695\n",
            "processed: 0.719\n",
            "processed: 0.742\n",
            "processed: 0.765\n",
            "processed: 0.788\n",
            "processed: 0.811\n",
            "processed: 0.835\n",
            "processed: 0.858\n",
            "processed: 0.881\n",
            "processed: 0.904\n",
            "processed: 0.927\n",
            "processed: 0.950\n",
            "processed: 0.974\n",
            "processed: 0.997\n",
            "Calling: update_usermovie2rating_test\n",
            "processed: 0.093\n",
            "processed: 0.185\n",
            "processed: 0.278\n",
            "processed: 0.371\n",
            "processed: 0.464\n",
            "processed: 0.556\n",
            "processed: 0.649\n",
            "processed: 0.742\n",
            "processed: 0.835\n",
            "processed: 0.927\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/small_rating.csv')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "# split into train and test\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "# a dictionary to tell us which users have rated which movies\n",
        "user2movie = {}\n",
        "# a dicationary to tell us which movies have been rated by which users\n",
        "movie2user = {}\n",
        "# a dictionary to look up ratings\n",
        "usermovie2rating = {}\n",
        "print(\"Calling: update_user2movie_and_movie2user\")\n",
        "count = 0\n",
        "def update_user2movie_and_movie2user(row):\n",
        "  global count\n",
        "  count += 1\n",
        "  if count % 100000 == 0:\n",
        "    print(\"processed: %.3f\" % (float(count)/cutoff))\n",
        "\n",
        "  i = int(row.userId)\n",
        "  j = int(row.movie_idx)\n",
        "  if i not in user2movie:\n",
        "    user2movie[i] = [j]\n",
        "  else:\n",
        "    user2movie[i].append(j)\n",
        "\n",
        "  if j not in movie2user:\n",
        "    movie2user[j] = [i]\n",
        "  else:\n",
        "    movie2user[j].append(i)\n",
        "\n",
        "  usermovie2rating[(i,j)] = row.rating\n",
        "df_train.apply(update_user2movie_and_movie2user, axis=1)\n",
        "\n",
        "# test ratings dictionary\n",
        "usermovie2rating_test = {}\n",
        "print(\"Calling: update_usermovie2rating_test\")\n",
        "count = 0\n",
        "def update_usermovie2rating_test(row):\n",
        "  global count\n",
        "  count += 1\n",
        "  if count % 100000 == 0:\n",
        "    print(\"processed: %.3f\" % (float(count)/len(df_test)))\n",
        "\n",
        "  i = int(row.userId)\n",
        "  j = int(row.movie_idx)\n",
        "  usermovie2rating_test[(i,j)] = row.rating\n",
        "df_test.apply(update_usermovie2rating_test, axis=1)\n",
        "\n",
        "# note: these are not really JSONs\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'wb') as f:\n",
        "  pickle.dump(user2movie, f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'wb') as f:\n",
        "  pickle.dump(movie2user, f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'wb') as f:\n",
        "  pickle.dump(usermovie2rating, f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'wb') as f:\n",
        "  pickle.dump(usermovie2rating_test, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTNXFKR6Pqmi"
      },
      "source": [
        "Training and prediction\n",
        "\n",
        "\\\n",
        "\n",
        "Weights:\n",
        "$$\n",
        "w_{ii'} = \\frac\n",
        "{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar r_i)(r_{i'j} - \\bar r_{i'})}\n",
        "{\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar r_i)^2 }\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{i'j} -\\bar r_{i'})^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "Predict:\n",
        "$$\n",
        "s(i,j) = \\bar r_i + \\frac{\\sum_{i'\\in \\Omega_j} w_{ii'} \\{ r_{i'j} - \\bar r_{i'} \\}}{\\sum_{i'\\in \\Omega_j} |w_{ii'}|}  \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRu8mqwjPqmi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from sortedcontainers import SortedList\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "if N > 10000:\n",
        "  print(\"N =\", N, \"are you sure you want to continue?\")\n",
        "  print(\"Comment out these lines if so...\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "# to find the user similarities, you have to do O(N^2 * M) calculations!\n",
        "# in the \"real-world\" you'd want to parallelize this\n",
        "# note: we really only have to do half the calculations, since w_ij is symmetric\n",
        "K = 25 # number of neighbors we'd like to consider\n",
        "limit = 5 # number of common movies users must have in common in order to consider\n",
        "neighbors = [] # store neighbors in this list\n",
        "averages = [] # each user's average rating for later use\n",
        "deviations = [] # each user's deviation for later use\n",
        "for i in range(N):\n",
        "  # find the 25 closest users to user i\n",
        "  movies_i = user2movie[i]\n",
        "  movies_i_set = set(movies_i)\n",
        "\n",
        "  # calculate avg and deviation\n",
        "  ratings_i = { movie:usermovie2rating[(i, movie)] for movie in movies_i }\n",
        "  avg_i = np.mean(list(ratings_i.values()))\n",
        "  dev_i = { movie:(rating - avg_i) for movie, rating in ratings_i.items() }\n",
        "  dev_i_values = np.array(list(dev_i.values()))\n",
        "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
        "\n",
        "  # save these for later use\n",
        "  averages.append(avg_i)\n",
        "  deviations.append(dev_i)\n",
        "\n",
        "  sl = SortedList()\n",
        "  for j in range(N):\n",
        "    # don't include yourself\n",
        "    if j != i:\n",
        "      movies_j = user2movie[j]\n",
        "      movies_j_set = set(movies_j)\n",
        "      common_movies = (movies_i_set & movies_j_set) # intersection\n",
        "      if len(common_movies) > limit:\n",
        "        # calculate avg and deviation\n",
        "        ratings_j = { movie:usermovie2rating[(j, movie)] for movie in movies_j }\n",
        "        avg_j = np.mean(list(ratings_j.values()))\n",
        "        dev_j = { movie:(rating - avg_j) for movie, rating in ratings_j.items() }\n",
        "        dev_j_values = np.array(list(dev_j.values()))\n",
        "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
        "\n",
        "        # calculate correlation coefficient\n",
        "        numerator = sum(dev_i[m]*dev_j[m] for m in common_movies)\n",
        "        w_ij = numerator / (sigma_i * sigma_j)\n",
        "\n",
        "        # insert into sorted list and truncate\n",
        "        # negate weight, because list is sorted ascending\n",
        "        # maximum value (1) is \"closest\"\n",
        "        sl.add((-w_ij, j))\n",
        "        if len(sl) > K:\n",
        "          del sl[-1]\n",
        "\n",
        "  # store the neighbors\n",
        "  neighbors.append(sl)\n",
        "\n",
        "  # print out useful things\n",
        "  if i % 1 == 0:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# using neighbors, calculate train and test MSE\n",
        "\n",
        "def predict(i, m):\n",
        "  # calculate the weighted sum of deviations\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "  for neg_w, j in neighbors[i]:\n",
        "    # remember, the weight is stored as its negative\n",
        "    # so the negative of the negative weight is the positive weight\n",
        "    try:\n",
        "      numerator += -neg_w * deviations[j][m]\n",
        "      denominator += abs(neg_w)\n",
        "    except KeyError:\n",
        "      # neighbor may not have rated the same movie\n",
        "      # don't want to do dictionary lookup twice\n",
        "      # so just throw exception\n",
        "      pass\n",
        "\n",
        "  if denominator == 0:\n",
        "    prediction = averages[i]\n",
        "  else:\n",
        "    prediction = numerator / denominator + averages[i]\n",
        "  prediction = min(5, prediction)\n",
        "  prediction = max(0.5, prediction) # min rating is 0.5\n",
        "  return prediction\n",
        "\n",
        "\n",
        "train_predictions = []\n",
        "train_targets = []\n",
        "for (i, m), target in usermovie2rating.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(i, m)\n",
        "\n",
        "  # save the prediction and target\n",
        "  train_predictions.append(prediction)\n",
        "  train_targets.append(target)\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "# same thing for test set\n",
        "for (i, m), target in usermovie2rating_test.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(i, m)\n",
        "\n",
        "  # save the prediction and target\n",
        "  test_predictions.append(prediction)\n",
        "  test_targets.append(target)\n",
        "\n",
        "\n",
        "# calculate accuracy\n",
        "def mse(p, t):\n",
        "  p = np.array(p)\n",
        "  t = np.array(t)\n",
        "  return np.mean((p - t)**2)\n",
        "\n",
        "print('train mse:', mse(train_predictions, train_targets))\n",
        "print('test mse:', mse(test_predictions, test_targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Item-Item Collaborative Filtering\n",
        "\n"
      ],
      "metadata": {
        "id": "RVJUmoIsVf11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out if 2 items are similar\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "w_{jj'} = \\frac\n",
        "{\\sum_{i\\in \\Omega_{jj'}} (r_{ij} -\\bar r_j)(r_{ij'} - \\bar r_{j'})}\n",
        "{\n",
        "  \\sqrt{\\sum_{i\\in \\Omega_{jj'}} (r_{ij} -\\bar r_j)^2 }\n",
        "  \\sqrt{\\sum_{j\\in \\Omega_{jj'}} (r_{ij'} -\\bar r_{j'})^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "- $\\Omega_j$ users who rated item j\n",
        "- $\\Omega_{jj'}$ users who rated item j and item j'\n",
        "- $\\bar r_j$ avergae rating for item j\n",
        "\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "s(i,j) = \\bar r_j + \\frac{\\sum_{j'\\in \\Psi_i} w_{jj'} \\{ r_{ij'} - \\bar r_{j'} \\}}{\\sum_{j'\\in \\Psi_i} |w_{jj'}|}  \\\\\n",
        "$$\n",
        "\n",
        "- $\\Psi_i$ items user i has rated\n",
        "\n",
        "Deviation: how much user i likes item j', compared to how much everyone else like j'\n",
        "\n",
        "If user i really likes j' (more than other users do) and j is similar to j' (weight is high), the user i probably likes j too."
      ],
      "metadata": {
        "id": "d4fCJtiVAwku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Python Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "QSNW575oXYDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eVrGLqqPXmkJ"
      },
      "outputs": [],
      "source": [
        "# LARGE_FILE_DIR  = \"MachineLearning/notes/large_files\"\n",
        "LARGE_FILE_DIR  = \".\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from sortedcontainers import SortedList\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "if M > 2000:\n",
        "  print(\"N =\", N, \"are you sure you want to continue?\")\n",
        "  print(\"Comment out these lines if so...\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "# to find the user similarities, you have to do O(M^2 * N) calculations!\n",
        "# in the \"real-world\" you'd want to parallelize this\n",
        "# note: we really only have to do half the calculations, since w_ij is symmetric\n",
        "K = 20 # number of neighbors we'd like to consider\n",
        "limit = 5 # number of common movies users must have in common in order to consider\n",
        "neighbors = [] # store neighbors in this list\n",
        "averages = [] # each item's average rating for later use\n",
        "deviations = [] # each item's deviation for later use\n",
        "\n",
        "for i in range(M):\n",
        "  # find the K closest items to item i\n",
        "  users_i = movie2user[i]\n",
        "  users_i_set = set(users_i)\n",
        "\n",
        "  # calculate avg and deviation\n",
        "  ratings_i = { user:usermovie2rating[(user, i)] for user in users_i }\n",
        "  avg_i = np.mean(list(ratings_i.values()))\n",
        "  dev_i = { user:(rating - avg_i) for user, rating in ratings_i.items() }\n",
        "  dev_i_values = np.array(list(dev_i.values()))\n",
        "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
        "\n",
        "  # save these for later use\n",
        "  averages.append(avg_i)\n",
        "  deviations.append(dev_i)\n",
        "\n",
        "  sl = SortedList()\n",
        "  for j in range(M):\n",
        "    # don't include yourself\n",
        "    if j != i:\n",
        "      users_j = movie2user[j]\n",
        "      users_j_set = set(users_j)\n",
        "      common_users = (users_i_set & users_j_set) # intersection\n",
        "      if len(common_users) > limit:\n",
        "        # calculate avg and deviation\n",
        "        ratings_j = { user:usermovie2rating[(user, j)] for user in users_j }\n",
        "        avg_j = np.mean(list(ratings_j.values()))\n",
        "        dev_j = { user:(rating - avg_j) for user, rating in ratings_j.items() }\n",
        "        dev_j_values = np.array(list(dev_j.values()))\n",
        "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
        "\n",
        "        # calculate correlation coefficient\n",
        "        numerator = sum(dev_i[m]*dev_j[m] for m in common_users)\n",
        "        w_ij = numerator / (sigma_i * sigma_j)\n",
        "\n",
        "        # insert into sorted list and truncate\n",
        "        # negate weight, because list is sorted ascending\n",
        "        # maximum value (1) is \"closest\"\n",
        "        sl.add((-w_ij, j))\n",
        "        if len(sl) > K:\n",
        "          del sl[-1]\n",
        "\n",
        "  # store the neighbors\n",
        "  neighbors.append(sl)\n",
        "\n",
        "  # print out useful things\n",
        "  if i % 1 == 0:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# using neighbors, calculate train and test MSE\n",
        "\n",
        "def predict(i, u):\n",
        "  # calculate the weighted sum of deviations\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "  for neg_w, j in neighbors[i]:\n",
        "    # remember, the weight is stored as its negative\n",
        "    # so the negative of the negative weight is the positive weight\n",
        "    try:\n",
        "      numerator += -neg_w * deviations[j][u]\n",
        "      denominator += abs(neg_w)\n",
        "    except KeyError:\n",
        "      # neighbor may not have been rated by the same user\n",
        "      # don't want to do dictionary lookup twice\n",
        "      # so just throw exception\n",
        "      pass\n",
        "\n",
        "  if denominator == 0:\n",
        "    prediction = averages[i]\n",
        "  else:\n",
        "    prediction = numerator / denominator + averages[i]\n",
        "  prediction = min(5, prediction)\n",
        "  prediction = max(0.5, prediction) # min rating is 0.5\n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "train_predictions = []\n",
        "train_targets = []\n",
        "for (u, m), target in usermovie2rating.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(m, u)\n",
        "\n",
        "  # save the prediction and target\n",
        "  train_predictions.append(prediction)\n",
        "  train_targets.append(target)\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "# same thing for test set\n",
        "for (u, m), target in usermovie2rating_test.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(m, u)\n",
        "\n",
        "  # save the prediction and target\n",
        "  test_predictions.append(prediction)\n",
        "  test_targets.append(target)\n",
        "\n",
        "\n",
        "# calculate accuracy\n",
        "def mse(p, t):\n",
        "  p = np.array(p)\n",
        "  t = np.array(t)\n",
        "  return np.mean((p - t)**2)\n",
        "\n",
        "print('train mse:', mse(train_predictions, train_targets))\n",
        "print('test mse:', mse(test_predictions, test_targets))"
      ],
      "metadata": {
        "id": "Im6W5cs9Xet6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison\n",
        "\n",
        "- User-User CF: choose items for a user, because those items have been liked by similar users\n",
        "\n",
        "- Item-item CF: choose items for a user, because this user liked similar items in the past\n",
        "\n",
        "- When comparing 2 items, you have a lot more data than when comparing 2 users\n",
        "\n",
        "- Item-Based CF is fater\n",
        "  - given a user, score for ecah item $O(M^2N)$\n",
        "    - There are $M^2$ item-item weights< and each vector is length N\n",
        "  - For user-based CF< we had  $O(N^2M)\n",
        "    - $N>M$, so $N^2$ compared to $M^2$ is even worse\n",
        "\n",
        "- Item-based CF is more accurate as there's more data to train the weights on."
      ],
      "metadata": {
        "id": "-Hewxxx2C7sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Factorization\n",
        "\n",
        "\n",
        "Ratings matrix (N × M)\n",
        "- N = number of users\n",
        "- M = number of items\n",
        "\n",
        "Sparse data (Lots of missing points)\n",
        "\n",
        "> Convert the matrix $\\hat R$ into 2 vector products, $W$ and $U^T$\n",
        "$$\n",
        "\\hat R = WU^T\n",
        "$$\n",
        "\n",
        "\n",
        "- $\\hat R$ is the model estimation, not actual R\n",
        "\n",
        "- W is a (N × K) matrix - users matrix\n",
        "- U is a (M × K) matrix - items matrix\n",
        " - K somewhere from 10-50\n"
      ],
      "metadata": {
        "id": "LPgwQOSzPfEH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparse representation\n",
        "\n",
        "**Sparse representation** of a matrix:\n",
        "\n",
        "$$\n",
        "\\text{sparse} = \\frac{n}{r \\times c} = \\frac{\\text{number of data}}{\\text{number of cells}}\n",
        "$$\n",
        "\n",
        "- n: number of data inputs\n",
        "- r: rows\n",
        "- c: columns"
      ],
      "metadata": {
        "id": "txxT6ymhSNL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SVD\n",
        "\n",
        "$WU^T$ is M×N which is quite large to fit in memory.\n",
        "\n",
        "Choosing $w_i$ and $u_j$, it's just a product between 2 vectors of size K.\n",
        "\n",
        "$$\n",
        "\\hat r_{ij} = \\hat R[i,j] \\quad\n",
        "w_i = W[i] \\quad\n",
        "u_j = U[j] \\\\[1cm]\n",
        "\\hat r_{ij} = w_i^T u_j\\\\\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "**Why?**\n",
        "\n",
        "From a mathematical standpoint, we know from SVD (singular value decomposition) that a matrix X can be decomposed into 3 separate matrices multiplied together.\n",
        "\n",
        "$$\n",
        "X = USV^T\n",
        "$$\n",
        "\n",
        "- X = N × M\n",
        "- U = N × K\n",
        "- S = K × K\n",
        "- V = M × K\n",
        "\n",
        "\n",
        "R (ratings matrix) is sparse, so if U, S and V can properly approximate a full X matrix, then surely it can approximate a mostly empty R matrix.\n",
        "\n",
        "\\\n",
        "\n",
        "**Interpretation**\n",
        "\n",
        "Each of the k elements in $w_i$ and $u_j$ is a feature.\n",
        "\n",
        "For example they could be categories.\n",
        "\n",
        "- $w_i(1)$ is how much user i likes category 1, and so on.\n",
        "- $u_j(1)$ is how much item j contains category 2, and so on.\n",
        "\n",
        "Each feature is **latent** and K is the latent dimensionality, AKA *hidden cause*. We don't know the meaning of any feature without inspecting it.\n",
        "\n",
        "\\\n",
        "\n",
        "$w_i^T \\cdot u_j$ represents how user's i preferences correlate with item j's attributes.\n",
        "\n",
        "Proportional to cosine similarity\n",
        "$$\n",
        "w_i^Tu_j = \\| w_i \\| \\ \\|u_j\\| \\cos \\theta \\propto \\text{sim}(i,j)\n",
        "$$"
      ],
      "metadata": {
        "id": "tqVBGCMWStEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dimensionality Reduction\n",
        "\n",
        "Our usecase is an example of *Dimensionality Reduction* using SVD.\n",
        "\n",
        "The idea is we have X, and it can be decomposed into U, S, V.\n",
        "\n",
        "It turns out that X is exactly equal to $USV^T$ when K=M (assuming N>M and rank of X is M)\n",
        "\n",
        "But by shrinking U, S and V, we make it an approximation, $\\hat X$\n",
        "\n",
        "It is called *Truncated SVD*, and it yeilds our best rank-K approximation of X\n",
        "\n",
        "Similarly, Matrix factorization reduces the dimensionality of R.\n",
        "\n",
        "We encourage the model to learn the most important features required to generate R"
      ],
      "metadata": {
        "id": "V3TKC_wIeg2E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MF vs SVD\n",
        "\n",
        "**SVD**\n",
        "- X can be decomposed into a product of U,S,V\n",
        "- X(N×M), U(N×M), S(M×M), V(M×M)\n",
        "- No immediate saveings. size(X) == size(U)\n",
        "\n",
        "$$\n",
        "X = USV^T\n",
        "$$\n",
        "\n",
        "**Truncated SVD**\n",
        "- U,S, and V are ordered by \"*importance*\"\n",
        "- Thus, it's possible to \"*cut off*\" parts of U, S, and V, to get the best possible \"*low rank*\" approximation of X\n",
        "- U(N×K), S(K×K), V(M×K) → result is still N×M\n",
        "\n",
        "**MF as SVD**\n",
        "- S can be absorbed into U or V, we get an (N×K) × (K×M) → (N×M)\n",
        "\n",
        "$$\n",
        "\\hat x_{ij} = \\sum_k u_{ik} s_{kk} v_{kj} =\n",
        "\\sum_k (u_{ik} s_{kk}) v_{kj} =\n",
        "\\sum_k (u_{ik}' v_{kj}) =\n",
        "u_i'^T v_j\n",
        "$$\n",
        "\n",
        "**MF vs SVD**\n",
        "- SVD values are orthonormal, but MF is not.\n",
        "- SVD doens't work when X has missing values"
      ],
      "metadata": {
        "id": "xC-qr1Jrc0NZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training\n",
        "\n",
        "How to esnure a good approximation?\n",
        "\n",
        "$$\n",
        "R \\approx \\hat R = WU^T\n",
        "$$\n",
        "\n",
        "Since this is **regression**, you know that the way to do this is the **sum of squared errors**, technically we could also look at the mean squared error but only has a size division which has no effect on the out come.\n",
        "\n",
        "**Loss Function: Sum of Squared Errors**\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j \\in \\Omega} (r_{ij} - \\hat r_{ij})^2 =\n",
        "\\sum_{i,j \\in \\Omega} (r_{ij} - w_i^Tu_j)^2 \\\\\n",
        "$$\n",
        "\n",
        "- Ω = set of pairs (i,j) where user i rated item j\n",
        "\n",
        "\n",
        "**Minimize the loss**\n",
        "\n",
        "Find the gradient, set it to 0, solve for the parameters.\n",
        "\n",
        "- Ω: Pairs of all items that are rated by all users\n",
        "- $\\Psi_i$: Set of all items that user i has rated.\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial w_i} =\n",
        "2 \\sum_{j \\in \\Psi_i} (r_{ij} - w_i^Tu_j)(-u_j) = 0 \\\\\n",
        "$$\n"
      ],
      "metadata": {
        "id": "FYoCYIFskzXP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Solivng for W**\n",
        "\n",
        "$\n",
        "\\sum_{j \\in \\Psi_i} (w_i^Tu_j)(u_j) =\n",
        "\\sum_{j \\in \\Psi_i} r_{ij}u_j\n",
        "$\n",
        "\n",
        "Dot product is commutative\n",
        "\n",
        "$\n",
        "\\sum_{j \\in \\Psi_i} (u_j^Tw_i)(u_j) =\n",
        "\\sum_{j \\in \\Psi_i} r_{ij}u_j\n",
        "$\n",
        "\n",
        "scalar × vector = vector × scalar - Drop the brackets\n",
        "\n",
        "$\n",
        "\\sum_{j \\in \\Psi_i} u_ju_j^Tw_i=\n",
        "\\sum_{j \\in \\Psi_i} r_{ij}u_j\n",
        "$\n",
        "\n",
        "Summation doesn't depend on i\n",
        "\n",
        "$\n",
        "\\left (\\sum_{j \\in \\Psi_i} u_ju_j^T\\right)w_i=\n",
        "\\sum_{j \\in \\Psi_i} r_{ij}u_j \\\\\n",
        "$\n",
        "\n",
        "Solution:\n",
        "\n",
        "$$\n",
        "w_i= \\left (\\sum_{j \\in \\Psi_i} u_ju_j^T\\right)^{-1}\n",
        "\\sum_{j \\in \\Psi_i} r_{ij}u_j\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "9SxfDZYUqKvC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solving for U**\n",
        "\n",
        "- Loss is symmetric in W and U, so the steps should be the same\n",
        "- Remember to be careful which set to sum over\n",
        "\n",
        "Loss:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j \\in \\Omega} (r_{ij} - \\hat r_{ij})^2 =\n",
        "\\sum_{i,j \\in \\Omega} (r_{ij} - w_i^Tu_j)^2 \\\\\n",
        "$$\n",
        "\n",
        "Gradient:\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial u_j} =\n",
        "2 \\sum_{i \\in \\Omega_j} (r_{ij} - w_i^Tu_j)(-w_i) = 0 \\\\\n",
        "$$\n",
        "\n",
        "Solution:\n",
        "\n",
        "$$\n",
        "u_j= \\left (\\sum_{i \\in \\Omega_j} w_iw_i^T\\right)^{-1}\n",
        "\\sum_{i \\in \\Omega_j} r_{ij}w_i\n",
        "$$"
      ],
      "metadata": {
        "id": "bCdFbodspMpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Altering Least Squares\n",
        "\n",
        "**2-way Dependency:**\n",
        "- Solution for W depends on U\n",
        "- Solution for U depends on W\n",
        "\n",
        "we have 2 parameters, and both need to be update.\n",
        "\n",
        "So the alogrithm is to simply apply the equations as is, iteratively.\n",
        "\n",
        "It is called **altering least squares**\n",
        "\n",
        "**Algorithm**:\n",
        "\n",
        "\n",
        "`w = random(N,K) ; U = random (M, K)`\n",
        "\n",
        "`for t in range(T):`\n",
        "\n",
        "$$\n",
        "w_i= \\left (\\sum_{j \\in \\Psi_i} u_ju_j^T\\right)^{-1}\n",
        "\\sum_{j \\in \\Psi_i} r_{ij}u_j\\\\\n",
        "u_j= \\left (\\sum_{i \\in \\Omega_j} w_iw_i^T\\right)^{-1}\n",
        "\\sum_{i \\in \\Omega_j} r_{ij}w_i\n",
        "$$\n",
        "\n",
        "The order does not matter.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ywYS29A_qfmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bias Terms\n",
        "\n",
        "Adding bias terms to the Matrix Factorization model\n",
        "\n",
        "$$\n",
        "\\hat r_{ij} = w_i^Tu_j+b_i+c_j+\\mu\n",
        "$$\n",
        "\n",
        "- $b_i$ user bias\n",
        "- $c_j$ item bias\n",
        "- $\\mu$ global average - used to shift and center the data histogram\n",
        "\n",
        "\n",
        "Loss:\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j \\in \\Omega} (r_{ij} - \\hat r_{ij})^2 \\\\\n",
        "\\hat r_{ij} = w_i^Tu_j+b_i+c_j+\\mu\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for W\n",
        "\n",
        "$\n",
        "\\frac{\\partial J}{\\partial w_i} =\n",
        "2 \\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-b_i-c_j-\\mu)(-u_j) = 0 \\\\\n",
        "\\sum_{j \\in \\Psi_i }(w_i^Tu_j)u_j = \\sum_{j \\in \\Psi_i }(r_{ij} -b_i-c_j-\\mu)u_j \\\\\n",
        "$\n",
        "\n",
        "$$\n",
        "w_i = \\left (\\sum_{j \\in \\Psi_i } u_ju_j^T \\right)^{-1}\\sum_{j \\in \\Psi_i }(r_{ij} -b_i-c_j-\\mu)u_j \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for U\n",
        "\n",
        "$$\n",
        "u_j = \\left (\\sum_{i \\in \\Omega_j } w_iw_i^T \\right)^{-1}\\sum_{i \\in \\Omega_j }(r_{ij} -b_i-c_j-\\mu)w_i \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for b\n",
        "\n",
        "$\n",
        "\\frac{\\partial J}{\\partial b_i} =\n",
        "2 \\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-b_i-c_j-\\mu)(-1) = 0 \\\\\n",
        "$\n",
        "\n",
        "$$\n",
        "b_i = \\frac{1}{|\\Psi_i|}\\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-c_j-\\mu)\\\\\n",
        "$$\n",
        "\n",
        "- $b_i$ is the average deviation between target and modeling prediction, if the prediciton did not involve $b_i$\n",
        "  - i.e. $b_i$ is exactly how much you need to add to the model prediction without $b_i$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for c\n",
        "\n",
        "$$\n",
        "c_j = \\frac{1}{|\\Omega_j|}\\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-b_i-\\mu)\n",
        "$$\n",
        "\n",
        "\n",
        "- $c_j$ is the average deviation of a model that doesn't involve $c_j$\n",
        "\n"
      ],
      "metadata": {
        "id": "CBe12epTbcXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Regularization\n",
        "\n",
        "> Regularization is a technique that helps prevent overfitting and help generalization\n",
        "\n",
        "In leaner regression:\n",
        "\n",
        "**Model**\n",
        "\n",
        "$$\n",
        "\\hat y = w^T x\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "**Objective**\n",
        "$$\n",
        "J = \\sum^N_{i=1} (y_i - \\hat y_i)^2 + \\lambda \\| w\\|_2^2\n",
        "$$\n",
        "\n",
        "$\\lambda \\| w\\|_2^2$ squared magnitude of weights AKA penalty term. If they wieght becomes too large, that's a sign of overfitting, so they penalize\n",
        "\n",
        "\\\n",
        "\n",
        "**Solution**\n",
        "$$\n",
        "w = (\\lambda I + X^TX)^{-1}X^Ty\n",
        "$$"
      ],
      "metadata": {
        "id": "ruhEy0yMldCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Regularization in Matrix Factorization\n",
        "\n",
        "Same approach, add squared magnitude of each parameter multiplied by regularization constant\n",
        "\n",
        "$\\| *\\|_F$ is called the **Frobenius norm** which represents the size of the matrix\n",
        "\n",
        "\n",
        "$$\n",
        "J = \\sum_{i,j \\in \\Omega} (r_{ij} - \\hat r_{ij})^2 + \\lambda\\left(\\|W\\|_F^2 + \\|U\\|_F^2 + \\|b\\|_2^2 + \\|c\\|_2^2\\right)\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for W\n",
        "\n",
        "$\\displaystyle{\n",
        "\\|W\\|_F^2 = \\sum^N_{i=1}\\sum^K_{k=1} |w_{ik}|^2 =\n",
        "\\sum_{i=1}^N \\| w_i \\|_2^2 = \\sum_{i=1}^N w_i^T w_i \\\\\n",
        "\\frac{\\partial J}{\\partial w_i} =\n",
        "2 \\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-b_i-c_j-\\mu)(-u_j) + 2\\lambda w_i= 0 \\\\\n",
        "\\sum_{j \\in \\Psi_i }u_ju_j^Tw_i+ \\lambda w_i = \\sum_{j \\in \\Psi_i }(r_{ij} -b_i-c_j-\\mu)u_j \\\\\n",
        "}$\n",
        "\n",
        "$$\n",
        "w_i = \\left (\\sum_{j \\in \\Psi_i } u_ju_j^T + \\lambda I \\right)^{-1}\\sum_{j \\in \\Psi_i }(r_{ij} -b_i-c_j-\\mu)u_j \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for U\n",
        "\n",
        "$$\n",
        "u_j = \\left (\\sum_{i \\in \\Omega_j } w_iw_i^T+ \\lambda I \\right)^{-1}\\sum_{i \\in \\Omega_j }(r_{ij} -b_i-c_j-\\mu)w_i \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for b\n",
        "\n",
        "$\n",
        "\\frac{\\partial J}{\\partial b_i} =\n",
        "2 \\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-b_i-c_j-\\mu)(-1) + 2\\lambda b_i= 0 \\\\\n",
        "$\n",
        "\n",
        "$$\n",
        "b_i = \\frac{1}{|\\Psi_i| + \\lambda}\\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-c_j-\\mu)\n",
        "$$\n",
        "\n",
        "- λ the decay multiplier\n",
        "\n",
        "\\\n",
        "\n",
        "Solving for c\n",
        "\n",
        "$$\n",
        "c_j = \\frac{1}{|\\Omega_j| + \\lambda}\\sum_{j \\in \\Psi_i }(r_{ij} - w_i^Tu_j-b_i-\\mu)\n",
        "$$"
      ],
      "metadata": {
        "id": "1_18KD9CnEk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Implementation\n",
        "\n",
        "Outline:\n",
        "- Load in the data\n",
        "- initialize W, b, U, c, μ\n",
        "- Implement training algorithm, iterate until loss converges\n",
        "- Plot the train/test MSE per iteration\n",
        "- Print the final train/test MSE"
      ],
      "metadata": {
        "id": "3LVneg7_sZYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# My implementation - with bias - without regularization\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from sortedcontainers import SortedList\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "training_ratings=[ usermovie2rating[i] for i in usermovie2rating]\n",
        "\n",
        "\n",
        "if M > 2000:\n",
        "  print(\"N =\", N, \"are you sure you want to continue?\")\n",
        "  print(\"Comment out these lines if so...\")\n",
        "  exit()\n",
        "\n",
        "K = 10\n",
        "\n",
        "W = np.random.rand(N, K)\n",
        "b = np.random.rand(N)\n",
        "U = np.random.rand(M, K)\n",
        "c = np.random.rand(M)\n",
        "mu = np.mean(training_ratings)\n",
        "max_epoches=25\n",
        "threashold=0.05\n",
        "\n",
        "# calculate accuracy\n",
        "def mse(p, t):\n",
        "  p = np.array(p)\n",
        "  t = np.array(t)\n",
        "  return np.mean((p - t)**2)\n",
        "\n",
        "def predict(i,j):\n",
        "  # prediction = np.dot(W[i].T, U[j]) + b[i] + c[j] + mu\n",
        "  # prediction = min(5, prediction)\n",
        "  # prediction = max(0.5, prediction) # min rating is 0.5\n",
        "  # return prediction\n",
        "  return np.dot(W[i].T, U[j]) + b[i] + c[j] + mu\n",
        "\n",
        "mse_history = []\n",
        "def train():\n",
        "  epoch=0\n",
        "  prev_mse = 0\n",
        "  while True:\n",
        "    start_epoch = datetime.now()\n",
        "    # update\n",
        "    for i in range(N):\n",
        "      # Updating w_i\n",
        "      numerator = 0\n",
        "      denominator = 0\n",
        "      b_multiplier = 0\n",
        "      for mj in user2movie[i]:\n",
        "        try:\n",
        "          numerator += np.dot((usermovie2rating[(i, mj)] - b[i] - c[mj] - mu), U[mj])\n",
        "          denominator +=  np.dot(U[mj],  U[mj].T)\n",
        "          b_multiplier += usermovie2rating[(i, mj)] - np.dot(W[i].T, U[mj]) - c[mj] - mu\n",
        "        except Exception:\n",
        "          pass\n",
        "      W[i] = numerator / denominator\n",
        "      # Updating b_i\n",
        "      b[i] = b_multiplier / len(user2movie[i])\n",
        "\n",
        "    for j in range(M):\n",
        "      # Updating U_j\n",
        "      numerator = 0\n",
        "      denominator = 0\n",
        "      c_multiplier = 0\n",
        "      for ui in movie2user[j]:\n",
        "        try:\n",
        "          numerator += np.dot((usermovie2rating[(ui, j)] - b[ui] - c[j] - mu), W[ui])\n",
        "          denominator +=  np.dot(W[ui],  W[ui].T)\n",
        "          c_multiplier += usermovie2rating[(ui, j)] - np.dot(W[ui].T, U[j]) - b[ui] - mu\n",
        "        except Exception:\n",
        "          pass\n",
        "      U[j] = numerator / denominator\n",
        "      # Updating c_j\n",
        "      c[j] = c_multiplier / len(movie2user[j])\n",
        "    # predict\n",
        "    pred=[ predict(i,j) for i,j in usermovie2rating]\n",
        "    # evaluate\n",
        "    pred_mse = mse(training_ratings, pred)\n",
        "    print(f\"Epoch: {epoch} - MSE: {pred_mse} - Took: {datetime.now() - start_epoch}\")\n",
        "    if np.abs(pred_mse - prev_mse) < threashold or epoch > max_epoches:\n",
        "      return\n",
        "    prev_mse = pred_mse\n",
        "    mse_history.append(pred_mse)\n",
        "\n",
        "train()\n",
        "\n",
        "# Test\n",
        "test_ratings=[ usermovie2rating_test[i] for i in usermovie2rating_test]\n",
        "test_pred=[ predict(i,j) for i,j in usermovie2rating_test]\n",
        "print('test mse:', mse(test_ratings, test_pred))\n",
        "\n",
        "# Plot\n",
        "plt.plot(mse_history)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "wxpEMm5VNZZM",
        "outputId": "776bf34a-bebf-4759-8e6b-6e6ad9f257a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N: 10000 M: 2000\n",
            "Epoch: 0 - MSE: 9.723776154937127 - Took: 0:02:30.506734\n",
            "Epoch: 0 - MSE: 7.114249812769896 - Took: 0:02:31.159368\n",
            "Epoch: 0 - MSE: 3.342966496552602 - Took: 0:02:09.127873\n",
            "Epoch: 0 - MSE: 1.966748967926234 - Took: 0:02:39.427145\n",
            "Epoch: 0 - MSE: 1.3104111683607644 - Took: 0:02:07.293915\n",
            "Epoch: 0 - MSE: 1.068487878242153 - Took: 0:01:59.228235\n",
            "Epoch: 0 - MSE: 0.7860079232189136 - Took: 0:01:58.507581\n",
            "Epoch: 0 - MSE: 0.6359328516640248 - Took: 0:02:02.229957\n",
            "Epoch: 0 - MSE: 0.6194313632270768 - Took: 0:02:05.059512\n",
            "test mse: 0.6272546388626463\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lazy Programmer Implementation w/ bais and regularization\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "\n",
        "# initialize variables\n",
        "K = 10 # latent dimensionality\n",
        "W = np.random.randn(N, K)\n",
        "b = np.zeros(N)\n",
        "U = np.random.randn(M, K)\n",
        "c = np.zeros(M)\n",
        "mu = np.mean(list(usermovie2rating.values()))\n",
        "\n",
        "# prediction[i,j] = W[i].dot(U[j]) + b[i] + c.T[j] + mu\n",
        "\n",
        "def get_loss(d):\n",
        "  # d: (user_id, movie_id) -> rating\n",
        "  N = float(len(d))\n",
        "  sse = 0\n",
        "  for k, r in d.items():\n",
        "    i, j = k\n",
        "    p = W[i].dot(U[j]) + b[i] + c[j] + mu\n",
        "    sse += (p - r)*(p - r)\n",
        "  return sse / N\n",
        "\n",
        "\n",
        "# train the parameters\n",
        "epochs = 25\n",
        "reg =20. # regularization penalty\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  print(\"epoch:\", epoch)\n",
        "  epoch_start = datetime.now()\n",
        "  # perform updates\n",
        "\n",
        "  # update W and b\n",
        "  t0 = datetime.now()\n",
        "  for i in range(N):\n",
        "    # for W\n",
        "    matrix = np.eye(K) * reg\n",
        "    vector = np.zeros(K)\n",
        "\n",
        "    # for b\n",
        "    bi = 0\n",
        "    for j in user2movie[i]:\n",
        "      r = usermovie2rating[(i,j)]\n",
        "      matrix += np.outer(U[j], U[j])\n",
        "      vector += (r - b[i] - c[j] - mu)*U[j]\n",
        "      bi += (r - W[i].dot(U[j]) - c[j] - mu)\n",
        "\n",
        "    # set the updates\n",
        "    W[i] = np.linalg.solve(matrix, vector)\n",
        "    b[i] = bi / (len(user2movie[i]) + reg)\n",
        "\n",
        "    if i % (N//10) == 0:\n",
        "      print(\"i:\", i, \"N:\", N)\n",
        "  print(\"updated W and b:\", datetime.now() - t0)\n",
        "\n",
        "  # update U and c\n",
        "  t0 = datetime.now()\n",
        "  for j in range(M):\n",
        "    # for U\n",
        "    matrix = np.eye(K) * reg\n",
        "    vector = np.zeros(K)\n",
        "\n",
        "    # for c\n",
        "    cj = 0\n",
        "    try:\n",
        "      for i in movie2user[j]:\n",
        "        r = usermovie2rating[(i,j)]\n",
        "        matrix += np.outer(W[i], W[i])\n",
        "        vector += (r - b[i] - c[j] - mu)*W[i]\n",
        "        cj += (r - W[i].dot(U[j]) - b[i] - mu)\n",
        "\n",
        "      # set the updates\n",
        "      U[j] = np.linalg.solve(matrix, vector)\n",
        "      c[j] = cj / (len(movie2user[j]) + reg)\n",
        "\n",
        "      if j % (M//10) == 0:\n",
        "        print(\"j:\", j, \"M:\", M)\n",
        "    except KeyError:\n",
        "      # possible not to have any ratings for a movie\n",
        "      pass\n",
        "  print(\"updated U and c:\", datetime.now() - t0)\n",
        "  print(\"epoch duration:\", datetime.now() - epoch_start)\n",
        "\n",
        "\n",
        "  # store train loss\n",
        "  t0 = datetime.now()\n",
        "  train_losses.append(get_loss(usermovie2rating))\n",
        "\n",
        "  # store test loss\n",
        "  test_losses.append(get_loss(usermovie2rating_test))\n",
        "  print(\"calculate cost:\", datetime.now() - t0)\n",
        "  print(\"train loss:\", train_losses[-1])\n",
        "  print(\"test loss:\", test_losses[-1])\n",
        "\n",
        "\n",
        "print(\"train losses:\", train_losses)\n",
        "print(\"test losses:\", test_losses)\n",
        "\n",
        "# plot losses\n",
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(test_losses, label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tu1ayzEaPbWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from copy import deepcopy\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "\n",
        "# convert user2movie and movie2user to include ratings\n",
        "print(\"converting...\")\n",
        "user2movierating = {}\n",
        "for i, movies in user2movie.items():\n",
        "  r = np.array([usermovie2rating[(i,j)] for j in movies])\n",
        "  user2movierating[i] = (movies, r)\n",
        "movie2userrating = {}\n",
        "for j, users in movie2user.items():\n",
        "  r = np.array([usermovie2rating[(i,j)] for i in users])\n",
        "  movie2userrating[j] = (users, r)\n",
        "\n",
        "# create a movie2user for test set, since we need it for loss\n",
        "movie2userrating_test = {}\n",
        "for (i, j), r in usermovie2rating_test.items():\n",
        "  if j not in movie2userrating_test:\n",
        "    movie2userrating_test[j] = [[i], [r]]\n",
        "  else:\n",
        "    movie2userrating_test[j][0].append(i)\n",
        "    movie2userrating_test[j][1].append(r)\n",
        "for j, (users, r) in movie2userrating_test.items():\n",
        "  movie2userrating_test[j][1] = np.array(r)\n",
        "print(\"conversion done\")\n",
        "\n",
        "# initialize variables\n",
        "K = 10 # latent dimensionality\n",
        "W = np.random.randn(N, K)\n",
        "b = np.zeros(N)\n",
        "U = np.random.randn(M, K)\n",
        "c = np.zeros(M)\n",
        "mu = np.mean(list(usermovie2rating.values()))\n",
        "\n",
        "\n",
        "\n",
        "def get_loss(m2u):\n",
        "  # d: movie_id -> (user_ids, ratings)\n",
        "  N = 0.\n",
        "  sse = 0\n",
        "  for j, (u_ids, r) in m2u.items():\n",
        "    p = W[u_ids].dot(U[j]) + b[u_ids] + c[j] + mu\n",
        "    delta = p - r\n",
        "    sse += delta.dot(delta)\n",
        "    N += len(r)\n",
        "  return sse / N\n",
        "\n",
        "\n",
        "\n",
        "# train the parameters\n",
        "epochs = 25\n",
        "reg = 20. # regularization penalty\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "for epoch in range(epochs):\n",
        "  print(\"epoch:\", epoch)\n",
        "  epoch_start = datetime.now()\n",
        "  # perform updates\n",
        "\n",
        "  # update W and b\n",
        "  t0 = datetime.now()\n",
        "  for i in range(N):\n",
        "    m_ids, r = user2movierating[i]\n",
        "    matrix = U[m_ids].T.dot(U[m_ids]) + np.eye(K) * reg\n",
        "    vector = (r - b[i] - c[m_ids] - mu).dot(U[m_ids])\n",
        "    bi = (r - U[m_ids].dot(W[i]) - c[m_ids] - mu).sum()\n",
        "\n",
        "    # set the updates\n",
        "    W[i] = np.linalg.solve(matrix, vector)\n",
        "    b[i] = bi / (len(user2movie[i]) + reg)\n",
        "\n",
        "    if i % (N//10) == 0:\n",
        "      print(\"i:\", i, \"N:\", N)\n",
        "  print(\"updated W and b:\", datetime.now() - t0)\n",
        "\n",
        "\n",
        "  # update U and c\n",
        "  t0 = datetime.now()\n",
        "  for j in range(M):\n",
        "    try:\n",
        "      u_ids, r = movie2userrating[j]\n",
        "      matrix = W[u_ids].T.dot(W[u_ids]) + np.eye(K) * reg\n",
        "      vector = (r - b[u_ids] - c[j] - mu).dot(W[u_ids])\n",
        "      cj = (r - W[u_ids].dot(U[j]) - b[u_ids] - mu).sum()\n",
        "\n",
        "      # set the updates\n",
        "      U[j] = np.linalg.solve(matrix, vector)\n",
        "      c[j] = cj / (len(movie2user[j]) + reg)\n",
        "\n",
        "      if j % (M//10) == 0:\n",
        "        print(\"j:\", j, \"M:\", M)\n",
        "    except KeyError:\n",
        "      # possible not to have any ratings for a movie\n",
        "      pass\n",
        "  print(\"updated U and c:\", datetime.now() - t0)\n",
        "  print(\"epoch duration:\", datetime.now() - epoch_start)\n",
        "\n",
        "\n",
        "  # store train loss\n",
        "  t0 = datetime.now()\n",
        "  train_losses.append(get_loss(movie2userrating))\n",
        "\n",
        "  # store test loss\n",
        "  test_losses.append(get_loss(movie2userrating_test))\n",
        "  print(\"calculate cost:\", datetime.now() - t0)\n",
        "  print(\"train loss:\", train_losses[-1])\n",
        "  print(\"test loss:\", test_losses[-1])\n",
        "\n",
        "\n",
        "print(\"train losses:\", train_losses)\n",
        "print(\"test losses:\", test_losses)\n",
        "\n",
        "# plot losses\n",
        "plt.plot(train_losses, label=\"train loss\")\n",
        "plt.plot(test_losses, label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vF1C4L2wTtaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Probabilistic Matrix Factorization\n",
        "\n",
        "Instead of saying $\\hat R = W^TU$, let's instead say $\\underline R$ is a random matrix and it comes from normal distribution\n",
        "\n",
        "$$\n",
        "\\underline R \\sim N(WU^T, \\sigma^2) \\\\\n",
        "r_{ij} \\sim N(w_i^Tu_j, \\sigma^2)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "K5DWDURXkhdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Maximum likelihood estimation\n",
        "\n",
        "\n",
        "To find the parameters of the probabilty distribution, we can use the **Maximum likelihood estimation**\n",
        "\n",
        "Collect data from the distribution, maximize the likelihood function wrt parameters\n",
        "\n",
        "$$\n",
        "L = \\prod_{i,j \\in \\Omega} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left ( \\frac{-1}{2\\sigma^2}(r_{ij}-w_i^Tu_j)^2 \\right ) \\\\[1cm]\n",
        "W,U + \\text{argmax}_{W,U}L \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "**Maximing the Likelihood**\n",
        "\n",
        "- Take the log of the likelihood\n",
        "- Maximizing this is usually easier (and yields the same answer)\n",
        "- This is just the negative of loss\n",
        "\n",
        "$$\n",
        "\\log(L) = C - \\sum_{i,j \\in \\Omega} \\frac{1}{\\sigma^2}(r_{ij}-w_i^Tu_j)^2\n",
        "$$"
      ],
      "metadata": {
        "id": "eE01L_Zvg_2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### MAP Estimation\n",
        "\n",
        "Maximum a posteriori estimation, instead maximizing p(R) given W and U, we want to maximize P(W,U) given R\n",
        "\n",
        "$$\n",
        "\\text{MLE}: p(R | W,U) \\\\\n",
        "\\text{MAP}: p(W,U | R) \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Using Bayes rule:\n",
        "$$\n",
        "p(W,U| R) = \\frac{p(R|W,U)p(W)p(U)}{p(R)}\n",
        "$$\n",
        "\n",
        "- Thus, we must define the priors p(W) and p(U)\n",
        "- Note: p(R) is a constant wrt W, U, can be dropped\n",
        "\n",
        "$$\n",
        "p(W) = N(0, \\lambda^{-1}), p(U) = N(0,\\lambda^{-1})\\\\\n",
        "$$\n",
        "\n",
        "**MAP Objective**\n",
        "\n",
        "We want to take the log of this (Likelihood × priors)\n",
        "\n",
        "$$\n",
        "L = \\prod_{i,j \\in \\Omega} \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\exp \\left ( \\frac{-1}{2\\sigma^2}(r_{ij}-w_i^Tu_j)^2 \\right ) \\\\[1cm]\n",
        "W,U + \\text{argmax}_{W,U}L \\times\n",
        "\\frac{\\lambda}{\\sqrt{2\\pi}}e^{-\\frac{\\lambda}{2}\\|W\\|_F^2}\n",
        "\\frac{\\lambda}{\\sqrt{2\\pi}}e^{-\\frac{\\lambda}{2}\\|U\\|_F^2} \\\\[2cm]\n",
        "L = C_0 - C_1 \\sum_{i,j \\in \\Omega} (r_{ij}-w_i^Tu_j)^2 - \\frac{\\lambda}{2}\\|W\\|_F^2-\\frac{\\lambda}{2}\\|U\\|_F^2 \\\\\n",
        "$$\n",
        "\n",
        "This is just regularized matrix factorization (constants are irrelevant)\n"
      ],
      "metadata": {
        "id": "RQn4tCIiht_2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Matrix Factorization\n",
        "\n",
        "Summarize the Bayesian approach as \"everything is a random variable\", eve our prediction $r_{ij}$\n",
        "\n",
        "\n",
        "In Bayesian paradigm, we don't want point estimates, but probability distributions. that means we don't want the argmax for W,U that maximze posterior, Instead we want to find this posterior distribution or at very least take samples from it.\n",
        "\n",
        "$$\n",
        "p(W,U| R) = \\frac{p(R|W,U)p(W)p(U)}{p(R)}\n",
        "$$\n",
        "\n",
        "\n",
        "**Goal**\n",
        "\n",
        "we want to predict ratings, i.e. $p(r_{ij} | R), where R is the data we collected so far\n",
        "\n",
        "\n",
        "Expected Value\n",
        "\n",
        "$$\n",
        "E(x) = \\int^{+\\infty}_{-\\infty} x \\ p(x) \\ dx \\\\\n",
        "$$\n",
        "\n",
        "How about $r_{ij} | R$:\n",
        "\n",
        "$$\n",
        "E(r_{ij} | R) = \\int r_{ij} \\ p(r_{ij} | R) \\ dr_{ij} \\\\\n",
        "$$\n",
        "\n",
        "$$\n",
        "E(r_{ij} | R) = \\int r_{ij} \\ p(r_{ij} | W,U)p(W,U | R) dWdUdr_{ij} \\\\\n",
        "$$\n",
        "\n",
        "- $p(r_{ij} | W,U)$ Original Guassian\n",
        "- $p(W,U | R)$ Posterior\n",
        "\n",
        "If we isolate terms containing $r_{ij}$, we see it's just another expected value.\n",
        "\n",
        "$$\n",
        "\\int r_{ij} \\ p(r_{ij} | W,U) dr_{ij} = E(r_{ij} | W,U) = w_i^Tu_j \\\\[1cm]\n",
        "r_{ij} \\sim N(w_i^Tu_j, \\sigma^2)\n",
        "$$\n",
        "\n",
        "Expected value is the same as mean of distribution.\n",
        "\n",
        "Pluging them back to integral\n",
        "\n",
        "\n",
        "$$\n",
        "E(r_{ij} | R) = \\int w_i^Tu_j \\ p(W,U | R) \\ dWdU \\\\\n",
        "E(r_{ij} | R) = E(w_i^Tu_j | R) \\\\\n",
        "$$\n",
        "\n",
        "We can approximate this expected value, a mean can be approximated by the sample mean, if we sample from the correct distribution.\n",
        "- MCMC - Gibbs Sampling\n",
        "\n",
        "$$\n",
        "E(w_i^Tu_j | R) \\approx \\frac{1}{T} \\sum_{t=1}^T w_i^{(t)T}u_i^{(t)}\n",
        "$$"
      ],
      "metadata": {
        "id": "-hHAwlHKpGgM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Keras Implementation"
      ],
      "metadata": {
        "id": "2LkBa6UxyVPO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Matrix Factorization Keras"
      ],
      "metadata": {
        "id": "AVZxznol1nQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Embedding, Dot, Add, Flatten\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "# load in the data\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/edited_rating.csv')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "# split into train and test\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "# initialize variables\n",
        "K = 10 # latent dimensionality\n",
        "mu = df_train.rating.mean()\n",
        "epochs = 15\n",
        "reg = 0. # regularization penalty\n",
        "\n",
        "\n",
        "# keras model\n",
        "u = Input(shape=(1,))\n",
        "m = Input(shape=(1,))\n",
        "u_embedding = Embedding(N, K, embeddings_regularizer=l2(reg))(u) # (N, 1, K)\n",
        "m_embedding = Embedding(M, K, embeddings_regularizer=l2(reg))(m) # (M, 1, K)\n",
        "\n",
        "u_bias = Embedding(N, 1, embeddings_regularizer=l2(reg))(u) # (N, 1, 1)\n",
        "m_bias = Embedding(M, 1, embeddings_regularizer=l2(reg))(m) # (M, 1, 1)\n",
        "x = Dot(axes=2)([u_embedding, m_embedding]) # (N, 1, 1)\n",
        "\n",
        "\n",
        "x = Add()([x, u_bias, m_bias])\n",
        "x = Flatten()(x) # (N, 1)\n",
        "\n",
        "model = Model(inputs=[u, m], outputs=x)\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  # optimizer='adam',\n",
        "  # optimizer=Adam(lr=0.01),\n",
        "  optimizer=SGD(learning_rate=0.08, momentum=0.9),\n",
        "  metrics=['mse'],\n",
        ")\n",
        "\n",
        "r = model.fit(\n",
        "  x=[df_train.userId.values, df_train.movie_idx.values],\n",
        "  y=df_train.rating.values - mu,\n",
        "  epochs=epochs,\n",
        "  batch_size=128,\n",
        "  validation_data=(\n",
        "    [df_test.userId.values, df_test.movie_idx.values],\n",
        "    df_test.rating.values - mu\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "# plot losses\n",
        "plt.plot(r.history['loss'], label=\"train loss\")\n",
        "plt.plot(r.history['val_loss'], label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot mse\n",
        "plt.plot(r.history['mean_squared_error'], label=\"train mse\")\n",
        "plt.plot(r.history['val_mean_squared_error'], label=\"test mse\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jXnZDYRmymY6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Keras - Dense"
      ],
      "metadata": {
        "id": "LhA72a6G2Ao4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
        "from keras.layers import Dropout, BatchNormalization, Activation\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# load in the data\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/edited_rating.csv')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "# split into train and test\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "# initialize variables\n",
        "K = 10 # latent dimensionality\n",
        "mu = df_train.rating.mean()\n",
        "epochs = 15\n",
        "# reg = 0.0001 # regularization penalty\n",
        "\n",
        "\n",
        "# keras model\n",
        "u = Input(shape=(1,))\n",
        "m = Input(shape=(1,))\n",
        "u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
        "m_embedding = Embedding(M, K)(m) # (N, 1, K)\n",
        "u_embedding = Flatten()(u_embedding) # (N, K)\n",
        "m_embedding = Flatten()(m_embedding) # (N, K)\n",
        "x = Concatenate()([u_embedding, m_embedding]) # (N, 2K)\n",
        "\n",
        "# the neural network\n",
        "x = Dense(400)(x)\n",
        "# x = BatchNormalization()(x)\n",
        "x = Activation('relu')(x)\n",
        "# x = Dropout(0.5)(x)\n",
        "# x = Dense(100)(x)\n",
        "# x = BatchNormalization()(x)\n",
        "# x = Activation('relu')(x)\n",
        "x = Dense(1)(x)\n",
        "\n",
        "model = Model(inputs=[u, m], outputs=x)\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  # optimizer='adam',\n",
        "  # optimizer=Adam(lr=0.01),\n",
        "  optimizer=SGD(learning_rate=0.08, momentum=0.9),\n",
        "  metrics=['mse'],\n",
        ")\n",
        "\n",
        "r = model.fit(\n",
        "  x=[df_train.userId.values, df_train.movie_idx.values],\n",
        "  y=df_train.rating.values - mu,\n",
        "  epochs=epochs,\n",
        "  batch_size=128,\n",
        "  validation_data=(\n",
        "    [df_test.userId.values, df_test.movie_idx.values],\n",
        "    df_test.rating.values - mu\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "# plot losses\n",
        "plt.plot(r.history['loss'], label=\"train loss\")\n",
        "plt.plot(r.history['val_loss'], label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot mse\n",
        "plt.plot(r.history['mean_squared_error'], label=\"train mse\")\n",
        "plt.plot(r.history['val_mean_squared_error'], label=\"test mse\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "36f7qIKc2Cbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Deep Learning Keras - Residuals"
      ],
      "metadata": {
        "id": "-VG8MuVN4nRh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Embedding, Dot, Add, Flatten, Dense, Concatenate\n",
        "from keras.layers import Dropout, BatchNormalization, Activation\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD, Adam\n",
        "\n",
        "# load in the data\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/edited_rating.csv')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "# split into train and test\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "# initialize variables\n",
        "K = 10 # latent dimensionality\n",
        "mu = df_train.rating.mean()\n",
        "epochs = 15\n",
        "reg = 0. # regularization penalty\n",
        "\n",
        "\n",
        "# keras model\n",
        "u = Input(shape=(1,))\n",
        "m = Input(shape=(1,))\n",
        "u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
        "m_embedding = Embedding(M, K)(m) # (N, 1, K)\n",
        "\n",
        "\n",
        "##### main branch\n",
        "u_bias = Embedding(N, 1)(u) # (N, 1, 1)\n",
        "m_bias = Embedding(M, 1)(m) # (N, 1, 1)\n",
        "x = Dot(axes=2)([u_embedding, m_embedding]) # (N, 1, 1)\n",
        "x = Add()([x, u_bias, m_bias])\n",
        "x = Flatten()(x) # (N, 1)\n",
        "\n",
        "\n",
        "##### side branch\n",
        "u_embedding = Flatten()(u_embedding) # (N, K)\n",
        "m_embedding = Flatten()(m_embedding) # (N, K)\n",
        "y = Concatenate()([u_embedding, m_embedding]) # (N, 2K)\n",
        "y = Dense(400)(y)\n",
        "y = Activation('relu')(y)\n",
        "# y = Dropout(0.5)(y)\n",
        "y = Dense(1)(y)\n",
        "\n",
        "\n",
        "##### merge\n",
        "x = Add()([x, y])\n",
        "\n",
        "model = Model(inputs=[u, m], outputs=x)\n",
        "model.compile(\n",
        "  loss='mse',\n",
        "  # optimizer='adam',\n",
        "  # optimizer=Adam(lr=0.01),\n",
        "  optimizer=SGD(learning_rate=0.08, momentum=0.9),\n",
        "  metrics=['mse'],\n",
        ")\n",
        "\n",
        "r = model.fit(\n",
        "  x=[df_train.userId.values, df_train.movie_idx.values],\n",
        "  y=df_train.rating.values - mu,\n",
        "  epochs=epochs,\n",
        "  batch_size=128,\n",
        "  validation_data=(\n",
        "    [df_test.userId.values, df_test.movie_idx.values],\n",
        "    df_test.rating.values - mu\n",
        "  )\n",
        ")\n",
        "\n",
        "\n",
        "# plot losses\n",
        "plt.plot(r.history['loss'], label=\"train loss\")\n",
        "plt.plot(r.history['val_loss'], label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot mse\n",
        "plt.plot(r.history['mean_squared_error'], label=\"train mse\")\n",
        "plt.plot(r.history['val_mean_squared_error'], label=\"test mse\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7alNzFo-5rWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AutoRec\n",
        "\n",
        "Autoencoders to recommendation.\n",
        "\n",
        "A neural network that just reproduces its own input\n",
        "\n",
        "$$\n",
        "\\text{Error} = (\\text{output} - \\text {input})^2\n",
        "$$\n",
        "\n",
        "Autoencoders can be used to convert a noisy image/image with missing spots to a full image. In our case, we can simply treat the user-items matrix as if ti were an samples-features data matrix (with tons of missing values).\n",
        "- Each user is a sample\n",
        "- EaCh feature is an item rating\n",
        "\n",
        "We can flip it as well, but this way works better.\n",
        "\n",
        "AutoRec learns faster than other methods, DDN that looks at each rating individually loop a lot more samples per epoch comparing to only having users.\n"
      ],
      "metadata": {
        "id": "Q-2Mgca6V8gu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Cost\n",
        "\n",
        "An array can't contain \"missing\" values just zeros (In our case, all ratings are between 0.5-5.0)\n",
        "\n",
        "Must make sure autoencoder doesn't literally try to reproduce input x (otherwise, it will try to reproduce the zeros too!)\n",
        "\n",
        "Solution, we need a cosst function:\n",
        "\n",
        "$$\n",
        "J = \\frac{1}{|\\Omega|} \\sum_{i=1}^N\\sum_{j=1}^M m_{ij}(r_{ij} - \\hat r_{ij})^2 \\\\[1cm]\n",
        "m_{ij} = 1 \\text{ if } (i,j) \\in \\Omega \\text{ else } 0\n",
        "$$\n",
        "\n",
        "**Test MSE**\n",
        "\n",
        "Normally, the test error is calculated from the test input. Doesn't make sense here. The \"test set\" is an N×M matrix of the ratings we're trying to predict. We can't use the test ratings to predict themselves.\n",
        "\n",
        "The train ratings should predict the test ratings"
      ],
      "metadata": {
        "id": "cNr-nKVAaM61"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Implementation"
      ],
      "metadata": {
        "id": "W0jRn7Yuav_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sparse Preprocess Python**\n",
        "\n",
        "We can't fit the N×M matrix in the memeory.\n",
        "\n",
        "Scipy supports a spacial format \"sparse matrix\"\n",
        "- lil_matrix\n",
        "- csr_matrix\n",
        "- coo_matrix\n",
        "\n",
        "Keras doesn't recognize sparse matrices, so we'd need a custom generator that densify only a batch at a time."
      ],
      "metadata": {
        "id": "Qxh0T_RTZGab"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.sparse import lil_matrix, csr_matrix, save_npz, load_npz\n",
        "\n",
        "# load in the data\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/edited_rating.csv')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "# split into train and test\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "A = lil_matrix((N, M))\n",
        "print(\"Calling: update_train\")\n",
        "count = 0\n",
        "def update_train(row):\n",
        "  global count\n",
        "  count += 1\n",
        "  if count % 100000 == 0:\n",
        "    print(\"processed: %.3f\" % (float(count)/cutoff))\n",
        "\n",
        "  i = int(row.userId)\n",
        "  j = int(row.movie_idx)\n",
        "  A[i,j] = row.rating\n",
        "df_train.apply(update_train, axis=1)\n",
        "\n",
        "# mask, to tell us which entries exist and which do not\n",
        "A = A.tocsr()\n",
        "mask = (A > 0)\n",
        "save_npz(LARGE_FILE_DIR + \"/Atrain.npz\", A)\n",
        "\n",
        "# test ratings dictionary\n",
        "A_test = lil_matrix((N, M))\n",
        "print(\"Calling: update_test\")\n",
        "count = 0\n",
        "def update_test(row):\n",
        "  global count\n",
        "  count += 1\n",
        "  if count % 100000 == 0:\n",
        "    print(\"processed: %.3f\" % (float(count)/len(df_test)))\n",
        "\n",
        "  i = int(row.userId)\n",
        "  j = int(row.movie_idx)\n",
        "  A_test[i,j] = row.rating\n",
        "df_test.apply(update_test, axis=1)\n",
        "A_test = A_test.tocsr()\n",
        "mask_test = (A_test > 0)\n",
        "save_npz(LARGE_FILE_DIR + \"/Atest.npz\", A_test)"
      ],
      "metadata": {
        "id": "4qtMzbTlZmMZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoRec"
      ],
      "metadata": {
        "id": "nvUw1S6XeKnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from scipy.sparse import save_npz, load_npz\n",
        "\n",
        "import keras.backend as K\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dropout, Dense\n",
        "from keras.regularizers import l2\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "# config\n",
        "batch_size = 128\n",
        "epochs = 20\n",
        "reg = 0.0001\n",
        "\n",
        "A = load_npz(LARGE_FILE_DIR + \"/Atrain.npz\")\n",
        "A_test = load_npz(LARGE_FILE_DIR + \"/Atest.npz\")\n",
        "mask = (A > 0) * 1.0\n",
        "mask_test = (A_test > 0) * 1.0\n",
        "\n",
        "# make copies since we will shuffle\n",
        "A_copy = A.copy()\n",
        "mask_copy = mask.copy()\n",
        "A_test_copy = A_test.copy()\n",
        "mask_test_copy = mask_test.copy()\n",
        "\n",
        "N, M = A.shape\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "print(\"N // batch_size:\", N // batch_size)\n",
        "\n",
        "# center the data\n",
        "mu = A.sum() / mask.sum()\n",
        "print(\"mu:\", mu)\n",
        "\n",
        "\n",
        "\n",
        "# build the model - just a 1 hidden layer autoencoder\n",
        "i = Input(shape=(M,))\n",
        "# bigger hidden layer size seems to help!\n",
        "x = Dropout(0.7)(i)\n",
        "x = Dense(700, activation='tanh', kernel_regularizer=l2(reg))(x)\n",
        "# x = Dropout(0.5)(x)\n",
        "x = Dense(M, kernel_regularizer=l2(reg))(x)\n",
        "\n",
        "\n",
        "\n",
        "def custom_loss(y_true, y_pred):\n",
        "  mask = K.cast(K.not_equal(y_true, 0), dtype='float32')\n",
        "  diff = y_pred - y_true\n",
        "  sqdiff = diff * diff * mask\n",
        "  sse = K.sum(K.sum(sqdiff))\n",
        "  n = K.sum(K.sum(mask))\n",
        "  return sse / n\n",
        "\n",
        "\n",
        "def generator(A, M):\n",
        "  while True:\n",
        "    A, M = shuffle(A, M)\n",
        "    for i in range(A.shape[0] // batch_size + 1):\n",
        "      upper = min((i+1)*batch_size, A.shape[0])\n",
        "      a = A[i*batch_size:upper].toarray()\n",
        "      m = M[i*batch_size:upper].toarray()\n",
        "      a = a - mu * m # must keep zeros at zero!\n",
        "      # m2 = (np.random.random(a.shape) > 0.5)\n",
        "      # noisy = a * m2\n",
        "      noisy = a # no noise\n",
        "      yield noisy, a\n",
        "\n",
        "\n",
        "def test_generator(A, M, A_test, M_test):\n",
        "  # assumes A and A_test are in corresponding order\n",
        "  # both of size N x M\n",
        "  while True:\n",
        "    for i in range(A.shape[0] // batch_size + 1):\n",
        "      upper = min((i+1)*batch_size, A.shape[0])\n",
        "      a = A[i*batch_size:upper].toarray()\n",
        "      m = M[i*batch_size:upper].toarray()\n",
        "      at = A_test[i*batch_size:upper].toarray()\n",
        "      mt = M_test[i*batch_size:upper].toarray()\n",
        "      a = a - mu * m\n",
        "      at = at - mu * mt\n",
        "      yield a, at\n",
        "\n",
        "\n",
        "\n",
        "model = Model(i, x)\n",
        "model.compile(\n",
        "  loss=custom_loss,\n",
        "  optimizer=SGD(learning_rate=0.08, momentum=0.9),\n",
        "  # optimizer='adam',\n",
        "  metrics=[custom_loss],\n",
        ")\n",
        "\n",
        "\n",
        "r = model.fit(\n",
        "  generator(A, mask),\n",
        "  validation_data=test_generator(A_copy, mask_copy, A_test_copy, mask_test_copy),\n",
        "  epochs=epochs,\n",
        "  steps_per_epoch=A.shape[0] // batch_size + 1,\n",
        "  validation_steps=A_test.shape[0] // batch_size + 1,\n",
        ")\n",
        "print(r.history.keys())\n",
        "\n",
        "\n",
        "\n",
        "# plot losses\n",
        "plt.plot(r.history['loss'], label=\"train loss\")\n",
        "plt.plot(r.history['val_loss'], label=\"test loss\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# plot mse\n",
        "plt.plot(r.history['custom_loss'], label=\"train mse\")\n",
        "plt.plot(r.history['val_custom_loss'], label=\"test mse\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "j7SJf2eseDXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RBMs\n",
        "\n",
        "Restricted Boltzmann Machines, RBMs, for recommender systems.\n",
        "\n",
        " A single layer of nueral network that doesn't have any output, In other words, if we take a neural network and we just look at a single block connecting some input to some output, this is the basic structure of an RBM.\n",
        "\n",
        " - Visible - v\n",
        " - Hidden - h\n",
        " - You can go forward and backward\n",
        "  - Can calculate h from v(v→h)\n",
        "  - Can also calculate v from h!(h→v)\n",
        "\n"
      ],
      "metadata": {
        "id": "cREtuwmsT8qY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bernoulli RBMs\n",
        "\n",
        "For most discussions about RBMs, we are typically working with Bernoulli RBMs, that means we assume that the **input can only be zero or one**, and we assume that the hidden values can also only take on the **values zero or one**.\n",
        "\n",
        "this constraint is usually relaxed\n",
        "\n",
        "**Bernoulli distribution**\n",
        "\n",
        "a Bernoulli distribution is a distribution that describes a random variable that can only take on the values zero or one. Eg coin toss.\n",
        "\n",
        "Nothing but a neural network layer with a sigmoid activation function\n"
      ],
      "metadata": {
        "id": "gB2VvyGwWqtj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculation RBM\n",
        "\n",
        "**V → H**\n",
        "\n",
        "Scalar Form:\n",
        "\n",
        "$$\n",
        "p(h_j = 1 | v) = \\sigma(\\sum_{i=1}^D W_{ij}v_i + c_j) \\quad i = 1 \\dots D \\quad, j= 1\\dots M \\\\\n",
        "\\text{len}(v)=D \\quad \\text{len}(h) = M \\\\\n",
        "$$\n",
        "\n",
        "Vector Form:\n",
        "\n",
        "$$\n",
        "p(h=1|v) = \\sigma(W^Tv+c)\n",
        "$$\n",
        "\n",
        "**H → V**\n",
        "\n",
        "$$\n",
        "p(v_i = 1 | h) = \\sigma(\\sum_{j=1}^M W_{ij}h_j + b_i) \\quad i = 1 \\dots D \\quad, j= 1\\dots M \\\\\n",
        "$$\n",
        "\n",
        "Vector Form:\n",
        "\n",
        "$$\n",
        "p(v=1|h) = \\sigma(Wh+b)\n",
        "$$\n",
        "\n",
        "- b and c are bias vectors\n",
        "- We only get probabilties\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "m48LycHPdNFb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Relaxing the Bernoulli constraint**\n",
        "\n",
        "Scale the values to 0…1, round all values to 0 or 1. It's also possible to not round the numbers to 0 and 1\n",
        "\n",
        "Also can be done for h, In RBM, we want to go to h from v and back to v from h. In other words, it's calculating a distribution for V given an observed V. We can use the probabilities for H itself\n",
        "\n",
        "$$\n",
        "\\tilde h = p(h=1|v) = \\sigma (W^Tv+c) \\\\\n",
        "p(v' = 1|h ) = \\sigma (W\\tilde h + b) \\\\\n",
        "$$\n",
        "\n",
        "Similar to autoencoders.\n",
        "\n",
        "If we go from v → h → v' in RBM, the cross entropy (\"distance\" between v and v') will go down as we train, even though we don't optimize it!\n",
        "\n",
        "$$\n",
        "h = \\sigma(W^Tv+c) \\\\\n",
        "v' = \\sigma(Wh+b)\n",
        "$$"
      ],
      "metadata": {
        "id": "m9zdNRD4a7jx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Motivation Behind RBMs**\n",
        "\n",
        "What if we create a neural network without imposing any structure on it? Everything connected to everything else.\n",
        "\n",
        "#### Boltzmann Machines\n",
        "\n",
        "It has been done, and it's called **Boltzmann Machines**. In a Boltzmann machine some units can be hidden and some can be visible.\n",
        "\n",
        "Comparing to our brain, You can think of the visible neurons as neurons that are conencted to outside world (our senses), you can think of hidden nuerons as all other neurons, neurons that are not directly sensing things from the environment, but they are still connected to other neurons that are either directly or indirectly.\n",
        "\n",
        "\\\n",
        "\n",
        "**Boltzmann Machine in Physics**\n",
        "\n",
        "- Energy of a Boltzmann machine\n",
        "- Goal is to find some equilibrium (E.g. thermal equilibrium)\n",
        "- Looks like a neural network equation! Has a weight matrix, bias term\n",
        "\n",
        "$$\n",
        "E = - \\left ( \\sum_{i,j} W_{ij} s_i s_j + \\sum_i b_is_i \\right ) \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n"
      ],
      "metadata": {
        "id": "r7vbYYjudTLk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training a Boltzmann Machine\n",
        "\n",
        "Boltsmann machines are difficult to train, Only works on trivial examples.\n",
        "\n",
        "*Restricted Boltzmann machines* do train well, and scale up to non-trivial problems\n",
        "\n",
        "$$\n",
        "G = \\sum_v P^+ (v) \\ln \\left ( \\frac{P^+(v)}{P^-(v)} \\right )\n",
        "$$\n",
        "\n",
        "In *Restricted Boltzmann machines*, we restricted which nodes to be connected.\n",
        "\n",
        "Move all hiddens to one side and visibles to the other side. Discard any connections between hidden-hidden and visible-visible.\n",
        "\n",
        "**Energy of an RBM**\n",
        "\n",
        "$$\n",
        "E(v,h) = - \\left(\\sum_{i=1}^D\\sum_{j=1}^M W_{ij} v_i h_j + \\sum^D_{i=1} b_iv_i + \\sum_{i=1}^M c_jh_j \\right)\\\\[1cm]\n",
        "\\text{In deep learning} \\\\\n",
        "E(v,h) = - (v^TWh+ b^Tv+ c^Th)\n",
        "$$"
      ],
      "metadata": {
        "id": "PvydCuqfpzDc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Probability Model**\n",
        "\n",
        "$$\n",
        "p(v,h) \\propto e^{-E(v,h)} \\\\\n",
        "p(v,h) = \\frac{1}{Z} e^{-E(v,h)} \\\\\n",
        "Z = \\sum_v\\sum_h e^{-E(v,h)} \\\\\n",
        "\\text{So that: } \\sum_v\\sum_h p(v,h) = 1\n",
        "$$\n",
        "\n",
        "*Why*?\n",
        "\n",
        "Statistical mechanics\n",
        "\n",
        "General outline: $p_i$ is the probability that a system is in microstate with energy $E_i$\n",
        "\n",
        "$T$ = temperature, $Z$ = partition function\n",
        "\n",
        "\n",
        "$$\n",
        "p_i \\propto e^{\\frac{-E_i}{kT}} \\\\\n",
        "p_i = \\frac{1}{Z} e^{\\frac{-E_i}{kT}} \\\\\n",
        "Z = \\sum_i e^{\\frac{-E_i}{kT}} \\\\\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "3dYyY7SUpmiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intractability\n",
        "\n",
        "If v has length D and h has length M, the number of total possiblities is: $2^D \\times 2^M = 2^{D+M}$\n",
        "\n",
        "$$\n",
        "p(v,h)=\\frac{1}{Z} e^{-E(v,h)} \\\\\n",
        "Z = \\sum_v\\sum_h e^{-E(v,h)} \\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "jVwDYapJt8mU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RBM Formula\n",
        "\n",
        "Statistical mechanics leads us back to the equations we already know\n",
        "\n",
        "Driving from Bayes rule:\n",
        "\n",
        "*Marginals*\n",
        "\n",
        "$$\n",
        "p(v|h) = p(v,h) / p(h) \\\\\n",
        "p(h|v) = p(v,h) / p(v) \\\\[1cm]\n",
        "p(v) = \\sum_hp(v,h)\\quad\\quad p(h) = \\sum_vp(v,h) \\\\\n",
        "$$\n",
        "\n",
        "*Plug in what we know*\n",
        "\n",
        "$$\n",
        "p(v,h) = \\frac{1}{Z} \\exp( v^TWh + b^Tv + c^Th) \\\\\n",
        "p(v) = \\sum_h\\frac{1}{Z} \\exp( v^TWh + b^Tv + c^Th) \\\\\n",
        "p(h|v) = \\frac{\\exp( v^TWh + b^Tv + c^Th)}{\\sum_h \\exp( v^TWh + b^Tv + c^Th)}\\\\\n",
        "$$\n",
        "\n",
        "*Simplify*\n",
        "\n",
        "denominator is simply another normalizing constant\n",
        "\n",
        "$$\n",
        "p(h|v) = \\frac{1}{Z'} \\exp\\left ( v^TWh + b^Tv + c^Th \\right )\\\\\n",
        "$$\n",
        "\n",
        "*Write i in scalar form*\n",
        "\n",
        "$$\n",
        "p(h|v) = \\frac{1}{Z'} \\exp \\left (\n",
        "\\sum_{i=1}^D\\sum_{j=1}^M W_{ij}v_ih_j +\n",
        "\\sum_{i=1}^D b_iv_i +\n",
        "\\sum_{j=1}^M c_jh_j\n",
        "\\right )\\\\\n",
        "$$\n",
        "\n",
        "*Exponent rule*\n",
        "\n",
        "exp(A+B) = exp(A)exp(B)\n",
        "\n",
        "$$\n",
        "p(h|v) = \\frac{1}{Z'} \\exp \\left (\n",
        "\\sum_{i=1}^D b_iv_i \\right )\n",
        "\\prod_{j=1}^M \\exp \\left (\n",
        "\\sum_{i=1}^D\n",
        " W_{ij}v_ih_j +\n",
        " c_jh_j\n",
        "\\right )\\\\\n",
        "$$\n"
      ],
      "metadata": {
        "id": "a0KU2AadvMKU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Absorb Normalizing Constant*\n",
        "\n",
        "Anything that doesn't depend on h can be absorbed. p(h|v) only depends on h, so anything else can be absorbed to normalizing constant\n",
        "\n",
        "$$\n",
        "p(h|v) = \\frac{1}{Z''}\n",
        "\\prod_{j=1}^M \\exp \\left (\n",
        "\\sum_{i=1}^D\n",
        " W_{ij}v_ih_j +\n",
        " c_jh_j\n",
        "\\right )\\\\\n",
        "$$\n",
        "\n",
        "*Factor out $h_j$*\n",
        "\n",
        "$$\n",
        "p(h|v) = \\frac{1}{Z''}\n",
        "\\prod_{j=1}^M \\exp \\left (\n",
        "h_j \\left \\{\n",
        "\\sum_{i=1}^D\n",
        " W_{ij}v_i +\n",
        " c_j\n",
        " \\right \\}\n",
        "\\right )\\\\\n",
        "$$\n",
        "\n",
        "*Independence*\n",
        "\n",
        "If A and B are independent, then p(A,B) = p(A)p(B)\n",
        "\n",
        "p(h|v) factors out where each $p(h_j|v)$ is independent of the others. Hidden units can't connect to other hidden units.\n",
        "\n",
        "$$\n",
        "p(h_j|v) = \\frac{1}{Z'''} \\exp \\left (\n",
        "h_j \\left \\{\n",
        "\\sum_{i=1}^D\n",
        " W_{ij}v_i +\n",
        " c_j\n",
        " \\right \\}\n",
        "\\right )\\\\\n",
        "$$\n",
        "\n",
        "*Bernoulli rule*\n",
        "\n",
        "$h_j$ can only ever be 0 or 1\n",
        "\n",
        "$$\n",
        "p(h_j = 1|v) = \\frac{1}{Z'''} \\exp \\left (\n",
        "\\sum_{i=1}^D\n",
        " W_{ij}v_i +\n",
        " c_j\n",
        "\\right )\n",
        "\\\\p(h_j=0|v) = \\frac{1}{Z'''}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "fcFJCjTQ41W8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*Solve*\n",
        "\n",
        "$$\n",
        "p(h_j = 1|v) + p(h_j = 0|v)  = 1 \\\\\n",
        "\\frac{1}{Z'''} \\exp \\left (\\sum_{i=1}^DW_{ij}v_i +c_j\\right ) + \\frac{1}{Z'''} = 1 \\\\\n",
        "Z''' = 1 + \\exp\\left (\\sum_{i=1}^DW_{ij}v_i +c_j\\right )\n",
        "$$\n",
        "\n",
        "*Plug back to original equation*\n",
        "\n",
        "$$\n",
        "p(h_j = 1|v)  = \\frac{\n",
        "\\exp\\left (\\sum_{i=1}^DW_{ij}v_i +c_j\\right )\n",
        "}{\n",
        "1 + \\exp\\left (\\sum_{i=1}^DW_{ij}v_i +c_j\\right )\n",
        "}  =\n",
        "\\sigma\\left (\\sum_{i=1}^DW_{ij}v_i +c_j\\right )\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "*Full Vector Form*\n",
        "\n",
        "$$\n",
        "p(h = 1|v)  = \\sigma\\left ( W^Tv + c \\right )\n",
        "$$\n",
        "\n",
        "\n",
        "Equations are symmetric in v and h\n"
      ],
      "metadata": {
        "id": "X-C-jWmw4x_n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resources\n",
        "\n",
        "- [Recommender Systems and Deep Learning in Python by LazyProgrammer](https://deeplearningcourses.com/c/recommender-systems)"
      ],
      "metadata": {
        "id": "IiWhdb8CU7jE"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}