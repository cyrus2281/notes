{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/notes/blob/main/MachineLearning/Recommenders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "COYGNbvomdLn"
      },
      "source": [
        "# Recommenders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "toc",
        "id": "cRJxEtXoOomo"
      },
      "source": [
        ">[Recommenders](#scrollTo=COYGNbvomdLn)\n",
        "\n",
        ">>[Mathematical Recommenders](#scrollTo=X7LG4JFYmfH9)\n",
        "\n",
        ">>>[Lift](#scrollTo=-NPdgLKNkwA2)\n",
        "\n",
        ">>>[Hacker News Formula](#scrollTo=bNo3EqUbmjRp)\n",
        "\n",
        ">>>[Reddit Formula](#scrollTo=OX9Eah-ppQoq)\n",
        "\n",
        ">>>[Google Page Rank](#scrollTo=cCDmUR3Fp0YC)\n",
        "\n",
        ">>>>[Markov Models](#scrollTo=bIU2zR8LtEfH)\n",
        "\n",
        ">>>>[Transition Probability Matrix](#scrollTo=UENDikH0tHHt)\n",
        "\n",
        ">>>>[Calculating Probabilities](#scrollTo=Cbn1ogLEtKl_)\n",
        "\n",
        ">>>>[Beta Posterior Mean](#scrollTo=-PvC0vn1xck2)\n",
        "\n",
        ">>>>[State Distribution](#scrollTo=pS0RySi_1Ecy)\n",
        "\n",
        ">>>>[PageRank](#scrollTo=Q640uQxg3_sJ)\n",
        "\n",
        ">>[Statistics](#scrollTo=Ivwe3u2lzHp3)\n",
        "\n",
        ">>>[Smoothing (Dampening)](#scrollTo=S34-RUx8zI_f)\n",
        "\n",
        ">>>[Explore-Exploit Dilemma](#scrollTo=gahD3CwG1XEv)\n",
        "\n",
        ">>>[Bayesian Method](#scrollTo=N4WrwUfO6MUp)\n",
        "\n",
        ">>[Collaborative Filtering](#scrollTo=biqlBm7295Kg)\n",
        "\n",
        ">>>[Sparsity](#scrollTo=VHSIzt7MLEya)\n",
        "\n",
        ">>>[Regression](#scrollTo=pmcJqzBwNcuy)\n",
        "\n",
        ">>>[User-User Collaborative Filtering](#scrollTo=XlrdEcfgxdmH)\n",
        "\n",
        ">>>>[Pearson Correlation Coefficient](#scrollTo=LJzwxoPl2ayu)\n",
        "\n",
        ">>>>[Cosine Similarity](#scrollTo=B2ZTwhaW4o-v)\n",
        "\n",
        ">>>>[Python Implementation](#scrollTo=7fXxflLq7YCc)\n",
        "\n",
        ">>>[Item-Item Collaborative Filtering](#scrollTo=RVJUmoIsVf11)\n",
        "\n",
        ">>>>[Python Implementation](#scrollTo=QSNW575oXYDB)\n",
        "\n",
        ">>>[Comparison](#scrollTo=-Hewxxx2C7sS)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7LG4JFYmfH9"
      },
      "source": [
        "## Mathematical Recommenders\n",
        "\n",
        "These examples are non-personalized recommendations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NPdgLKNkwA2"
      },
      "source": [
        "### Lift\n",
        "\n",
        "$$\n",
        "\\text{Lift} = \\frac{p(A,B)}{p(A)p(B)} = \\frac{p(A|B)}{p(A)} = \\frac{p(B|A)}{p(B)}\n",
        "$$\n",
        "\n",
        "- Symmetric\n",
        "- If A and B are independent, then $p(A|B) = p(A)$\n",
        "  - $p(A|B) / p(A) = 1$\n",
        "- if increasing the probability of B increases the probability of A, then Lift > 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNo3EqUbmjRp"
      },
      "source": [
        "### Hacker News Formula\n",
        "\n",
        "Balancing Popularirty with Age\n",
        "\n",
        "$$\n",
        "\\frac{f(\\text{popularity})}{g(\\text{age})} \\\\[1cm]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{score} = \\frac{(\\text{ups} - \\text{downs} -1 )^{0.8}}{(\\text{age}+2)^{\\text{gravity}}} \\times \\text{penalty}\n",
        "$$\n",
        "\n",
        "- gravity = 1.8\n",
        "- penalty = multiplier to implement \"business rules\" (e.g. penalize self-posts, \"controversial\" posts, + many more rules)\n",
        "\n",
        "\\\n",
        "\n",
        "age starts from 2, to prevent division by zero\n",
        "\n",
        "exponent of numerator is bigger than the exponent of the denominator, meaning denominator grows faster.\n",
        "\n",
        "Age always overtakes popularity\n",
        "\n",
        "\\\n",
        "\n",
        "exponent 0.8 causes sublinear growth.\n",
        "meaning 0 → 100 worth more than 1000 → 1100\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OX9Eah-ppQoq"
      },
      "source": [
        "### Reddit Formula\n",
        "\n",
        "$$\n",
        "\\text{score} = \\text{sign}(\\text{ups}-\\text{down}) \\times \\log \\{ \\max(1, |\\text{ups} - \\text{downs}|) \\} + \\frac{\\text{age}}{45000}\n",
        "$$\n",
        "\n",
        "- log of the absoulte value of net votes - sublinear curve - initial votes matter more - max since log 0 is not possible\n",
        "\n",
        "\\\n",
        "\n",
        "- can be positive or negative\n",
        "  - The more downvotes you get, the futher your score goes down\n",
        "\n",
        "\\\n",
        "\n",
        "- age is in seconds from inception of reddit\n",
        "- age is always positive\n",
        "- newer links → more score\n",
        "- reddit scores will forever increase linearly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cCDmUR3Fp0YC"
      },
      "source": [
        "### Google Page Rank\n",
        "\n",
        "Logic: The page rank of a page is the probability I would end up on that page if I surfed the internet randomly for an infinite amount of time\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIU2zR8LtEfH"
      },
      "source": [
        "#### Markov Models\n",
        "\n",
        "- Simplest way to think about Markov Models are bigrams from NLP\n",
        "- Build a probablistic language model\n",
        "- Can ask \"what is the probability of the next word in the sentence 'love' give the previous word was 'I'?\" i.e., p(love | I )\n",
        "\n",
        "**Bigrams**\n",
        "\n",
        "- It's a bigram because we only consider 2 words at a time\n",
        "\n",
        "We don't have to think of each item as a word, just a generic state: $x(t)$\n",
        "\n",
        "\"Markov\" means $x(t)$ doesn't depend on any values 2 or more steps behind, only the immediate last value.\n",
        "$$\n",
        "p(x_t | x_{t-1}, x_{t-2}, \\cdots, x_1) = p(x_t | x_{t-1})\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UENDikH0tHHt"
      },
      "source": [
        "#### Transition Probability Matrix\n",
        "\n",
        "- $A(i,j)$ tells use the probability of going to state j from state i\n",
        "$$\n",
        "A(i,j) = p(x_t = j | x_{t-1} = i )\n",
        "$$\n",
        "\n",
        "- Key: rows must sum to 1\n",
        "    - Since it's a probability this must be true\n",
        "    - If true, A is called a \"stochastic matrix\" or \"Markov Matrix\"\n",
        "$$\n",
        "\\sum^M_{j=1}A(i,j) = \\sum^M_{j=1}p(x_t=j|x_{t-1}=i) =1\n",
        "$$\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "Example:\n",
        "\n",
        "Weather is\n",
        "- state 1 = Sunny\n",
        "- state 2 = Rainy\n",
        "\n",
        "Suppose:\n",
        "- p( sunny | sunny ) = 0.9\n",
        "- p( sunny | rainy ) = 0.1\n",
        "- p( rainy | sunny ) = 0.1\n",
        "- p( rainy | rainy ) = 0.9\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cbn1ogLEtKl_"
      },
      "source": [
        "#### Calculating Probabilities\n",
        "\n",
        "$$\n",
        "p(\\text{rainy} | \\text{sunny}) = \\frac{\\text{count}(\\text{sunny} \\rightarrow \\text{rainy})}{\\text{count}(\\text{sunny})}\n",
        "$$\n",
        "\n",
        "Generalized\n",
        "\n",
        "$$\n",
        "p(B|A) = \\frac{\\text{count}(A \\rightarrow B)}{\\text{count}(A)}\n",
        "$$\n",
        "\n",
        "Now probability of a sentence would be\n",
        "\n",
        "$$\n",
        "p(x_1, \\cdots, x_T) = p(x_1) \\prod^T_{t=2} p(x_t | x_{t-1})\n",
        "$$\n",
        "\n",
        "Problem: If a bigram didn't apear in the train set, the probility would be 0 and anything × 0 is 0.\n",
        "\n",
        "**Add-1 Smoothing**\n",
        "\n",
        "$$\n",
        "p(x_t=j|x_{t-1}=i) = \\frac{\\text{count}(i \\rightarrow j)+\\epsilon}{\\text{count}(i)+\\epsilon V} \\\\\n",
        "$$\n",
        "\n",
        "- Add a \"fake count\" to every possible bigram\n",
        "  - ϵ can be any value, for example 1.\n",
        "- V = Vocabulary size = number of unique words in dataset\n",
        "- In this case, V=M (number of states) since each state is a word\n",
        "\n",
        "\n",
        "e.g. p(and | and) never occurs but would get positve probability\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-PvC0vn1xck2"
      },
      "source": [
        "#### Beta Posterior Mean\n",
        "\n",
        "In our case, the equation is the beta posterior mean instead of only 2 possible outcomes, V possible outcomts\n",
        "\n",
        "$$\n",
        "E(\\pi) = \\frac{\\alpha'}{\\alpha'+\\beta'} = \\frac{\\alpha+(\\sum_{i=1}^N X_i)}{\\alpha + \\beta + N} \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pS0RySi_1Ecy"
      },
      "source": [
        "#### State Distribution\n",
        "\n",
        "$\\pi_t$ = state probability distribution at time t\n",
        "\n",
        "$\\pi(t)$ is a row vector by convention\n",
        "\n",
        "For the weather example,\n",
        "\n",
        "$$\n",
        "\\pi_t = [p(x_t = \\text{sunny}), p(x_t = \\text{rainy})]\n",
        "$$\n",
        "\n",
        "**Future State Distribution**\n",
        "\n",
        "Calculating the $\\pi(t+1)$ use Bayes rule\n",
        "\n",
        "$$\n",
        "p(x_{t+1} = j) = \\sum^M_{i=1} p(x_{t+1} = j, x_t =i) \\\\\n",
        "= \\sum^M_{i=1} p(x_{t+1}=j | x_t = i) p(x_t = i) \\\\\n",
        "= \\sum^M_{i=1} A(i,j)\\pi(i) \\\\\n",
        "= \\pi_{t+1}(j)\n",
        "$$\n",
        "\n",
        "Since A is a matrix and $\\pi(t)$ is a vector, we can express it in terms of matrix math\n",
        "$$\n",
        "\\pi_{x+1}(j) = \\sum^M_{i=1} A(i,j)\\pi_t(i) \\\\\n",
        "\\pi_{t+1} = \\pi_t A\n",
        "$$\n",
        "\n",
        "Further future\n",
        "\n",
        "$$\n",
        "\\pi_{t+2} = \\pi_t A^2 \\\\\n",
        "\\pi_{t+k} = \\pi_t A^k \\\\\n",
        "$$\n",
        "\n",
        "For infinity\n",
        "\n",
        "$$\n",
        "\\pi_\\infty = \\lim_{t\\rightarrow \\infty} \\pi_0 A^t \\\\\n",
        "\\pi_\\infty = \\pi_\\infty A\n",
        "$$\n",
        "\n",
        "This is just the eigenvalue problem\n",
        "  - Give matrix A, find a vector and a scalar s.t. multiplying the vector by A is equivalent to stretching it be the scalar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q640uQxg3_sJ"
      },
      "source": [
        "#### PageRank\n",
        "\n",
        "Every page on the internet is a state in a Markov Model\n",
        "\n",
        "The transition probablity is distributed equally amongst all links on a page\n",
        "- p(dlc.com | lp.me ) = 0.5\n",
        "- p(yt.com | lp.me ) = 0.5\n",
        "\n",
        "In general, we can write the transition probability as:\n",
        "\n",
        "$$\n",
        "p(x_t =j | x_{x-1} = i) = \\frac{1}{n(i)}\n",
        "$$\n",
        "if $i$ links to $j$, $n(i) = $ number of links on page $i$, otherwise $0$.\n",
        "\n",
        "**Smoothing**\n",
        "\n",
        "$$\n",
        "G = 0.85A + 0.15U \\\\\n",
        "U(i,j) = \\frac{1}{M} \\\\\n",
        "\\forall i,j = 1 \\dots M\n",
        "$$\n",
        "\n",
        "Find the limiting distribution of G - yields a vector of length M - these probabilities are the respective PageRanks for ech page on the internet\n",
        "\n",
        "$$\n",
        "\\pi_\\infty = \\pi_\\infty G\n",
        "$$\n",
        "\n",
        "\n",
        "**Perron-Frobenius Theorem**:\n",
        "> If G is a valid Markov matrix and all its elements are positive then the stationary distribution and limiting distribution are the same\n",
        "- Limiting Distribution: state distribution you'd arrive at after transitioning by G an infinite number of times\n",
        "- Stationary Distribution: a state distribution that does not change after transitioning by G"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ivwe3u2lzHp3"
      },
      "source": [
        "## Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S34-RUx8zI_f"
      },
      "source": [
        "### Smoothing (Dampening)\n",
        "\n",
        "To resolve the issue of 0 in sample data when getting mean\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum^N_{i=1} X_i + \\lambda \\mu_0}{N+\\lambda}\n",
        "$$\n",
        "\n",
        "\n",
        "- $\\lambda$ some random small non-zero number\n",
        "- $\\mu_0$ the global avergage or just some middle value\n",
        "\n",
        "\\\n",
        "\n",
        "For example:\n",
        "- 1000 reviews of 4 star - μ = 3 - λ = 1 → 3.999\n",
        "- 5 reviews of 4 star - μ = 3 - λ = 1 → 3.83\n",
        "- 1 review of 4 star - μ = 3 - λ = 1 → 3.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gahD3CwG1XEv"
      },
      "source": [
        "### Explore-Exploit Dilemma\n",
        "\n",
        "Example 1\n",
        "\n",
        "Imagine we want to find the slot machine with the highest win rate among 10 slot machines.\n",
        "\n",
        "Traditional statistical test can tell us whether or not there's a significant difference between win rates between machines.\n",
        "\n",
        "If playing each machine 100 times, meaning 1000 turns total, 900 (9/10) turns yielded a suboptimal reward.\n",
        "\n",
        "Hence the dilemma, Play more or play less!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Example 2\n",
        "\n",
        "Watching a bunach of YouTube videos on how to make eggs.\n",
        "\n",
        "Now your reccomendations are filled with videos about making eggs\n",
        "\n",
        "porbably suboptimal - once I've figured out how to make eggs, I don't want to watch more egg videos.\n",
        "\n",
        "YouTube is not exploiting the fact that I watched eggs video and not exploring other topics\n",
        "\n",
        "Should there be a stronger exploration component?\n",
        "\n",
        "Maybe I'd like to seE movie trailers or machine learning videos\n",
        "\n",
        "---\n",
        "\n",
        "How do we strike a balance between these 2 opposing forces?\n",
        "\n",
        "Smoothed average gives us one part of the solution\n",
        "\n",
        "Making good things look worse and bad things look better\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N4WrwUfO6MUp"
      },
      "source": [
        "### Bayesian Method\n",
        "\n",
        "Bayesian method automatically balances need to explore and exploit\n",
        "\n",
        "- 2 fat distributions: explore both (totally random ranking)\n",
        "- 2 skinny distributions: exploit both (nearly deterministic ranking)\n",
        "- Mixed: explore and exploit co-exit\n",
        "\n",
        "\n",
        "Completely automatic - does not require A/B testing\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biqlBm7295Kg"
      },
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "Non-specific to any particular user, Score each item from 1 to M a number.\n",
        "\n",
        "Basic algorithm is to make s(j) the average rating for j\n",
        "\n",
        "$$\n",
        "s(j) = \\frac{\\sum_{i \\in \\Omega_j} r_{ij}}{|\\Omega_j|} \\\\\n",
        "$$\n",
        "\n",
        "- $\\Omega_j$ = set of all users who rated item j\n",
        "- $r_{ij}$ = rating user i gave item j\n",
        "\n",
        "Translates to, average rating for a product is the sum of rating divided by the number of the ratings.\n",
        "\n",
        "**Personalize the score**\n",
        "\n",
        "s(i,j) can depend both on user i and item j\n",
        "\n",
        "$$\n",
        "s(i,j) = \\frac{\\sum_{i' \\in \\Omega_j} r_{i'j}}{|\\Omega_j|} \\\\\n",
        "$$\n",
        "\n",
        "i' is just an index\n",
        "\n",
        "i = 1 … N, N = number of users\n",
        "\n",
        "j = 1 … M, M = number of items\n",
        "\n",
        "$R_{N\\times M}$ = user -item ratings matrix of size N × M\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "- User-item matrix is reminiscent of term-document.\n",
        "- X(t,d) = # of time term t appears in document d\n",
        "- In terms of recommender systems, can think of X(t,d) as \"how much does t like the item d\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHSIzt7MLEya"
      },
      "source": [
        "### Sparsity\n",
        "\n",
        "One characteristic of the user items matrix that makes it unique to recommender systems is that it's Sparse.\n",
        "\n",
        "- Term-document matrix is sparse because most entries are 0\n",
        "- User-item matrix is sparse because most entries are **empty**\n",
        "\n",
        "The average user does not interact with all items.\n",
        "\n",
        "\n",
        "**Goal of Collaborative Filtering**\n",
        "\n",
        "- Most of r(i,j) doesn't exist - this is good.\n",
        "\n",
        "If every user has seen every item, then there's nothing to recommend\n",
        "\n",
        "Goal:\n",
        "> We want to guess what you might rate an item you haven't seen yet\n",
        "\n",
        "$$\n",
        "s(i,j) = \\hat r (i,j) = \\text{ guess what user i might rate item j}\n",
        "$$\n",
        "\n",
        "E.g. if we think you might rate some move a 5, we definitely want you to watch that movie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmcJqzBwNcuy"
      },
      "source": [
        "### Regression\n",
        "\n",
        "Since this is a regression probelm, the evaluation metric is going to be the mean squared error.\n",
        "\n",
        "Outline:\n",
        "- user-user collaborative filtering\n",
        "- item-item collaborative filtering\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{|\\Omega|} \\sum_{i,j \\in \\Omega}(r_{ij} - \\hat r_{ij})^2\n",
        "$$\n",
        "\n",
        "Ω = set of pairs (i,j) where user i has rated item j\n",
        "\n",
        "we're going to take our models predicted ratings, compare them to the actual ratings, square the difference and then take the average of those squared differences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlrdEcfgxdmH"
      },
      "source": [
        "### User-User Collaborative Filtering\n",
        "\n",
        "||item 1 | item2 | ... | item n |\n",
        "|--|--|--|--|--|\n",
        "|user 1| score |score | ... |score |\n",
        "|user 2| score |score | ... |score |\n",
        "| ... | ... |... | ... |... |\n",
        "|user n| score |score | ... |score |\n",
        "\n",
        "Comparing rows together, if 2 rows are very similar, it can be concluded that they have similar taste\n",
        "\n",
        "\n",
        "Average Rating reminder\n",
        "$\n",
        "s(i,j) = \\frac{\\sum_{i'\\in \\Omega_j} r_{i'j}}{|\\Omega_j|}\n",
        "$\n",
        "\n",
        "It treats everyone's rating of the items equally\n",
        "\n",
        "User 1's s(i,j) equally depends on user 2 rating and user 3 rating. even though user 1 doesn't agree with user 2.\n",
        "\n",
        "\n",
        "**Weighting Ratings**\n",
        "\n",
        "To make it small for users who don't agree and large for users who do agree\n",
        "\n",
        "$$\n",
        "s(i,j) = \\frac{\\sum_{i'\\in \\Omega_j} w_{ii'} r_{i'j}}{\\sum_{i'\\in\\Omega_j} w_{ii'}} \\\\\n",
        "$$\n",
        "\n",
        "Users can be biased, optimistic or perstimistic.\n",
        "\n",
        "**Deviation**\n",
        "\n",
        "Don't care about your absolute rating, but how much it deviates from your own average.\n",
        "- if your average is 2.5, but you rate something 5, it must be really great\n",
        "- if you rate everything a 5, it's diffcult to know how those items compare\n",
        "\n",
        "$$\n",
        "\\text{dev}(i,j) = r(i,j) - \\bar r_i, \\text{ for a known rating}\n",
        "$$\n",
        "\n",
        "My predicted rating is my own average + predicted deviation\n",
        "\n",
        "$$\n",
        "\\hat{\\text{dev}}(i,j) = \\frac{1}{|\\Omega_j|}\\sum_{i'\\in \\Omega_j} r(i',j) - \\bar r_{i'} \\\\\n",
        "$$\n",
        "\n",
        "For a prediction from known ratings\n",
        "\n",
        "$$\n",
        "s(i,j) = \\bar r_i + \\frac{1}{|\\Omega_j|}\\sum_{i'\\in \\Omega_j} r(i',j) - \\bar r_{i'} \\\\\n",
        "= \\bar r_i + \\hat{\\text{dev}}(i,j) \\\\\n",
        "$$\n",
        "\n",
        "Note: In order to make recommendation, I don't need to add back the average, because it's the same over all items\n",
        "\n",
        "**Combine**\n",
        "\n",
        "Combine the idea of deviations with the idea of weightings to get our final formula\n",
        "\n",
        "$$\n",
        "s(i,j) = \\bar r_i + \\frac{\\sum_{i'\\in \\Omega_j} w_{ii'} \\{ r_{i'j} - \\bar r_{i'} \\}}{\\sum_{i'\\in \\Omega_j} |w_{ii'}|}  \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJzwxoPl2ayu"
      },
      "source": [
        "How to calculate wieghts\n",
        "\n",
        "#### Pearson Correlation Coefficient\n",
        "\n",
        "$$\n",
        "\\varrho_{xy} = \\frac\n",
        "{\\sum^N_{i=1} (x_i -\\bar x)(y_i - \\bar y)}\n",
        "{\n",
        "  \\sqrt{\\sum^N_{i=1} (x_i - \\bar x)^2 }\n",
        "  \\sqrt{\\sum^N_{i=1} (y_i - \\bar y)^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "Our data is sparse, meaning we have a lot of missing data.\n",
        "\n",
        "Update formula:\n",
        "\n",
        "$$\n",
        "w_{ii'} = \\frac\n",
        "{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar i_i)(r_{i'j} - \\bar r_{i'})}\n",
        "{\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar i_i)^2 }\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{i'j} -\\bar i_{i'})^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "- $\\Psi_i$ = set of items that user i has rated\n",
        "- $\\Psi_{ii'}$ = set of items that user i and i' have rated\n",
        "- $\\Psi_{ii'} = \\Psi_i \\cap \\Psi_{i'}$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2ZTwhaW4o-v"
      },
      "source": [
        "#### Cosine Similarity\n",
        "\n",
        "$$\n",
        "\\cos \\theta = \\frac{x^Ty}{|x| \\ |y|} = \\frac\n",
        "{\\sum_{i=1}^N x_iy_i}\n",
        "{\n",
        "  \\sqrt{\\sum_{i=1}^N x_i^2 }\n",
        "  \\sqrt{\\sum_{i=1}^N y_i^2 }\n",
        "} \\\\\n",
        "$$\n",
        "\n",
        "They are the same, execpt pearson is centered.\n",
        "\n",
        "We want to center them anyway because we're working with deviations, not absolute ratings\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "If 2 users have zero or very few items in common, we don't want to consider them in the calculation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1QrBRRp5mq_"
      },
      "source": [
        "**Neighborhood**\n",
        "\n",
        "In practice, don't sum over all users who rated item j (takes too long)\n",
        "\n",
        "- It can help to precompute weights beforehand\n",
        "- Instead of summing over all users, take the ones with highest weight\n",
        "  - E.g. use K nearest neighbors, K=25 upto 50\n",
        "\n",
        "\\\n",
        "\n",
        "In Summary,\n",
        "Discard users w/ no items in common, or few items. Keep only users whose weights are high"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fXxflLq7YCc"
      },
      "source": [
        "#### Python Implementation\n",
        "\n",
        "Outline\n",
        "- Split data into train and test sets\n",
        "- Calculate weightsss using train set\n",
        "- Make a predict function, e.g. score ← predict(i,j)\n",
        "- Output MSE for train and test sets\n",
        "\n",
        "\n",
        "Data fetched from [MovieLens 20M Dataset](https://www.kaggle.com/datasets/grouplens/movielens-20m-dataset) - File `rating.csv`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmytnrgSPqmg"
      },
      "outputs": [],
      "source": [
        "LARGE_FILE_DIR  = \"MachineLearning/notes/large_files\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parsing IDs"
      ],
      "metadata": {
        "id": "eEZTqNZgP_3r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y01Or58a9kt_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(LARGE_FILE_DIR + \"/rating.csv\")\n",
        "\n",
        "# note:\n",
        "# user ids are ordered sequentially from 1..138493\n",
        "# with no missing numbers\n",
        "# movie ids are integers from 1..131262\n",
        "# NOT all movie ids appear\n",
        "# there are only 26744 movie ids\n",
        "\n",
        "# make the user ids go from 0...N-1\n",
        "df.userId = df.userId - 1\n",
        "\n",
        "# create a mapping for movie ids\n",
        "unique_movie_ids = set(df.movieId.values)\n",
        "movie2idx = {}\n",
        "count = 0\n",
        "for movie_id in unique_movie_ids:\n",
        "  movie2idx[movie_id] = count\n",
        "  count += 1\n",
        "\n",
        "# add them to the data frame\n",
        "# takes awhile\n",
        "df['movie_idx'] = df.apply(lambda row: movie2idx[row.movieId], axis=1)\n",
        "\n",
        "df = df.drop(columns=['timestamp'])\n",
        "\n",
        "df.to_csv(LARGE_FILE_DIR + '/edited_rating.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shrinking data size"
      ],
      "metadata": {
        "id": "0M40cyyvQBf0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e-Fvw4ZPqmh",
        "outputId": "d2a39be8-ceb3-4f05-e75b-4810766fccc3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "original dataframe size: 20000263\n",
            "i: 10000\n",
            "j: 2000\n",
            "Setting new ids\n",
            "max user id: 9999\n",
            "max movie id: 1999\n",
            "small dataframe size: 5392025\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n",
        "# load in the data\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/edited_rating.csv')\n",
        "print(\"original dataframe size:\", len(df))\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "user_ids_count = Counter(df.userId)\n",
        "movie_ids_count = Counter(df.movie_idx)\n",
        "\n",
        "# number of users and movies we would like to keep\n",
        "n = 10000\n",
        "m = 2000\n",
        "\n",
        "user_ids = [u for u, c in user_ids_count.most_common(n)]\n",
        "movie_ids = [m for m, c in movie_ids_count.most_common(m)]\n",
        "\n",
        "# make a copy, otherwise ids won't be overwritten\n",
        "df_small = df[df.userId.isin(user_ids) & df.movie_idx.isin(movie_ids)].copy()\n",
        "\n",
        "# need to remake user ids and movie ids since they are no longer sequential\n",
        "new_user_id_map = {}\n",
        "i = 0\n",
        "for old in user_ids:\n",
        "  new_user_id_map[old] = i\n",
        "  i += 1\n",
        "print(\"i:\", i)\n",
        "\n",
        "new_movie_id_map = {}\n",
        "j = 0\n",
        "for old in movie_ids:\n",
        "  new_movie_id_map[old] = j\n",
        "  j += 1\n",
        "print(\"j:\", j)\n",
        "\n",
        "print(\"Setting new ids\")\n",
        "df_small.loc[:, 'userId'] = df_small.apply(lambda row: new_user_id_map[row.userId], axis=1)\n",
        "df_small.loc[:, 'movie_idx'] = df_small.apply(lambda row: new_movie_id_map[row.movie_idx], axis=1)\n",
        "# df_small.drop(columns=['userId', 'movie_idx'])\n",
        "# df_small.rename(index=str, columns={'new_userId': 'userId', 'new_movie_idx': 'movie_idx'})\n",
        "print(\"max user id:\", df_small.userId.max())\n",
        "print(\"max movie id:\", df_small.movie_idx.max())\n",
        "\n",
        "print(\"small dataframe size:\", len(df_small))\n",
        "df_small.to_csv(LARGE_FILE_DIR + '/small_rating.csv', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating user-movie-rating data structures"
      ],
      "metadata": {
        "id": "3zBimvPKQF0b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZvEbyS7Pqmh",
        "outputId": "36694632-74fd-413f-bd07-82e20d0967be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calling: update_user2movie_and_movie2user\n",
            "processed: 0.023\n",
            "processed: 0.046\n",
            "processed: 0.070\n",
            "processed: 0.093\n",
            "processed: 0.116\n",
            "processed: 0.139\n",
            "processed: 0.162\n",
            "processed: 0.185\n",
            "processed: 0.209\n",
            "processed: 0.232\n",
            "processed: 0.255\n",
            "processed: 0.278\n",
            "processed: 0.301\n",
            "processed: 0.325\n",
            "processed: 0.348\n",
            "processed: 0.371\n",
            "processed: 0.394\n",
            "processed: 0.417\n",
            "processed: 0.440\n",
            "processed: 0.464\n",
            "processed: 0.487\n",
            "processed: 0.510\n",
            "processed: 0.533\n",
            "processed: 0.556\n",
            "processed: 0.580\n",
            "processed: 0.603\n",
            "processed: 0.626\n",
            "processed: 0.649\n",
            "processed: 0.672\n",
            "processed: 0.695\n",
            "processed: 0.719\n",
            "processed: 0.742\n",
            "processed: 0.765\n",
            "processed: 0.788\n",
            "processed: 0.811\n",
            "processed: 0.835\n",
            "processed: 0.858\n",
            "processed: 0.881\n",
            "processed: 0.904\n",
            "processed: 0.927\n",
            "processed: 0.950\n",
            "processed: 0.974\n",
            "processed: 0.997\n",
            "Calling: update_usermovie2rating_test\n",
            "processed: 0.093\n",
            "processed: 0.185\n",
            "processed: 0.278\n",
            "processed: 0.371\n",
            "processed: 0.464\n",
            "processed: 0.556\n",
            "processed: 0.649\n",
            "processed: 0.742\n",
            "processed: 0.835\n",
            "processed: 0.927\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "df = pd.read_csv(LARGE_FILE_DIR + '/small_rating.csv')\n",
        "\n",
        "N = df.userId.max() + 1 # number of users\n",
        "M = df.movie_idx.max() + 1 # number of movies\n",
        "\n",
        "# split into train and test\n",
        "df = shuffle(df)\n",
        "cutoff = int(0.8*len(df))\n",
        "df_train = df.iloc[:cutoff]\n",
        "df_test = df.iloc[cutoff:]\n",
        "\n",
        "# a dictionary to tell us which users have rated which movies\n",
        "user2movie = {}\n",
        "# a dicationary to tell us which movies have been rated by which users\n",
        "movie2user = {}\n",
        "# a dictionary to look up ratings\n",
        "usermovie2rating = {}\n",
        "print(\"Calling: update_user2movie_and_movie2user\")\n",
        "count = 0\n",
        "def update_user2movie_and_movie2user(row):\n",
        "  global count\n",
        "  count += 1\n",
        "  if count % 100000 == 0:\n",
        "    print(\"processed: %.3f\" % (float(count)/cutoff))\n",
        "\n",
        "  i = int(row.userId)\n",
        "  j = int(row.movie_idx)\n",
        "  if i not in user2movie:\n",
        "    user2movie[i] = [j]\n",
        "  else:\n",
        "    user2movie[i].append(j)\n",
        "\n",
        "  if j not in movie2user:\n",
        "    movie2user[j] = [i]\n",
        "  else:\n",
        "    movie2user[j].append(i)\n",
        "\n",
        "  usermovie2rating[(i,j)] = row.rating\n",
        "df_train.apply(update_user2movie_and_movie2user, axis=1)\n",
        "\n",
        "# test ratings dictionary\n",
        "usermovie2rating_test = {}\n",
        "print(\"Calling: update_usermovie2rating_test\")\n",
        "count = 0\n",
        "def update_usermovie2rating_test(row):\n",
        "  global count\n",
        "  count += 1\n",
        "  if count % 100000 == 0:\n",
        "    print(\"processed: %.3f\" % (float(count)/len(df_test)))\n",
        "\n",
        "  i = int(row.userId)\n",
        "  j = int(row.movie_idx)\n",
        "  usermovie2rating_test[(i,j)] = row.rating\n",
        "df_test.apply(update_usermovie2rating_test, axis=1)\n",
        "\n",
        "# note: these are not really JSONs\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'wb') as f:\n",
        "  pickle.dump(user2movie, f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'wb') as f:\n",
        "  pickle.dump(movie2user, f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'wb') as f:\n",
        "  pickle.dump(usermovie2rating, f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'wb') as f:\n",
        "  pickle.dump(usermovie2rating_test, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTNXFKR6Pqmi"
      },
      "source": [
        "Training and prediction\n",
        "\n",
        "\\\n",
        "\n",
        "Weights:\n",
        "$$\n",
        "w_{ii'} = \\frac\n",
        "{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar r_i)(r_{i'j} - \\bar r_{i'})}\n",
        "{\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{ij} -\\bar r_i)^2 }\n",
        "  \\sqrt{\\sum_{j\\in \\Psi_{ii'}} (r_{i'j} -\\bar r_{i'})^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "Predict:\n",
        "$$\n",
        "s(i,j) = \\bar r_i + \\frac{\\sum_{i'\\in \\Omega_j} w_{ii'} \\{ r_{i'j} - \\bar r_{i'} \\}}{\\sum_{i'\\in \\Omega_j} |w_{ii'}|}  \\\\\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRu8mqwjPqmi"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from sortedcontainers import SortedList\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "if N > 10000:\n",
        "  print(\"N =\", N, \"are you sure you want to continue?\")\n",
        "  print(\"Comment out these lines if so...\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "# to find the user similarities, you have to do O(N^2 * M) calculations!\n",
        "# in the \"real-world\" you'd want to parallelize this\n",
        "# note: we really only have to do half the calculations, since w_ij is symmetric\n",
        "K = 25 # number of neighbors we'd like to consider\n",
        "limit = 5 # number of common movies users must have in common in order to consider\n",
        "neighbors = [] # store neighbors in this list\n",
        "averages = [] # each user's average rating for later use\n",
        "deviations = [] # each user's deviation for later use\n",
        "for i in range(N):\n",
        "  # find the 25 closest users to user i\n",
        "  movies_i = user2movie[i]\n",
        "  movies_i_set = set(movies_i)\n",
        "\n",
        "  # calculate avg and deviation\n",
        "  ratings_i = { movie:usermovie2rating[(i, movie)] for movie in movies_i }\n",
        "  avg_i = np.mean(list(ratings_i.values()))\n",
        "  dev_i = { movie:(rating - avg_i) for movie, rating in ratings_i.items() }\n",
        "  dev_i_values = np.array(list(dev_i.values()))\n",
        "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
        "\n",
        "  # save these for later use\n",
        "  averages.append(avg_i)\n",
        "  deviations.append(dev_i)\n",
        "\n",
        "  sl = SortedList()\n",
        "  for j in range(N):\n",
        "    # don't include yourself\n",
        "    if j != i:\n",
        "      movies_j = user2movie[j]\n",
        "      movies_j_set = set(movies_j)\n",
        "      common_movies = (movies_i_set & movies_j_set) # intersection\n",
        "      if len(common_movies) > limit:\n",
        "        # calculate avg and deviation\n",
        "        ratings_j = { movie:usermovie2rating[(j, movie)] for movie in movies_j }\n",
        "        avg_j = np.mean(list(ratings_j.values()))\n",
        "        dev_j = { movie:(rating - avg_j) for movie, rating in ratings_j.items() }\n",
        "        dev_j_values = np.array(list(dev_j.values()))\n",
        "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
        "\n",
        "        # calculate correlation coefficient\n",
        "        numerator = sum(dev_i[m]*dev_j[m] for m in common_movies)\n",
        "        w_ij = numerator / (sigma_i * sigma_j)\n",
        "\n",
        "        # insert into sorted list and truncate\n",
        "        # negate weight, because list is sorted ascending\n",
        "        # maximum value (1) is \"closest\"\n",
        "        sl.add((-w_ij, j))\n",
        "        if len(sl) > K:\n",
        "          del sl[-1]\n",
        "\n",
        "  # store the neighbors\n",
        "  neighbors.append(sl)\n",
        "\n",
        "  # print out useful things\n",
        "  if i % 1 == 0:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# using neighbors, calculate train and test MSE\n",
        "\n",
        "def predict(i, m):\n",
        "  # calculate the weighted sum of deviations\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "  for neg_w, j in neighbors[i]:\n",
        "    # remember, the weight is stored as its negative\n",
        "    # so the negative of the negative weight is the positive weight\n",
        "    try:\n",
        "      numerator += -neg_w * deviations[j][m]\n",
        "      denominator += abs(neg_w)\n",
        "    except KeyError:\n",
        "      # neighbor may not have rated the same movie\n",
        "      # don't want to do dictionary lookup twice\n",
        "      # so just throw exception\n",
        "      pass\n",
        "\n",
        "  if denominator == 0:\n",
        "    prediction = averages[i]\n",
        "  else:\n",
        "    prediction = numerator / denominator + averages[i]\n",
        "  prediction = min(5, prediction)\n",
        "  prediction = max(0.5, prediction) # min rating is 0.5\n",
        "  return prediction\n",
        "\n",
        "\n",
        "train_predictions = []\n",
        "train_targets = []\n",
        "for (i, m), target in usermovie2rating.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(i, m)\n",
        "\n",
        "  # save the prediction and target\n",
        "  train_predictions.append(prediction)\n",
        "  train_targets.append(target)\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "# same thing for test set\n",
        "for (i, m), target in usermovie2rating_test.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(i, m)\n",
        "\n",
        "  # save the prediction and target\n",
        "  test_predictions.append(prediction)\n",
        "  test_targets.append(target)\n",
        "\n",
        "\n",
        "# calculate accuracy\n",
        "def mse(p, t):\n",
        "  p = np.array(p)\n",
        "  t = np.array(t)\n",
        "  return np.mean((p - t)**2)\n",
        "\n",
        "print('train mse:', mse(train_predictions, train_targets))\n",
        "print('test mse:', mse(test_predictions, test_targets))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Item-Item Collaborative Filtering\n",
        "\n"
      ],
      "metadata": {
        "id": "RVJUmoIsVf11"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find out if 2 items are similar\n",
        "\n",
        "\n",
        "\n",
        "$$\n",
        "w_{jj'} = \\frac\n",
        "{\\sum_{i\\in \\Omega_{jj'}} (r_{ij} -\\bar r_j)(r_{ij'} - \\bar r_{j'})}\n",
        "{\n",
        "  \\sqrt{\\sum_{i\\in \\Omega_{jj'}} (r_{ij} -\\bar r_j)^2 }\n",
        "  \\sqrt{\\sum_{j\\in \\Omega_{jj'}} (r_{ij'} -\\bar r_{j'})^2 }\n",
        "}\n",
        "$$\n",
        "\n",
        "- $\\Omega_j$ users who rated item j\n",
        "- $\\Omega_{jj'}$ users who rated item j and item j'\n",
        "- $\\bar r_j$ avergae rating for item j\n",
        "\n",
        "\n",
        "Formula:\n",
        "\n",
        "$$\n",
        "s(i,j) = \\bar r_j + \\frac{\\sum_{j'\\in \\Psi_i} w_{jj'} \\{ r_{ij'} - \\bar r_{j'} \\}}{\\sum_{j'\\in \\Psi_i} |w_{jj'}|}  \\\\\n",
        "$$\n",
        "\n",
        "- $\\Psi_i$ items user i has rated\n",
        "\n",
        "Deviation: how much user i likes item j', compared to how much everyone else like j'\n",
        "\n",
        "If user i really likes j' (more than other users do) and j is similar to j' (weight is high), the user i probably likes j too."
      ],
      "metadata": {
        "id": "d4fCJtiVAwku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Python Implementation\n",
        "\n"
      ],
      "metadata": {
        "id": "QSNW575oXYDB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "eVrGLqqPXmkJ"
      },
      "outputs": [],
      "source": [
        "# LARGE_FILE_DIR  = \"MachineLearning/notes/large_files\"\n",
        "LARGE_FILE_DIR  = \".\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils import shuffle\n",
        "from datetime import datetime\n",
        "from sortedcontainers import SortedList\n",
        "\n",
        "with open(LARGE_FILE_DIR + '/user2movie.json', 'rb') as f:\n",
        "  user2movie = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/movie2user.json', 'rb') as f:\n",
        "  movie2user = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating.json', 'rb') as f:\n",
        "  usermovie2rating = pickle.load(f)\n",
        "with open(LARGE_FILE_DIR + '/usermovie2rating_test.json', 'rb') as f:\n",
        "  usermovie2rating_test = pickle.load(f)\n",
        "\n",
        "\n",
        "N = np.max(list(user2movie.keys())) + 1\n",
        "# the test set may contain movies the train set doesn't have data on\n",
        "m1 = np.max(list(movie2user.keys()))\n",
        "m2 = np.max([m for (u, m), r in usermovie2rating_test.items()])\n",
        "M = max(m1, m2) + 1\n",
        "print(\"N:\", N, \"M:\", M)\n",
        "\n",
        "if M > 2000:\n",
        "  print(\"N =\", N, \"are you sure you want to continue?\")\n",
        "  print(\"Comment out these lines if so...\")\n",
        "  exit()\n",
        "\n",
        "\n",
        "# to find the user similarities, you have to do O(M^2 * N) calculations!\n",
        "# in the \"real-world\" you'd want to parallelize this\n",
        "# note: we really only have to do half the calculations, since w_ij is symmetric\n",
        "K = 20 # number of neighbors we'd like to consider\n",
        "limit = 5 # number of common movies users must have in common in order to consider\n",
        "neighbors = [] # store neighbors in this list\n",
        "averages = [] # each item's average rating for later use\n",
        "deviations = [] # each item's deviation for later use\n",
        "\n",
        "for i in range(M):\n",
        "  # find the K closest items to item i\n",
        "  users_i = movie2user[i]\n",
        "  users_i_set = set(users_i)\n",
        "\n",
        "  # calculate avg and deviation\n",
        "  ratings_i = { user:usermovie2rating[(user, i)] for user in users_i }\n",
        "  avg_i = np.mean(list(ratings_i.values()))\n",
        "  dev_i = { user:(rating - avg_i) for user, rating in ratings_i.items() }\n",
        "  dev_i_values = np.array(list(dev_i.values()))\n",
        "  sigma_i = np.sqrt(dev_i_values.dot(dev_i_values))\n",
        "\n",
        "  # save these for later use\n",
        "  averages.append(avg_i)\n",
        "  deviations.append(dev_i)\n",
        "\n",
        "  sl = SortedList()\n",
        "  for j in range(M):\n",
        "    # don't include yourself\n",
        "    if j != i:\n",
        "      users_j = movie2user[j]\n",
        "      users_j_set = set(users_j)\n",
        "      common_users = (users_i_set & users_j_set) # intersection\n",
        "      if len(common_users) > limit:\n",
        "        # calculate avg and deviation\n",
        "        ratings_j = { user:usermovie2rating[(user, j)] for user in users_j }\n",
        "        avg_j = np.mean(list(ratings_j.values()))\n",
        "        dev_j = { user:(rating - avg_j) for user, rating in ratings_j.items() }\n",
        "        dev_j_values = np.array(list(dev_j.values()))\n",
        "        sigma_j = np.sqrt(dev_j_values.dot(dev_j_values))\n",
        "\n",
        "        # calculate correlation coefficient\n",
        "        numerator = sum(dev_i[m]*dev_j[m] for m in common_users)\n",
        "        w_ij = numerator / (sigma_i * sigma_j)\n",
        "\n",
        "        # insert into sorted list and truncate\n",
        "        # negate weight, because list is sorted ascending\n",
        "        # maximum value (1) is \"closest\"\n",
        "        sl.add((-w_ij, j))\n",
        "        if len(sl) > K:\n",
        "          del sl[-1]\n",
        "\n",
        "  # store the neighbors\n",
        "  neighbors.append(sl)\n",
        "\n",
        "  # print out useful things\n",
        "  if i % 1 == 0:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "# using neighbors, calculate train and test MSE\n",
        "\n",
        "def predict(i, u):\n",
        "  # calculate the weighted sum of deviations\n",
        "  numerator = 0\n",
        "  denominator = 0\n",
        "  for neg_w, j in neighbors[i]:\n",
        "    # remember, the weight is stored as its negative\n",
        "    # so the negative of the negative weight is the positive weight\n",
        "    try:\n",
        "      numerator += -neg_w * deviations[j][u]\n",
        "      denominator += abs(neg_w)\n",
        "    except KeyError:\n",
        "      # neighbor may not have been rated by the same user\n",
        "      # don't want to do dictionary lookup twice\n",
        "      # so just throw exception\n",
        "      pass\n",
        "\n",
        "  if denominator == 0:\n",
        "    prediction = averages[i]\n",
        "  else:\n",
        "    prediction = numerator / denominator + averages[i]\n",
        "  prediction = min(5, prediction)\n",
        "  prediction = max(0.5, prediction) # min rating is 0.5\n",
        "  return prediction\n",
        "\n",
        "\n",
        "\n",
        "train_predictions = []\n",
        "train_targets = []\n",
        "for (u, m), target in usermovie2rating.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(m, u)\n",
        "\n",
        "  # save the prediction and target\n",
        "  train_predictions.append(prediction)\n",
        "  train_targets.append(target)\n",
        "\n",
        "test_predictions = []\n",
        "test_targets = []\n",
        "# same thing for test set\n",
        "for (u, m), target in usermovie2rating_test.items():\n",
        "  # calculate the prediction for this movie\n",
        "  prediction = predict(m, u)\n",
        "\n",
        "  # save the prediction and target\n",
        "  test_predictions.append(prediction)\n",
        "  test_targets.append(target)\n",
        "\n",
        "\n",
        "# calculate accuracy\n",
        "def mse(p, t):\n",
        "  p = np.array(p)\n",
        "  t = np.array(t)\n",
        "  return np.mean((p - t)**2)\n",
        "\n",
        "print('train mse:', mse(train_predictions, train_targets))\n",
        "print('test mse:', mse(test_predictions, test_targets))"
      ],
      "metadata": {
        "id": "Im6W5cs9Xet6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Comparison\n",
        "\n",
        "- User-User CF: choose items for a user, because those items have been liked by similar users\n",
        "\n",
        "- Item-item CF: choose items for a user, because this user liked similar items in the past\n",
        "\n",
        "- When comparing 2 items, you have a lot more data than when comparing 2 users\n",
        "\n",
        "- Item-Based CF is fater\n",
        "  - given a user, score for ecah item $O(M^2N)$\n",
        "    - There are $M^2$ item-item weights< and each vector is length N\n",
        "  - For user-based CF< we had  $O(N^2M)\n",
        "    - $N>M$, so $N^2$ compared to $M^2$ is even worse\n",
        "\n",
        "- Item-based CF is more accurate as there's more data to train the weights on."
      ],
      "metadata": {
        "id": "-Hewxxx2C7sS"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}