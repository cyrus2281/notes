{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM0uOAfNItwW7Gx2uL619ht",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/notes/blob/main/MachineLearning/Recommenders.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Recommenders"
      ],
      "metadata": {
        "id": "COYGNbvomdLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Recommenders\n",
        "\n",
        "These examples are non-personalized recommendations\n"
      ],
      "metadata": {
        "id": "X7LG4JFYmfH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lift\n",
        "\n",
        "$$\n",
        "\\text{Lift} = \\frac{p(A,B)}{p(A)p(B)} = \\frac{p(A|B)}{p(A)} = \\frac{p(B|A)}{p(B)}\n",
        "$$\n",
        "\n",
        "- Symmetric\n",
        "- If A and B are independent, then $p(A|B) = p(A)$\n",
        "  - $p(A|B) / p(A) = 1$\n",
        "- if increasing the probability of B increases the probability of A, then Lift > 1"
      ],
      "metadata": {
        "id": "-NPdgLKNkwA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hacker News Formula\n",
        "\n",
        "Balancing Popularirty with Age\n",
        "\n",
        "$$\n",
        "\\frac{f(\\text{popularity})}{g(\\text{age})} \\\\[1cm]\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\text{score} = \\frac{(\\text{ups} - \\text{downs} -1 )^{0.8}}{(\\text{age}+2)^{\\text{gravity}}} \\times \\text{penalty}\n",
        "$$\n",
        "\n",
        "- gravity = 1.8\n",
        "- penalty = multiplier to implement \"business rules\" (e.g. penalize self-posts, \"controversial\" posts, + many more rules)\n",
        "\n",
        "\\\n",
        "\n",
        "age starts from 2, to prevent division by zero\n",
        "\n",
        "exponent of numerator is bigger than the exponent of the denominator, meaning denominator grows faster.\n",
        "\n",
        "Age always overtakes popularity\n",
        "\n",
        "\\\n",
        "\n",
        "exponent 0.8 causes sublinear growth.\n",
        "meaning 0 → 100 worth more than 1000 → 1100\n",
        "\n"
      ],
      "metadata": {
        "id": "bNo3EqUbmjRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reddit Formula\n",
        "\n",
        "$$\n",
        "\\text{score} = \\text{sign}(\\text{ups}-\\text{down}) \\times \\log \\{ \\max(1, |\\text{ups} - \\text{downs}|) \\} + \\frac{\\text{age}}{45000}\n",
        "$$\n",
        "\n",
        "- log of the absoulte value of net votes - sublinear curve - initial votes matter more - max since log 0 is not possible\n",
        "\n",
        "\\\n",
        "\n",
        "- can be positive or negative\n",
        "  - The more downvotes you get, the futher your score goes down\n",
        "\n",
        "\\\n",
        "\n",
        "- age is in seconds from inception of reddit\n",
        "- age is always positive\n",
        "- newer links → more score\n",
        "- reddit scores will forever increase linearly\n"
      ],
      "metadata": {
        "id": "OX9Eah-ppQoq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Google Page Rank\n",
        "\n",
        "Logic: The page rank of a page is the probability I would end up on that page if I surfed the internet randomly for an infinite amount of time\n",
        "\n"
      ],
      "metadata": {
        "id": "cCDmUR3Fp0YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Markov Models\n",
        "\n",
        "- Simplest way to think about Markov Models are bigrams from NLP\n",
        "- Build a probablistic language model\n",
        "- Can ask \"what is the probability of the next word in the sentence 'love' give the previous word was 'I'?\" i.e., p(love | I )\n",
        "\n",
        "**Bigrams**\n",
        "\n",
        "- It's a bigram because we only consider 2 words at a time\n",
        "\n",
        "We don't have to think of each item as a word, just a generic state: $x(t)$\n",
        "\n",
        "\"Markov\" means $x(t)$ doesn't depend on any values 2 or more steps behind, only the immediate last value.\n",
        "$$\n",
        "p(x_t | x_{t-1}, x_{t-2}, \\cdots, x_1) = p(x_t | x_{t-1})\n",
        "$$\n",
        "\n"
      ],
      "metadata": {
        "id": "bIU2zR8LtEfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Transition Probability Matrix\n",
        "\n",
        "- $A(i,j)$ tells use the probability of going to state j from state i\n",
        "$$\n",
        "A(i,j) = p(x_t = j | x_{t-1} = i )\n",
        "$$\n",
        "\n",
        "- Key: rows must sum to 1\n",
        "    - Since it's a probability this must be true\n",
        "    - If true, A is called a \"stochastic matrix\" or \"Markov Matrix\"\n",
        "$$\n",
        "\\sum^M_{j=1}A(i,j) = \\sum^M_{j=1}p(x_t=j|x_{t-1}=i) =1\n",
        "$$\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "Example:\n",
        "\n",
        "Weather is\n",
        "- state 1 = Sunny\n",
        "- state 2 = Rainy\n",
        "\n",
        "Suppose:\n",
        "- p( sunny | sunny ) = 0.9\n",
        "- p( sunny | rainy ) = 0.1\n",
        "- p( rainy | sunny ) = 0.1\n",
        "- p( rainy | rainy ) = 0.9\n",
        "\n"
      ],
      "metadata": {
        "id": "UENDikH0tHHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Calculating Probabilities\n",
        "\n",
        "$$\n",
        "p(\\text{rainy} | \\text{sunny}) = \\frac{\\text{count}(\\text{sunny} \\rightarrow \\text{rainy})}{\\text{count}(\\text{sunny})}\n",
        "$$\n",
        "\n",
        "Generalized\n",
        "\n",
        "$$\n",
        "p(B|A) = \\frac{\\text{count}(A \\rightarrow B)}{\\text{count}(A)}\n",
        "$$\n",
        "\n",
        "Now probability of a sentence would be\n",
        "\n",
        "$$\n",
        "p(x_1, \\cdots, x_T) = p(x_1) \\prod^T_{t=2} p(x_t | x_{t-1})\n",
        "$$\n",
        "\n",
        "Problem: If a bigram didn't apear in the train set, the probility would be 0 and anything × 0 is 0.\n",
        "\n",
        "**Add-1 Smoothing**\n",
        "\n",
        "$$\n",
        "p(x_t=j|x_{t-1}=i) = \\frac{\\text{count}(i \\rightarrow j)+\\epsilon}{\\text{count}(i)+\\epsilon V} \\\\\n",
        "$$\n",
        "\n",
        "- Add a \"fake count\" to every possible bigram\n",
        "  - ϵ can be any value, for example 1.\n",
        "- V = Vocabulary size = number of unique words in dataset\n",
        "- In this case, V=M (number of states) since each state is a word\n",
        "\n",
        "\n",
        "e.g. p(and | and) never occurs but would get positve probability\n"
      ],
      "metadata": {
        "id": "Cbn1ogLEtKl_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Beta Posterior Mean\n",
        "\n",
        "In our case, the equation is the beta posterior mean instead of only 2 possible outcomes, V possible outcomts\n",
        "\n",
        "$$\n",
        "E(\\pi) = \\frac{\\alpha'}{\\alpha'+\\beta'} = \\frac{\\alpha+(\\sum_{i=1}^N X_i)}{\\alpha + \\beta + N} \\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "-PvC0vn1xck2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### State Distribution\n",
        "\n",
        "$\\pi_t$ = state probability distribution at time t\n",
        "\n",
        "$\\pi(t)$ is a row vector by convention\n",
        "\n",
        "For the weather example,\n",
        "\n",
        "$$\n",
        "\\pi_t = [p(x_t = \\text{sunny}), p(x_t = \\text{rainy})]\n",
        "$$\n",
        "\n",
        "**Future State Distribution**\n",
        "\n",
        "Calculating the $\\pi(t+1)$ use Bayes rule\n",
        "\n",
        "$$\n",
        "p(x_{t+1} = j) = \\sum^M_{i=1} p(x_{t+1} = j, x_t =i) \\\\\n",
        "= \\sum^M_{i=1} p(x_{t+1}=j | x_t = i) p(x_t = i) \\\\\n",
        "= \\sum^M_{i=1} A(i,j)\\pi(i) \\\\\n",
        "= \\pi_{t+1}(j)\n",
        "$$\n",
        "\n",
        "Since A is a matrix and $\\pi(t)$ is a vector, we can express it in terms of matrix math\n",
        "$$\n",
        "\\pi_{x+1}(j) = \\sum^M_{i=1} A(i,j)\\pi_t(i) \\\\\n",
        "\\pi_{t+1} = \\pi_t A\n",
        "$$\n",
        "\n",
        "Further future\n",
        "\n",
        "$$\n",
        "\\pi_{t+2} = \\pi_t A^2 \\\\\n",
        "\\pi_{t+k} = \\pi_t A^k \\\\\n",
        "$$\n",
        "\n",
        "For infinity\n",
        "\n",
        "$$\n",
        "\\pi_\\infty = \\lim_{t\\rightarrow \\infty} \\pi_0 A^t \\\\\n",
        "\\pi_\\infty = \\pi_\\infty A\n",
        "$$\n",
        "\n",
        "This is just the eigenvalue problem\n",
        "  - Give matrix A, find a vector and a scalar s.t. multiplying the vector by A is equivalent to stretching it be the scalar."
      ],
      "metadata": {
        "id": "pS0RySi_1Ecy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PageRank\n",
        "\n",
        "Every page on the internet is a state in a Markov Model\n",
        "\n",
        "The transition probablity is distributed equally amongst all links on a page\n",
        "- p(dlc.com | lp.me ) = 0.5\n",
        "- p(yt.com | lp.me ) = 0.5\n",
        "\n",
        "In general, we can write the transition probability as:\n",
        "\n",
        "$$\n",
        "p(x_t =j | x_{x-1} = i) = \\frac{1}{n(i)}\n",
        "$$\n",
        "if $i$ links to $j$, $n(i) = $ number of links on page $i$, otherwise $0$.\n",
        "\n",
        "**Smoothing**\n",
        "\n",
        "$$\n",
        "G = 0.85A + 0.15U \\\\\n",
        "U(i,j) = \\frac{1}{M} \\\\\n",
        "\\forall i,j = 1 \\dots M\n",
        "$$\n",
        "\n",
        "Find the limiting distribution of G - yields a vector of length M - these probabilities are the respective PageRanks for ech page on the internet\n",
        "\n",
        "$$\n",
        "\\pi_\\infty = \\pi_\\infty G\n",
        "$$\n",
        "\n",
        "\n",
        "**Perron-Frobenius Theorem**:\n",
        "> If G is a valid Markov matrix and all its elements are positive then the stationary distribution and limiting distribution are the same\n",
        "- Limiting Distribution: state distribution you'd arrive at after transitioning by G an infinite number of times\n",
        "- Stationary Distribution: a state distribution that does not change after transitioning by G"
      ],
      "metadata": {
        "id": "Q640uQxg3_sJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics"
      ],
      "metadata": {
        "id": "Ivwe3u2lzHp3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Smoothing (Dampening)\n",
        "\n",
        "To resolve the issue of 0 in sample data when getting mean\n",
        "\n",
        "$$\n",
        "r = \\frac{\\sum^N_{i=1} X_i + \\lambda \\mu_0}{N+\\lambda}\n",
        "$$\n",
        "\n",
        "\n",
        "- $\\lambda$ some random small non-zero number\n",
        "- $\\mu_0$ the global avergage or just some middle value\n",
        "\n",
        "\\\n",
        "\n",
        "For example:\n",
        "- 1000 reviews of 4 star - μ = 3 - λ = 1 → 3.999\n",
        "- 5 reviews of 4 star - μ = 3 - λ = 1 → 3.83\n",
        "- 1 review of 4 star - μ = 3 - λ = 1 → 3.5"
      ],
      "metadata": {
        "id": "S34-RUx8zI_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Explore-Exploit Dilemma\n",
        "\n",
        "Example 1\n",
        "\n",
        "Imagine we want to find the slot machine with the highest win rate among 10 slot machines.\n",
        "\n",
        "Traditional statistical test can tell us whether or not there's a significant difference between win rates between machines.\n",
        "\n",
        "If playing each machine 100 times, meaning 1000 turns total, 900 (9/10) turns yielded a suboptimal reward.\n",
        "\n",
        "Hence the dilemma, Play more or play less!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Example 2\n",
        "\n",
        "Watching a bunach of YouTube videos on how to make eggs.\n",
        "\n",
        "Now your reccomendations are filled with videos about making eggs\n",
        "\n",
        "porbably suboptimal - once I've figured out how to make eggs, I don't want to watch more egg videos.\n",
        "\n",
        "YouTube is not exploiting the fact that I watched eggs video and not exploring other topics\n",
        "\n",
        "Should there be a stronger exploration component?\n",
        "\n",
        "Maybe I'd like to seE movie trailers or machine learning videos\n",
        "\n",
        "---\n",
        "\n",
        "How do we strike a balance between these 2 opposing forces?\n",
        "\n",
        "Smoothed average gives us one part of the solution\n",
        "\n",
        "Making good things look worse and bad things look better\n"
      ],
      "metadata": {
        "id": "gahD3CwG1XEv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayesian Method\n",
        "\n",
        "Bayesian method automatically balances need to explore and exploit\n",
        "\n",
        "- 2 fat distributions: explore both (totally random ranking)\n",
        "- 2 skinny distributions: exploit both (nearly deterministic ranking)\n",
        "- Mixed: explore and exploit co-exit\n",
        "\n",
        "\n",
        "Completely automatic - does not require A/B testing\n",
        "\n"
      ],
      "metadata": {
        "id": "N4WrwUfO6MUp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Collaborative Filtering\n",
        "\n",
        "Non-specific to any particular user, Score each item from 1 to M a number.\n",
        "\n",
        "Basic algorithm is to make s(j) the average rating for j\n",
        "\n",
        "$$\n",
        "s(j) = \\frac{\\sum_{i \\in \\Omega_j} r_{ij}}{|\\Omega_j|} \\\\\n",
        "$$\n",
        "\n",
        "- $\\Omega_j$ = set of all users who rated item j\n",
        "- $r_{ij}$ = rating user i gave item j\n",
        "\n",
        "Translates to, average rating for a product is the sum of rating divided by the number of the ratings.\n",
        "\n",
        "**Personalize the score**\n",
        "\n",
        "s(i,j) can depend both on user i and item j\n",
        "\n",
        "$$\n",
        "s(i,j) = \\frac{\\sum_{i' \\in \\Omega_j} r_{i'j}}{|\\Omega_j|} \\\\\n",
        "$$\n",
        "\n",
        "i' is just an index\n",
        "\n",
        "i = 1 … N, N = number of users\n",
        "\n",
        "j = 1 … M, M = number of items\n",
        "\n",
        "$R_{N\\times M}$ = user -item ratings matrix of size N × M\n",
        "\n",
        "\n",
        "\\\n",
        "\n",
        "- User-item matrix is reminiscent of term-document.\n",
        "- X(t,d) = # of time term t appears in document d\n",
        "- In terms of recommender systems, can think of X(t,d) as \"how much does t like the item d\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "biqlBm7295Kg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparsity\n",
        "\n",
        "One characteristic of the user items matrix that makes it unique to recommender systems is that it's Sparse.\n",
        "\n",
        "- Term-document matrix is sparse because most entries are 0\n",
        "- User-item matrix is sparse because most entries are **empty**\n",
        "\n",
        "The average user does not interact with all items.\n",
        "\n",
        "\n",
        "**Goal of Collaborative Filtering**\n",
        "\n",
        "- Most of r(i,j) doesn't exist - this is good.\n",
        "\n",
        "If every user has seen every item, then there's nothing to recommend\n",
        "\n",
        "Goal:\n",
        "> We want to guess what you might rate an item you haven't seen yet\n",
        "\n",
        "$$\n",
        "s(i,j) = \\hat r (i,j) = \\text{ guess what user i might rate item j}\n",
        "$$\n",
        "\n",
        "E.g. if we think you might rate some move a 5, we definitely want you to watch that movie.\n"
      ],
      "metadata": {
        "id": "VHSIzt7MLEya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regression\n",
        "\n",
        "Since this is a regression probelm, the evaluation metric is going to be the mean squared error.\n",
        "\n",
        "Outline:\n",
        "- user-user collaborative filtering\n",
        "- item-item collaborative filtering\n",
        "\n",
        "$$\n",
        "\\text{MSE} = \\frac{1}{|\\Omega|} \\sum_{i,j \\in \\Omega}(r_{ij} - \\hat r_{ij})^2\n",
        "$$\n",
        "\n",
        "Ω = set of pairs (i,j) where user i has rated item j\n",
        "\n",
        "we're going to take our models predicted ratings, compare them to the actual ratings, square the difference and then take the average of those squared differences"
      ],
      "metadata": {
        "id": "pmcJqzBwNcuy"
      }
    }
  ]
}