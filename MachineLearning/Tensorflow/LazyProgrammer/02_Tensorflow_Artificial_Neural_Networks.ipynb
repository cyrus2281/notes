{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZpb1e+TIh0U6GGNdq6whR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/notes/blob/main/MachineLearning/Tensorflow/LazyProgrammer/02_Tensorflow_Artificial_Neural_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content"
      ],
      "metadata": {
        "id": "66dOmkdh_3Bx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Content](#scrollTo=66dOmkdh_3Bx)\n",
        "\n",
        ">[Artificial Neural Networks](#scrollTo=YiVkndZ5Up77)\n",
        "\n",
        ">>[Feedforward Neural Networks](#scrollTo=2gY3gD-9ZwPi)\n",
        "\n",
        ">>[Activation Functions](#scrollTo=cMpmO42gcjJe)\n",
        "\n",
        ">>>[Identity](#scrollTo=zwnoUVqU-2jN)\n",
        "\n",
        ">>>[Sigmoid](#scrollTo=Z-Oyiikzx7EF)\n",
        "\n",
        ">>>>[Standardization](#scrollTo=7xO7_bnsxEI5)\n",
        "\n",
        ">>>[Tanh](#scrollTo=SbGf15PRx9Pa)\n",
        "\n",
        ">>>>[The Vanishing Gradient Problem](#scrollTo=BJNIGlf-yj6d)\n",
        "\n",
        ">>>[ReLU](#scrollTo=oKkr1zJFz0jZ)\n",
        "\n",
        ">>>>[Dead Neuron Problem](#scrollTo=98grfxfc2T-A)\n",
        "\n",
        ">>>[LReLU](#scrollTo=SwJuY83B2yom)\n",
        "\n",
        ">>>[ELU](#scrollTo=rFwSAjlK4T87)\n",
        "\n",
        ">>>[Softplus](#scrollTo=IYJ9QMSE5HWO)\n",
        "\n",
        ">>>[BRU](#scrollTo=N1r-a2oC7E8H)\n",
        "\n",
        ">>>>[Multiclass Classification](#scrollTo=4bhxjQMB7-GO)\n",
        "\n",
        ">>>[Softmax](#scrollTo=vJeAKLLx8_5-)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "Kb-qIXis_6bF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Artificial Neural Networks"
      ],
      "metadata": {
        "id": "YiVkndZ5Up77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feedforward Neural Networks"
      ],
      "metadata": {
        "id": "2gY3gD-9ZwPi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiple layers of interconntected neurons (logistic regressions). Left side is input, right side is output. and signal goes from left to right. Hence is called a \"feedforward\" neural network.\n",
        "\n",
        "\\\n",
        "\n",
        "Different neurons look for different features.\n",
        "\n",
        "\\\n",
        "\n",
        "The same inputs can be fed to multiple different neurons, each calculating something different (more neurons per layer).\n",
        "\n",
        "Neurons in one layer can act as inputs to another layer."
      ],
      "metadata": {
        "id": "DJTAVtascAPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A line: $ax+b$\n",
        "\n",
        "A neuron: $\\sigma(w^Tx+b)$\n",
        "\n",
        "If there's $M$ neurons, and we call each $z_j$:\n",
        "\n",
        "$\n",
        "z_j = \\sigma(w^T_jx+b_j), \\text{ for } j = 1\\dots M \\\\\n",
        "$\n",
        "\n",
        "Vectorize the neurons (Single Layer):\n",
        "\n",
        "$$\n",
        "z = \\sigma(W^Tx+b)\n",
        "$$\n",
        "\n",
        "- $z$ is a vector of size M\n",
        "- $x$ is a vector of size D\n",
        "- $W$ is a matrix of size D×M\n",
        "- $b$ is a vector of size M\n",
        "\n",
        "\\\n",
        "\n",
        "Vectorize the neurons (Multiple Layer):\n",
        "\n",
        "We show layer as a superscript\n",
        "\n",
        "$\n",
        "z^{(1)} = \\sigma(W^{(1)T}x+b^{(1)}) \\\\\n",
        "z^{(2)} = \\sigma(W^{(2)T}z^{(1)}+b^{(2)}) \\\\\n",
        "z^{(3)} = \\sigma(W^{(3)T}z^{(2)}+b^{(3)}) \\\\\n",
        "$\n",
        "\n",
        "$$\n",
        "p(y=1 | x) = \\sigma(W^{(L)T}z^{(L-1)}+b^{(L)}) \\\\\n",
        "$$\n",
        "\n",
        "- $L$ number of layers\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YAHnmfq4gWKG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we don't need the final sigmoid (eg for prediction of non-binary values), we can just remove it\n",
        "\n",
        "$$\n",
        "\\hat y = W^{(L)T}z^{(L-1)}+b^{(L)} \\\\\n",
        "$$\n",
        "\n",
        "(This just looks like a linear regression)"
      ],
      "metadata": {
        "id": "r39XS8eFjJeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n"
      ],
      "metadata": {
        "id": "cMpmO42gcjJe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Identity\n",
        "\n",
        "Identity function is just a function that returns it's input.\n",
        "\n",
        "$$\n",
        "f(x) = x\n",
        "$$\n",
        "\n",
        "This is used for regression models"
      ],
      "metadata": {
        "id": "zwnoUVqU-2jN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sigmoid\n",
        "\n",
        "$$\n",
        "\\sigma(a) = \\frac{1}{1+\\exp(-a)}\n",
        "$$\n",
        "\n",
        "- Maps input to 0…1\n",
        "- Non-linear\n",
        "\n",
        "![sigmoid](https://miro.medium.com/v2/resize:fit:640/format:webp/1*Xu7B5y9gp0iL5ooBj7LtWw.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "Z-Oyiikzx7EF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Standardization\n",
        "\n",
        "We don't want to have one input in the range 1..5 million, and another in the range 0...0.0001, We prefer inputs centered around 0 and approx. the same range.\n",
        "\n",
        "The sigmoid output goes between 0 and 1, center is 0.5. Its output therefore can never be centered around 0.\n",
        "\n",
        "The concept of \"uniformity\", the output of the sigmoid (previous layer) is the input to the next layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "7xO7_bnsxEI5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tanh\n",
        "\n",
        "The solution to this issue, is another activation function similar to sigmoid, but centered around zero, which is **Hyperbolic Tangent (tanh)**.\n",
        "\n",
        "$$\n",
        "\\tanh(a) = \\frac{\\exp(2a)-1}{\\exp(2a)+1} \\\\\n",
        "$$\n",
        "\n",
        "![tanh](https://images.squarespace-cdn.com/content/v1/5acbdd3a25bf024c12f4c8b4/1524687495762-MQLVJGP4I57NT34XXTF4/TanhFunction.jpg?format=1500w)"
      ],
      "metadata": {
        "id": "SbGf15PRx9Pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Vanishing Gradient Problem\n",
        "\n",
        "Gradient of multiple layers\n",
        "\n",
        "$$\n",
        "\\frac{\\partial J}{\\partial W^{(1)}} =\n",
        "\\frac{\\partial J}{\\partial z^{(L)}}\n",
        "\\frac{\\partial z^{(L)}}{\\partial z^{(L-1)}} \\cdots\n",
        "\\frac{\\partial z^{(2)}}{\\partial z^{(1)}}\n",
        "\\frac{\\partial z^{(1)}}{\\partial W^{(1)}}\n",
        "$$\n",
        "\n",
        "Output\n",
        "\n",
        "$$\n",
        "\\sigma(\\dots \\sigma( \\dots (\\sigma \\dots(\\dots))))\n",
        "$$\n",
        "\n",
        "We end up multiplying by the derivative of the sigmoid over and over again.\n",
        "\n",
        "\n",
        "Derivative of sigmoid is very tiny number! Maximum value is only 0.25\n",
        "\n",
        "\n",
        "![Derivative of sigmoid](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*6A3A_rt4YmumHusvTvVTxw.png)\n",
        "\n",
        "\n",
        "Multiplying many small numbers, only result in an even smaller number. E.g. $0.25^5 \\approx 0.001$\n",
        "\n",
        "This results in *the further back we go in a neural network, the smaller the gradient becomes*. This is known as **Vanishing Gradient Problem**.\n",
        "\n",
        "\n",
        "the training algorithm is to take small steps in the direction of the gradient. If the gradient is nearly zero that means the update to the weights is also nearly zero. The end result is that weights close to the input of the neural network are almost not trained at all."
      ],
      "metadata": {
        "id": "BJNIGlf-yj6d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReLU\n",
        "\n",
        "Solution was simple. Don't use activation functions that have vanishing gradients.\n",
        "\n",
        "For example, **Rectifier Linear Unit (ReLU)**.\n",
        "\n",
        "$$\n",
        "R(z) = \\max(0, z) \\\\\n",
        "$$\n",
        "\n",
        "![relu](https://miro.medium.com/v2/resize:fit:1100/format:webp/1*DfMRHwxY1gyyDmrIAd-gjQ.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "oKkr1zJFz0jZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dead Neuron Problem\n",
        "\n",
        "*Problem*, The ReLU doesn't have a \"vanishing\" gradient, but the gradient in the left half is already vanished!\n",
        "\n",
        "This phenomenon is knonw as the \"**dead neuron**\" Probelm.\n",
        "\n",
        "That fact that the right side does not vanish, seems to be enough for the majority of deep learning experiments.\n"
      ],
      "metadata": {
        "id": "98grfxfc2T-A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LReLU\n",
        "\n",
        "Leaky ReLU is one solution to the Dead Neuron problem. It has a small Positive slope for negative inputs\n",
        "\n",
        "$$\n",
        "f(x) = \\left\\{\\begin{matrix}\n",
        "x & x \\ge 0\n",
        "\\\\\n",
        "\\alpha x & x < 0\n",
        "\\end{matrix}\\right.\n",
        "$$\n",
        "- α is a small number like 0.1\n",
        "\n",
        "- Slope is always positive\n",
        "- It's a non-linear function\n",
        "\n",
        "![lrelu](https://www.researchgate.net/publication/340644173/figure/fig4/AS:880423093686272@1586920631085/5a-Graph-of-the-LReLU-function-5b-Graph-of-gradient-of-LReLU-function.ppm)\n"
      ],
      "metadata": {
        "id": "SwJuY83B2yom"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ELU\n",
        "\n",
        "Exponential linera unit (ELU) is another solution to dead neuron problem which has a more steadily decreasing value on the left side.\n",
        "\n",
        "Authors claim it speeds up learning and leads to higher accuracy.\n",
        "\n",
        "Negative values possible, the mean can be zero (unlike ReLU)\n",
        "\n",
        "\n",
        "$$\n",
        "f(x) = \\left\\{\\begin{matrix}\n",
        "x & x > 0\n",
        "\\\\\n",
        "a (e^x -1) & x \\le 0\n",
        "\\end{matrix}\\right. \\\\\n",
        "$$\n",
        "\n",
        "![elu](https://ml-cheatsheet.readthedocs.io/en/latest/_images/elu.png)"
      ],
      "metadata": {
        "id": "rFwSAjlK4T87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softplus\n",
        "\n",
        "Softplus is another option which is very similar because you're taking the log of the exponent looks very linear when the input is reasonably large.\n",
        "\n",
        "$$\n",
        "f(x) = \\log(1 + e^x) \\\\\n",
        "$$\n",
        "\n",
        "![softplus](https://www.researchgate.net/profile/Hussam-Lawen/publication/336602359/figure/fig2/AS:814832592908288@1571282637278/The-Softplus-function-ln1-exp-compared-to-max0.ppm)"
      ],
      "metadata": {
        "id": "IYJ9QMSE5HWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Note: both softplus and ELU have vanishing gradients on the left - but we already know it's not too problematic because ReLU works.\n",
        "\n",
        "- Softplus and ReLU are in the range $0\\dots\\infty$\n",
        "  - they can't be centered around 0\n",
        "  - Does it matter?\n"
      ],
      "metadata": {
        "id": "XDcuR3-25u9g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BRU\n",
        "\n",
        "Bionodal Root Unit (BRU) are activation functions that are more modeled like real neurons.\n",
        "\n",
        "More at: https://arxiv.org/pdf/1804.11237.pdf\n",
        "\n"
      ],
      "metadata": {
        "id": "N1r-a2oC7E8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Multiclass Classification\n",
        "\n",
        "If we have K possible outcomes, then we should have K output nodes. i.e. $a^{(L)}$ is a vector of size k.\n",
        "\n",
        "$$\n",
        "a^{(L)} = W^{(L)T}z^{(L-1)}+b^{(L)} \\\\\n",
        "$$\n",
        "\n",
        "So we need a probability distribution over K distinct values.\n",
        "- They must be non negative (>= 0) with an upper limit of 1 (<= 1)\n",
        "- All probabilites must some to 1\n",
        "\n",
        "Requirement 1:\n",
        "$$\n",
        "p(y=k|x) \\ge 0\n",
        "$$\n",
        "\n",
        "Requirement21:\n",
        "$$\n",
        "\\sum_{k=1}^K p(y=k|x) = 1\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4bhxjQMB7-GO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Softmax\n",
        "\n",
        "Softmax is a function that does exactly this.\n",
        "\n",
        "$$\n",
        "p(y= k|x) =\n",
        "\\frac{\\exp(a_k)}{\\sum_{j=1}^K\\exp(a_j)}\n",
        "$$"
      ],
      "metadata": {
        "id": "vJeAKLLx8_5-"
      }
    }
  ]
}