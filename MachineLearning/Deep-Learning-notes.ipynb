{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " view in Google Colab for best experience"
      ],
      "metadata": {
        "id": "XAKI4G_uOywK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table of Contents"
      ],
      "metadata": {
        "id": "j5-omnpm4t_3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Table of Contents](#scrollTo=j5-omnpm4t_3)\n",
        "\n",
        ">[Preprocessing](#scrollTo=xsNekpZKBOEc)\n",
        "\n",
        ">>[Normalization](#scrollTo=7p33kYVQBRGQ)\n",
        "\n",
        ">>>[Scale](#scrollTo=E9zLAgPcCDSo)\n",
        "\n",
        ">>>>[MinMaxScaler](#scrollTo=Ok0T-s4sEQd1)\n",
        "\n",
        ">>>[Standardize](#scrollTo=PrKtPYkGCDMb)\n",
        "\n",
        ">>>>[RobustScaler](#scrollTo=4wuCZztWEjZl)\n",
        "\n",
        ">>>>[StandardScaler](#scrollTo=wgcHnXy9E5Y6)\n",
        "\n",
        ">>>>[Normalizer](#scrollTo=jcWMNwX9FurO)\n",
        "\n",
        ">>[Data Augmentation](#scrollTo=2hBKNVqjAAKM)\n",
        "\n",
        ">>[Batches](#scrollTo=0t8i2Fky1Rv2)\n",
        "\n",
        ">[Neural Network](#scrollTo=ELNsz_33314I)\n",
        "\n",
        ">>[Fundamentals](#scrollTo=p4mDMzyMhmqY)\n",
        "\n",
        ">>>[Tensor](#scrollTo=4sx5wS7khqcZ)\n",
        "\n",
        ">>>[Neuron](#scrollTo=8ycTqUltiD_N)\n",
        "\n",
        ">>>[Layer](#scrollTo=ORwIvoWwi-Cv)\n",
        "\n",
        ">>>[Weights & Biases](#scrollTo=rYPkK9IXjGUY)\n",
        "\n",
        ">>>[Mixed Precision](#scrollTo=SK7o1zAQc1NQ)\n",
        "\n",
        ">>[Layers](#scrollTo=GRhEhh7oSlbR)\n",
        "\n",
        ">>>[Input Layer](#scrollTo=219kZKYYmIkh)\n",
        "\n",
        ">>>[Fully Connected (Dense) Layer](#scrollTo=CM8xMdybSnXm)\n",
        "\n",
        ">>>[Convolution Layer](#scrollTo=MGf4ou3GSxcS)\n",
        "\n",
        ">>>>[Padding](#scrollTo=uL1UPiq1rPUE)\n",
        "\n",
        ">>>>>[Same Padding](#scrollTo=9ecJwax3Uy0y)\n",
        "\n",
        ">>>>>[Valid Padding](#scrollTo=9TPAjFUBV5eo)\n",
        "\n",
        ">>>>[Kernel size](#scrollTo=R8WFbXzfrTHM)\n",
        "\n",
        ">>>>[Stride](#scrollTo=asP9JLplrr4r)\n",
        "\n",
        ">>>[Pooling Layer](#scrollTo=BFF9DpWVXsPy)\n",
        "\n",
        ">>>>[Max Pooling](#scrollTo=i2wnx3wkYE3E)\n",
        "\n",
        ">>>>[Average Pooling](#scrollTo=7XJBKaoUYOJR)\n",
        "\n",
        ">>>[Flatten Layer](#scrollTo=eUGmGyoOStMX)\n",
        "\n",
        ">>[Transfer learning](#scrollTo=hNaoDxVHDteM)\n",
        "\n",
        ">>>[Feature Extraction](#scrollTo=Yk71ZT8dDwCP)\n",
        "\n",
        ">>>[Fine Tuning](#scrollTo=cRlvsjz8D0b1)\n",
        "\n",
        ">>[Activation Functions](#scrollTo=_s8F6y7T46-Q)\n",
        "\n",
        ">>>[Sigmoid](#scrollTo=_shE9oTDtXk9)\n",
        "\n",
        ">>>[Softmax](#scrollTo=jWjrCgx84_mH)\n",
        "\n",
        ">>>[ReLU](#scrollTo=xAv8zU3RsSyN)\n",
        "\n",
        ">>[Algorithms](#scrollTo=kf17bf2Rjpkl)\n",
        "\n",
        ">>>[Convolutional Neural Network (CNN)](#scrollTo=jWAQaROejuRB)\n",
        "\n",
        ">>>[Natural Language Processing (NLP)](#scrollTo=VEQpNB0aOkpC)\n",
        "\n",
        ">>>>[Syntax Analysis](#scrollTo=tC-Pb7RWQmW-)\n",
        "\n",
        ">>>>[Semantic Analysis](#scrollTo=cOH6hL_kQmkJ)\n",
        "\n",
        ">>>>[Tokenization](#scrollTo=zaID8rlJX8HX)\n",
        "\n",
        ">>>>[Embeddings](#scrollTo=_eAb7clXYEA_)\n",
        "\n",
        ">>>[Recurrent Neural Networks (RNN)](#scrollTo=RxqEMJsgxQyH)\n",
        "\n",
        ">>>[Transformers](#scrollTo=QtIA3pOIKb7n)\n",
        "\n",
        ">[Compiling a Model](#scrollTo=EhZfLrKulBpn)\n",
        "\n",
        ">>[Optimization Function](#scrollTo=FbgEgkqUfSsW)\n",
        "\n",
        ">>>[Stochastic Gradient Descent (SDG)](#scrollTo=NdKtMngE6aiR)\n",
        "\n",
        ">>[Loss Function](#scrollTo=tgylCsjI6UDs)\n",
        "\n",
        ">>>[Empirical Loss](#scrollTo=MsZj9cIYY4dJ)\n",
        "\n",
        ">>>[Regression Loss Functions](#scrollTo=iOYd6N6EfWq_)\n",
        "\n",
        ">>>>[Mean Square Error Loss (MSE)](#scrollTo=efhGe2hBfZeV)\n",
        "\n",
        ">>>>[Mean Squared Logarithmic Error Loss](#scrollTo=s0uhvs0dfy00)\n",
        "\n",
        ">>>>[Mean Absolute Error Loss](#scrollTo=L9OFgTGggPew)\n",
        "\n",
        ">>>[Binary Classification Loss Functions](#scrollTo=_urvie6nhgbR)\n",
        "\n",
        ">>>>[Binary Cross-Entropy Loss](#scrollTo=KJcaCabCkRz8)\n",
        "\n",
        ">>>>[Hinge Loss](#scrollTo=n7sxDb-dmA4v)\n",
        "\n",
        ">>>>[Squared Hinge Loss](#scrollTo=fu8QrIP4rQO8)\n",
        "\n",
        ">>>[Multi-Class Classification Loss Functions](#scrollTo=0A9XLcElsZUr)\n",
        "\n",
        ">>>>[Multi-Class Cross-Entropy Loss](#scrollTo=X61eXdGDx2fP)\n",
        "\n",
        ">>>>[Sparse Multiclass Cross-Entropy Loss](#scrollTo=bXr8oJ9x0C_Z)\n",
        "\n",
        ">>>>[Kullback Leibler Divergence Loss](#scrollTo=GNMDVkZQ0sZz)\n",
        "\n",
        ">>[Metrics](#scrollTo=1yisTFEg5veU)\n",
        "\n",
        ">>>[$R^2$](#scrollTo=4hHt0uXD5xji)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "mnQ2U85-3xLO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Preprocessing***"
      ],
      "metadata": {
        "id": "xsNekpZKBOEc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalization\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "resources:\n",
        "* [Scale, Standardize, or Normalize with Scikit-Learn](https://towardsdatascience.com/scale-standardize-or-normalize-with-scikit-learn-6ccc7d176a02)"
      ],
      "metadata": {
        "id": "7p33kYVQBRGQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalize can be used to mean either `scale` or `standardize` (or even more!). Avoid the term normalize, because it has many definitions and is prone to creating confusion.\n",
        "Many machine learning algorithms perform better or converge faster when features are on a relatively similar scale and/or close to normally distributed.\n",
        "\n",
        "Examples of such algorithm families include:\n",
        "* linear and logistic regression\n",
        "* nearest neighbors\n",
        "* neural networks\n",
        "* support vector machines with radial bias kernel functions\n",
        "* principal components analysis\n",
        "* linear discriminant analysis\n",
        "\n",
        "MinMaxScaler, RobustScaler, StandardScaler, and Normalizer are scikit-learn methods to preprocess data for machine learning.\n"
      ],
      "metadata": {
        "id": "khYRxiuGBldY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scale**\n",
        "Scale generally means to change the range of the values. The shape of the distribution doesn’t change. Think about how a scale model of a building has the same proportions as the original, just smaller. That’s why we say it is drawn to scale. The range is often set at 0 to 1."
      ],
      "metadata": {
        "id": "E9zLAgPcCDSo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *MinMaxScaler*\n",
        "For each value in a feature, MinMaxScaler subtracts the minimum value in the feature and then divides by the range. The range is the difference between the original maximum and original minimum.\n",
        "\n",
        "MinMaxScaler preserves the shape of the original distribution. It doesn’t meaningfully change the information embedded in the original data.\n",
        "\n",
        "Note that MinMaxScaler **doesn’t reduce the importance of outliers**. It’s non-distorting.\n",
        "\n",
        "The default range for the feature returned by MinMaxScaler is 0 to 1."
      ],
      "metadata": {
        "id": "Ok0T-s4sEQd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Standardize**\n",
        "Standardize generally means changing the values so that the distribution’s standard deviation equals one. Scaling is often implied."
      ],
      "metadata": {
        "id": "PrKtPYkGCDMb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *RobustScaler*\n",
        "RobustScaler transforms the feature vector by subtracting the median and then dividing by the interquartile range (75% value — 25% value).\n",
        "\n",
        "Note that RobustScaler does not scale the data into a predetermined interval like MinMaxScaler. It does not meet the strict definition of *scale*.\n",
        "\n",
        "Use RobustScaler if you want to **reduce the effects of outliers**, relative to MinMaxScaler."
      ],
      "metadata": {
        "id": "4wuCZztWEjZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *StandardScaler*\n",
        "StandardScaler is the industry’s go-to algorithm.\n",
        "\n",
        "StandardScaler standardizes a feature by subtracting the mean and then scaling to unit variance. Unit variance means dividing all the values by the standard deviation. StandardScaler does not meet the strict definition of *scale*.\n",
        "\n",
        "StandardScaler results in a distribution with a standard deviation equal to 1. The variance is equal to 1 also, because variance = standard deviation squared. And 1 squared = 1.\n",
        "\n",
        "StandardScaler makes the mean of the distribution approximately 0.\n",
        "\n",
        "Deep learning algorithms often call for zero mean and unit variance. Regression-type algorithms also benefit from normally distributed data with small sample sizes.\n",
        "\n",
        "Use StandardScaler if you want each feature to have zero-mean, unit standard-deviation. If you want more normally distributed data, and are okay with transforming your data\n",
        "\n",
        "The difference between standard normalization and softmax is that although both rescale the logits between 0 and 1, in softmax the correct answer have the largest “signal”. By using softmax, we are effectively “approximating” argmax while gaining differentiability. Rescaling doesn’t weigh the max significantly higher than other logits, whereas softmax does. Simply put, softmax is a “softer” argmax."
      ],
      "metadata": {
        "id": "wgcHnXy9E5Y6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Normalizer*\n",
        "Normalizer works on the rows, not the columns! I find that very unintuitive. It’s easy to miss this information in the docs.\n",
        "\n",
        "By default, L2 normalization is applied to each observation so the that the values in a row have a unit norm. Unit norm with L2 means that if each element were squared and summed, the total would equal 1. Alternatively, L1 (aka taxicab or Manhattan) normalization can be applied instead of L2 normalization.\n",
        "\n",
        "Normalizer does transform all the features to values between -1 and 1"
      ],
      "metadata": {
        "id": "jcWMNwX9FurO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Augmentation**\n",
        "\n",
        "a technique to increase the diversity of your training set by applying random (but realistic) transformations, such as image rotation.\n",
        "\n",
        "When you don't have a large image dataset, it's a good practice to artificially introduce sample diversity by applying random, yet realistic, transformations to the training images, such as rotation and horizontal flipping. This helps expose the model to different aspects of the training data and reduce overfitting."
      ],
      "metadata": {
        "id": "2hBKNVqjAAKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Batches**\n",
        "\n",
        "A batch is a small subset of the dataset that a model looks at during training at a time.\n",
        "\n",
        "Reason to use batches:\n",
        "* The full dataset might not fit into the memoery of the processer (or GPU).\n",
        "* Model might not learn well if training on a very large dataset at a time."
      ],
      "metadata": {
        "id": "0t8i2Fky1Rv2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Neural Network***\n"
      ],
      "metadata": {
        "id": "ELNsz_33314I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Fundamentals \n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "p4mDMzyMhmqY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tensor**\n",
        "\n",
        "A tensor can be thought of as an n-dimensional matrix. In the CNN, tensors will be 3-dimensional with the exception of the output layer."
      ],
      "metadata": {
        "id": "4sx5wS7khqcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Neuron**\n",
        "\n",
        "A neuron also known as **perceptron**, can be thought of as a function that takes in multiple inputs and yields a single output. \n",
        "\n",
        "$$\n",
        "y = g(w_0 + Σ^{m}_{i=1} x_i w_i)\n",
        "$$\n",
        "* y: Output\n",
        "* g: Non-linear activation function\n",
        "* $w_0$: Bias\n",
        "* $x_i w_i$: Linear Combination of inputs\n"
      ],
      "metadata": {
        "id": "8ycTqUltiD_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Layer**\n",
        "\n",
        "A layer is simply a collection of neurons with the same operation, including the same hyperparameters."
      ],
      "metadata": {
        "id": "ORwIvoWwi-Cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Weights & Biases**\n",
        "\n",
        "Kernel weights and biases, while unique to each neuron, are tuned during the training phase, and allow the classifier to adapt to the problem and dataset provided."
      ],
      "metadata": {
        "id": "rYPkK9IXjGUY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Mixed Precision**\n",
        "\n",
        "\n",
        "Mixed precision is the use of both 16-bit and 32-bit floating-point types in a model during training to make it run faster and use less memory. By keeping certain parts of the model in the 32-bit types for numeric stability, the model will have a lower step time and train equally as well in terms of the evaluation metrics such as accuracy.\n",
        "\n",
        "Today, most models use the float32 dtype, which takes 32 bits of memory. However, there are two lower-precision dtypes, float16 and bfloat16, each which take 16 bits of memory instead. Modern accelerators can run operations faster in the 16-bit dtypes, as they have specialized hardware to run 16-bit computations and 16-bit dtypes can be read from memory faster.\n",
        "However, variables and a few computations should still be in float32 for numeric reasons so that the model trains to the same quality. \n",
        "\n",
        "While mixed precision will run on most hardware, it will only speed up models on recent NVIDIA GPUs and Cloud TPUs. NVIDIA GPUs support using a mix of float16 and float32, while TPUs support a mix of bfloat16 and float32.\n",
        "\n",
        "Among NVIDIA GPUs, those with compute capability 7.0 or higher will see the greatest performance benefit from mixed precision because they have special hardware units, called Tensor Cores, to accelerate float16 matrix multiplications and convolutions. Older GPUs offer no math performance benefit for using mixed precision, however memory and bandwidth savings can enable some speedups. On CPUs, mixed precision will run significantly slower, however.\n",
        "\n",
        "\n",
        "If it doesn't affect model quality, try running with double the batch size when using mixed precision. As float16 tensors use half the memory, this often allows you to double your batch size without running out of memory. Increasing batch size typically increases training throughput, i.e. the training elements per second your model can run on.\n",
        "\n",
        "\n",
        "Modern NVIDIA GPUs use a special hardware unit called Tensor Cores that can multiply float16 matrices very quickly. However, Tensor Cores requires certain dimensions of tensors to be a multiple of 8.e"
      ],
      "metadata": {
        "id": "SK7o1zAQc1NQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Layers\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Resources:\n",
        "* [CNN Explainer](https://poloclub.github.io/cnn-explainer/)\n",
        "* [A Comprehensive Guide to Convolutional Neural Networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)"
      ],
      "metadata": {
        "id": "GRhEhh7oSlbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Input Layer**\n",
        "\n",
        "***In a ConvNet***, The input layer represents the input image into the CNN. Because we use RGB images as input, the input layer has three channels, corresponding to the red, green, and blue channels"
      ],
      "metadata": {
        "id": "219kZKYYmIkh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fully Connected (Dense) Layer**\n",
        "\n",
        "***In a ConvNet*** adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space."
      ],
      "metadata": {
        "id": "CM8xMdybSnXm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convolution Layer**\n",
        "\n",
        "The convolutional (Kernel) layers  are the foundation of CNN, as they contain the learned kernels (weights), which extract features that distinguish different images from one another.\n",
        "\n",
        "![convolving layer](https://miro.medium.com/v2/resize:fit:786/1*GcI7G-JLAQiEoCON7xFbhg.gif)\n",
        "\n",
        "In the above demonstration, the green section resembles our 5x5x1 input image. The element involved in the convolution operation in the first part of a Convolutional Layer is called the **Kernel/Filter**, K, represented in color yellow. We have selected K as a 3x3x1 matrix.\n",
        "\n",
        "The Kernel shifts 9 times because of Stride Length = 1 (Non-Strided)(stride of 1 means that the kernel is shifted over 1 pixel per dot product), every time the convolutional neuron performs an elementwise dot product with a unique kernel and the output of the previous layer’s corresponding neuron. This will yield as many intermediate results as there are unique kernels. The convolutional neuron is the result of all of the intermediate results summed together with the learned bias.\n",
        "\n",
        "\n",
        "***The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image.***\n",
        "\n",
        "Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network that has a wholesome understanding of images in the dataset.\n",
        "\n",
        "**Hyperparameters:**"
      ],
      "metadata": {
        "id": "MGf4ou3GSxcS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Padding*\n",
        "Padding is often necessary when the kernel extends beyond the activation map. Padding conserves data at the borders of activation maps, which leads to better performance, and it can help preserve the input's spatial size, which allows an architecture designer to build depper, higher performing networks. There exist many padding techniques, but the most commonly used approach is zero-padding because of its performance, simplicity, and computational efficiency. The technique involves adding zeros symmetrically around the edges of an input. This approach is adopted by many high-performing CNNs such as AlexNet.\n",
        "\n",
        "There are two types of results to the operation — one in which the convolved feature is reduced in dimensionality as compared to the input, and the other in which the dimensionality is either increased or remains the same. This is done by applying ***Valid Padding*** in the case of the former, or ***Same Padding*** in the case of the latter."
      ],
      "metadata": {
        "id": "uL1UPiq1rPUE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Same Padding\n",
        "\n",
        "![same padding: 5x5x1 image is padded with 0s to create a 6x6x1 image](https://miro.medium.com/v2/resize:fit:640/1*nYf_cUIHFEWU1JXGwnz-Ig.gif)\n",
        "\n",
        "When we augment the 5x5x1 image into a 6x6x1 image and then apply the 3x3x1 kernel over it, we find that the convolved matrix turns out to be of dimensions 5x5x1. Hence the name — Same Padding.\n",
        "\n",
        "```\n",
        "\"SAME\" = with zero padding:\n",
        "\n",
        "               pad|                                      |pad\n",
        "   inputs:      0 |1  2  3  4  5  6  7  8  9  10 11 12 13|0  0\n",
        "               |________________|\n",
        "                              |_________________|\n",
        "                                             |________________|\n",
        "\n",
        "```\n",
        "\n",
        "\"SAME\" tries to pad evenly left and right, but if the amount of columns to be added is odd, it will add the extra column to the right, as is the case in this example (the same logic applies vertically: there may be an extra row of zeros at the bottom).\n",
        "\n",
        "This keeps the input tensors's shape"
      ],
      "metadata": {
        "id": "9ecJwax3Uy0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Valid Padding\n",
        "\n",
        "\n",
        "```\n",
        "\"VALID\" = without padding:\n",
        "\n",
        "   inputs:         1  2  3  4  5  6  7  8  9  10 11 (12 13)\n",
        "                  |________________|                dropped\n",
        "                                 |_________________|\n",
        "```\n",
        "\"VALID\" only ever drops the right-most columns (or bottom-most rows).\n",
        "\n",
        "This reduces the input tensor's shape"
      ],
      "metadata": {
        "id": "9TPAjFUBV5eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Kernel size*\n",
        "\n",
        "Kernel size, often also referred to as filter size, refers to the dimensions of the sliding window over the input. Choosing this hyperparameter has a massive impact on the image classification task. For example, small kernel sizes are able to extract a much larger amount of information containing highly local features from the input. a smaller kernel size also leads to a smaller reduction in layer dimensions, which allows for a deeper architecture. Conversely, a large kernel size extracts less information, which leads to a faster reduction in layer dimensions, often leading to worse performance. Large kernels are better suited to extract features that are larger. At the end of the day, choosing an appropriate kernel size will be dependent on your task and dataset, but generally, smaller kernel sizes lead to better performance for the image classification task because an architecture designer is able to stack more and more layers together to learn more and more complex features!"
      ],
      "metadata": {
        "id": "R8WFbXzfrTHM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Stride*\n",
        "\n",
        "Stride indicates how many pixels the kernel should be shifted over at a time. For example, Tiny VGG uses a stride of 1 for its convolutional layers, which means that the dot product is performed on a 3x3 window of the input to yield an output value, then is shifted to the right by one pixel for every subsequent operation. The impact stride has on a CNN is similar to kernel size. As stride is decreased, more features are learned because more data is extracted, which also leads to larger output layers. On the contrary, as stride is increased, this leads to more limited feature extraction and smaller output layer dimensions. One responsibility of the architecture designer is to ensure that the kernel slides across the input symmetrically when implementing a CNN."
      ],
      "metadata": {
        "id": "asP9JLplrr4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Pooling Layer**\n",
        "\n",
        "![3x3 pooling over 5x5 convolved feature](https://miro.medium.com/v2/resize:fit:640/1*uoWYsCV5vBU8SHFPAPao-w.gif)\n",
        "\n",
        "Similar to the Convolutional Layer, the Pooling layer is responsible for gradually reducing the spatial size of the Convolved Feature. This is to **decrease the number of parameters, and the computational power required to process the data** through dimensionality reduction. Furthermore, it is useful for **extracting dominant features** which are rotational and positional invariant, thus maintaining the process of effectively training the model.\n",
        "\n",
        "There are two types of Pooling: Max Pooling and Average Pooling.\n",
        "\n",
        "![Types of Pooling](https://miro.medium.com/v2/resize:fit:828/format:webp/1*KQIEqhxzICU7thjaQBfPBQ.png)"
      ],
      "metadata": {
        "id": "BFF9DpWVXsPy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Max Pooling*\n",
        "\n",
        "Max Pooling returns the **maximum value** from the portion of the image covered by the Kernel.\n",
        "\n",
        "The Max-Pooling operation requires selecting a kernel size and a stride length during architecture design. Once selected, the operation slides the kernel with the specified stride over the input while only selecting the largest value at each kernel slice from the input to yield a value for the output. "
      ],
      "metadata": {
        "id": "i2wnx3wkYE3E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Average Pooling*\n",
        "\n",
        "Average Pooling returns the **average of all the values** from the portion of the image covered by the Kernel.\n",
        "\n",
        "The Average-Pooling operation requires selecting a kernel size and a stride length during architecture design. Once selected, the operation slides the kernel with the specified stride over the input while only selecting the average value at each kernel slice from the input to yield a value for the output. \n"
      ],
      "metadata": {
        "id": "7XJBKaoUYOJR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Flatten Layer**\n",
        "\n",
        "Flattening is used to convert all the resultant 2-Dimensional arrays from pooled feature maps into a single long continuous linear vector. The flattened matrix is fed as input to the fully connected layer to classify the image."
      ],
      "metadata": {
        "id": "eUGmGyoOStMX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transfer learning\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Resources:\n",
        "* [TensorFlow - Transfer Learning](https://www.tensorflow.org/tutorials/images/transfer_learning)\n",
        "\n"
      ],
      "metadata": {
        "id": "hNaoDxVHDteM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The intuition behind transfer learning for image classification is that if a model is trained on a large and general enough dataset, this model will effectively serve as a generic model of the visual world. You can then take advantage of these learned feature maps without having to start from scratch by training a large model on a large dataset.\n",
        "\n",
        "The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful. Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer."
      ],
      "metadata": {
        "id": "dlC1R-xjD6AK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Feature Extraction**\n",
        "\n",
        "Feature Extraction: Use the representations learned by a previous network to extract meaningful features from new samples. You simply add a new classifier, which will be trained from scratch, on top of the pretrained model so that you can repurpose the feature maps learned previously for the dataset.\n",
        "\n",
        "You do not need to (re)train the entire model. The base convolutional network already contains features that are generically useful for classifying pictures. However, the final, classification part of the pretrained model is specific to the original classification task, and subsequently specific to the set of classes on which the model was trained.\n",
        "\n",
        "It is important to freeze the convolutional base before you compile and train the model. Freezing (keras: by setting `layer.trainable = False`) prevents the weights in a given layer from being updated during training."
      ],
      "metadata": {
        "id": "Yk71ZT8dDwCP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fine Tuning**\n",
        "\n",
        "Fine-Tuning: Unfreeze a few of the top layers of a frozen model base and jointly train both the newly-added classifier layers and the last layers of the base model. This allows us to \"fine-tune\" the higher-order feature representations in the base model in order to make them more relevant for the specific task.\n",
        "\n",
        "You should try to fine-tune a small number of top layers rather than the whole base model. In most convolutional networks, the higher up a layer is, the more specialized it is. The first few layers learn very simple and generic features that generalize to almost all types of images. As you go higher up, the features are increasingly more specific to the dataset on which the model was trained. The goal of fine-tuning is to adapt these specialized features to work with the new dataset, rather than overwrite the generic learning.\n",
        "\n",
        "**Important note about BatchNormalization layers in TensorFlow**\n",
        "Many models contain tf.keras.layers.BatchNormalization layers. This layer is a special case and precautions should be taken in the context of fine-tuning.\n",
        "\n",
        "When you set layer.trainable = False, the BatchNormalization layer will run in inference mode, and will not update its mean and variance statistics.\n",
        "\n",
        "When you unfreeze a model that contains BatchNormalization layers in order to do fine-tuning, you should keep the BatchNormalization layers in inference mode by passing training = False when calling the base model. Otherwise, the updates applied to the non-trainable weights will destroy what the model has learned."
      ],
      "metadata": {
        "id": "cRlvsjz8D0b1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Activation Functions\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_s8F6y7T46-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sigmoid**\n",
        "\n",
        "The sigmoid activation function is a commonly used mathematical function in artificial neural networks (NNs) and deep learning models. It is a type of nonlinear activation function that maps any input value to a range between 0 and 1.\n",
        "\n",
        "The mathematical expression for the sigmoid function is:\n",
        "\n",
        "\\begin{equation}\n",
        "S = \\frac{1}{1+e^{-(m \\times x+b)}}\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "The sigmoid function has a distinctive S-shaped curve that increases gradually at first and then more steeply, before leveling off again as it approaches its maximum value of 1.0. The sigmoid function has the property of being differentiable, which is important for training neural networks using backpropagation.\n",
        "\n",
        "One advantage of the sigmoid function is that it is easy to compute and is well-suited for binary classification problems. However, the main disadvantage of the sigmoid function is that it can suffer from the vanishing gradient problem when used in deep neural networks, which can slow down or even prevent convergence during the training process. As a result, other activation functions like ReLU, Leaky ReLU, and ELU have become more popular in modern deep learning architectures."
      ],
      "metadata": {
        "id": "_shE9oTDtXk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Softmax**\n",
        "\n",
        "The softmax function, also known as softargmax or normalized exponential function, converts a vector of K real numbers into a probability distribution of K possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression. The softmax function is often used as the last activation function of a neural network to normalize the output of a network to a probability distribution over predicted output classes, based on Luce's choice axiom.\n",
        "\n",
        "The softmax function takes as input a vector z of K real numbers, and normalizes it into a probability distribution consisting of K probabilities proportional to the exponentials of the input numbers. That is, prior to applying softmax, some vector components could be negative, or greater than one; and might not sum to 1; but after applying softmax, `each component will be in the interval (0,1), and the components will add up to 1`, so that they can be interpreted as probabilities. Furthermore, the larger input components will correspond to larger probabilities.\n",
        "\n"
      ],
      "metadata": {
        "id": "jWjrCgx84_mH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The standard (unit) softmax function $ \\sigma: \\mathbb{R}^K \\rightarrow (0,1)^K $ is defined when $K \\geq 1$ by the formula:\n",
        "\n",
        "\n",
        "$$\n",
        "\\sigma(z)_i = \\frac{e^{z_i}}{\\sum_{j=1}^{K}e^{z_j}} \\text{ for } i = 1,...,K \\text{ and } z= (z_1,...,z_K) \\in \\mathbb{R}^K\n",
        "$$"
      ],
      "metadata": {
        "id": "kUOjGwUq6wpq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In simple words, it applies the standard exponential function to each element $z_{i}$ of the input vector $z$ and normalizes these values by dividing by the sum of all these exponentials; this normalization ensures that the sum of the components of the output vector $\\sigma (\\mathbf {z} )$ is 1. The term \"softmax\" derives from the amplifying effects of the exponential on any maxima in the input vector.\n",
        "\n",
        "For example, the standard softmax of (1,2,8) is approximately (0.001,0.002,0.997), which amounts to assigning almost all of the total unit weight in the result to the position of the vector's maximal element (of 8).\n",
        "\n",
        "\\\\\n",
        "\n",
        "The softmax function is used in various `multiclass classification methods`\n",
        "\n",
        "\\\\\n",
        "\n",
        "The difference between standard normalization and softmax is that although both rescale the logits between 0 and 1, in softmax the correct answer have the largest “signal”. By using softmax, we are effectively “approximating” argmax while gaining differentiability. Rescaling doesn’t weigh the max significantly higher than other logits, whereas softmax does. Simply put, softmax is a “softer” argmax."
      ],
      "metadata": {
        "id": "L6adeqO8-tR4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **ReLU**\n",
        "\n",
        "the rectifier or ReLU (rectified linear unit) activation function is an activation function defined as the positive part of its argument:\n",
        "\n",
        "$$\n",
        "\\text{ReLU}(x) = \\text{max}(0,x) = \\left\\{\\begin{matrix}\n",
        "x \\text{ if } x > 0\\\\ \n",
        "0 \\text{ if } x \\leq  0\n",
        "\\end{matrix}\\right.\n",
        "$$\n",
        "\n",
        "\\\\\n",
        "\n",
        "![relu graph](https://machinelearningmastery.com/wp-content/uploads/2018/10/Line-Plot-of-Rectified-Linear-Activation-for-Negative-and-Positive-Inputs.png)\n"
      ],
      "metadata": {
        "id": "xAv8zU3RsSyN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algorithms\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kf17bf2Rjpkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Convolutional Neural Network (CNN)**\n",
        "\n",
        "A CNN is a neural network: an algorithm used to recognize patterns in data. Neural Networks in general are composed of a collection of neurons that are organized in layers, each with their own learnable weights and biases.\n",
        "\n",
        "A CNN conveys a differentiable score function, which is represented as class scores in the visualization on the output layer.\n",
        "\n",
        "CNNs utilize a special type of layer, aptly named a convolutional layer, that makes them well-positioned to learn from image and image-like data. Regarding image data, CNNs can be used for many different computer vision tasks, such as image processing, classification, segmentation, and object detection."
      ],
      "metadata": {
        "id": "jWAQaROejuRB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Natural Language Processing (NLP)**\n",
        "\n",
        "Resources:\n",
        "* [A Simple Introduction to Natural Language Processing](https://becominghuman.ai/a-simple-introduction-to-natural-language-processing-ea66a1747b32)\n",
        "* [The illustrated Word2Vec](http://jalammar.github.io/illustrated-word2vec/)\n",
        "\n",
        "Natural Language Processing is the technology used to aid computers to understand the human’s natural language.\n",
        "\n"
      ],
      "metadata": {
        "id": "VEQpNB0aOkpC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Syntax Analysis*\n",
        "\n",
        "Syntax refers to the arrangement of words in a sentence such that they make grammatical sense.\n",
        "\n",
        "In NLP, syntactic analysis is used to assess how the natural language aligns with the grammatical rules.\n",
        "\n",
        "Here are some syntax techniques that can be used:\n",
        "\n",
        "* **Lemmatization**: It entails reducing the various inflected forms of a word into a single form for easy analysis.\n",
        "* **Morphological segmentation**: It involves dividing words into individual units called morphemes.\n",
        "* **Word segmentation**: It involves dividing a large piece of continuous text into distinct units.\n",
        "* **Part-of-speech tagging**: It involves identifying the part of speech for every word.\n",
        "* **Parsing**: It involves undertaking grammatical analysis for the provided sentence.\n",
        "* **Sentence breaking**: It involves placing sentence boundaries on a large piece of text.\n",
        "* **Stemming**: It involves cutting the inflected words to their root form."
      ],
      "metadata": {
        "id": "tC-Pb7RWQmW-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Semantic Analysis*\n",
        "\n",
        "Semantics refers to the meaning that is conveyed by a text.\n",
        "It involves applying computer algorithms to understand the meaning and interpretation of words and how sentences are structured.\n",
        "\n",
        "Here are some techniques in semantic analysis:\n",
        "\n",
        "* **Named entity recognition (NER)**: It involves determining the parts of a text that can be identified and categorized into preset groups. Examples of such groups include names of people and names of places.\n",
        "* **Word sense disambiguation**: It involves giving meaning to a word based on the context.\n",
        "* **Natural language generation**: It involves using databases to derive semantic intentions and convert them into human language."
      ],
      "metadata": {
        "id": "cOH6hL_kQmkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In NLP, there are two main concepts for turning text into numbers: Tokenization, Embeddings\n"
      ],
      "metadata": {
        "id": "3A1_p4HhX2VQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Tokenization*\n",
        "A straight mapping from word or character or sub-word to a numerical value. There are three main levels of tokenization:\n",
        "  1. Using **word-level tokenization** with the sentence \"I love TensorFlow\" might result in \"I\" being `0`, \"love\" being `1` and \"TensorFlow\" being `2`. In this case, every word in a sequence considered a single **token**.\n",
        "  2. **Character-level tokenization**, such as converting the letters A-Z to values `1-26`. In this case, every character in a sequence considered a single **token**.\n",
        "  3. **Sub-word tokenization** is in between word-level and character-level tokenization. It involves breaking invidual words into smaller parts and then converting those smaller parts into numbers. For example, \"my favourite food is pineapple pizza\" might become \"my, fav, avour, rite, fo, oo, od, is, pin, ine, app, le, piz, za\". After doing this, these sub-words would then be mapped to a numerical value. In this case, every word could be considered multiple **tokens**.\n"
      ],
      "metadata": {
        "id": "zaID8rlJX8HX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### *Embeddings*\n",
        "An embedding is a representation of natural language which can be learned. Representation comes in the form of a **feature vector**. For example, the word \"dance\" could be represented by the 5-dimensional vector `[-0.8547, 0.4559, -0.3332, 0.9877, 0.1112]`. It's important to note here, the size of the feature vector is tuneable. There are two ways to use embeddings: \n",
        "  1. **Create your own embedding** - Once your text has been turned into numbers (required for an embedding), you can put them through an embedding layer (such as [`tf.keras.layers.Embedding`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding)) and an embedding representation will be learned during model training.\n",
        "  2. **Reuse a pre-learned embedding** - Many pre-trained embeddings exist online. These pre-trained embeddings have often been learned on large corpuses of text (such as all of Wikipedia) and thus have a good underlying representation of natural language. You can use a pre-trained embedding to initialize your model and fine-tune it to your own specific task."
      ],
      "metadata": {
        "id": "_eAb7clXYEA_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Recurrent Neural Networks (RNN)**\n",
        "\n",
        "Resources:\n",
        "* [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "* [Visualizing A Neural Machine Translation Model](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/)"
      ],
      "metadata": {
        "id": "RxqEMJsgxQyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Transformers**\n",
        "\n",
        "Resources:\n",
        "* [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)"
      ],
      "metadata": {
        "id": "QtIA3pOIKb7n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The core reason that recurrent nets are more exciting is that they allow us to operate over sequences of vectors: Sequences in the input, the output, or in the most general case both. A few examples may make this more concrete:\n",
        "\n",
        "![rnn-types](https://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "So how do these things work? At the core, RNNs have a deceptively simple API: They accept an input vector x and give you an output vector y. However, crucially this output vector’s contents are influenced not only by the input you just fed in, but also on the entire history of inputs you’ve fed in in the past."
      ],
      "metadata": {
        "id": "8zUcnlu1xYSk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Compiling a Model***\n"
      ],
      "metadata": {
        "id": "EhZfLrKulBpn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimization Function\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "FbgEgkqUfSsW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stochastic Gradient Descent (SDG)**"
      ],
      "metadata": {
        "id": "NdKtMngE6aiR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Linear Regession Gradient Descent univariate Simplified Steps*:\n",
        "1. Pick a value for 'b' and 'm'\n",
        "2. Calculate the slope of MSE with respect to 'b' and 'm' (Usign slope, instead of comparing 2 MSE)\n",
        "3. The slope or rate of change can be used to figure out whether 'b' or 'm' was too high or low? if both very small, we're done!\n",
        "4. Multiply both by learning rate\n",
        "5. Subtract that from 'b' and 'm' -> go to step 2"
      ],
      "metadata": {
        "id": "06StVsEaCZxQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating the slope of MSE with repect to M and B in one step:\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\text{Features}^T * ((\\text{Features} * \\text{Weights}) - \\text{Labels})}{n}\n",
        "\\end{equation}\n",
        "\n",
        "Where: `Labels` is tensor of our label data, `Features` is tensor of our feature data, `n` number of obeservations, and `weights` is 'M' and 'B' in a tensor"
      ],
      "metadata": {
        "id": "gVoUuI7XkKUv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Equation of Stochastic Gradient Descent*\n",
        "\n",
        "\\begin{equation}\n",
        "\\theta_{j} := \\theta_{j} - \\alpha \\frac{\\partial}{\\partial \\theta_{j}} J(\\theta_{0}, \\theta_{1}, \\dots, \\theta_{n})\n",
        "\\end{equation}\n",
        "\n",
        "where $\\theta_{j}$ is the parameter to be updated, $\\alpha$ is the learning rate, and $J(\\theta_{0}, \\theta_{1}, \\dots, \\theta_{n})$ is the cost function. The derivative of the cost function with respect to the parameter $\\theta_{j}$ is computed and used to update the value of $\\theta_{j}$."
      ],
      "metadata": {
        "id": "AW2SgAeH901f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loss Function\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "resources:\n",
        "* [How to Choose Loss Functions When Training Deep Learning Neural Networks](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/)"
      ],
      "metadata": {
        "id": "tgylCsjI6UDs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Empirical Loss**\n",
        "\n",
        "The empirical loss measures the total loss Over our entire dataset.\n",
        "Also known as **Objective Fucntion**, **Cost Function**, **Empirical Risk**.\n",
        "\n",
        "$$\n",
        "J(W)=\\frac{1}{n}\\sum^{n}_{i=1}\\mathcal{L}(f(x^{(i)};W), y^{(i)})\n",
        "$$\n",
        "\n",
        "* $f(x^{(i)};W)$: Prediceted\n",
        "* $y^{(i)}$: Actual\n",
        "\n"
      ],
      "metadata": {
        "id": "MsZj9cIYY4dJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Regression Loss Functions**"
      ],
      "metadata": {
        "id": "iOYd6N6EfWq_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Mean Square Error Loss (MSE)*\n",
        "\n",
        "The Mean Squared Error, or MSE, loss is the default loss to use for regression problems.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Mean squared error is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0. The squaring means that larger mistakes result in more error than smaller mistakes, meaning that the model is punished for making larger mistakes.\n",
        "\n",
        "keras: `mean_squared_error`"
      ],
      "metadata": {
        "id": "efhGe2hBfZeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "\\text{Mean Squared Error} = \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{y_{i}} - y_{i})^2\n",
        "\\end{equation}\n",
        "\n",
        "where $n$ is the number of samples, $y_{i}$ is the true value of the target variable for the $i^{th}$ sample, and $\\hat{y_{i}}$ is the predicted value of the target variable for the $i^{th}$ sample. The MSE is a measure of the average squared difference between the predicted and true values of the target variable."
      ],
      "metadata": {
        "id": "fWNMVXlZ-Dow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vectorized Mean Squared Error:*\n",
        "\n",
        "\\begin{equation}\n",
        "\\text{VMSE} = \\frac{\\text{sum}( ((\\text{Features} * \\text{Weights}) - \\text{Lables})^2}{n}\n",
        "\\end{equation}\n"
      ],
      "metadata": {
        "id": "JCnbkjbfQU7V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Equation for the **derivative** of Mean Squared Error (MSE)\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\partial}{\\partial \\hat{y}} MSE = \\frac{2}{n} \\sum_{i=1}^{n} (\\hat{y} - y_i)\n",
        "\\end{equation}\n",
        "\n",
        "\n",
        "where $\\frac{\\partial}{\\partial \\hat{y}} MSE$ is the derivative of the MSE with respect to the predicted value $\\hat{y}$, $n$ is the number of samples, $\\hat{y}$ is the predicted value, and $y_i$ is the true value of the target variable for the $i^{th}$ sample. The derivative of the MSE with respect to the predicted values is used in gradient descent to update the parameters of the model."
      ],
      "metadata": {
        "id": "DPYVl36zKku5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vectorized Equation for the derivative of Mean Squared Error:*\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\text{Features.T} * ((\\text{Features} * \\text{Weights}) - \\text{Labels})}{n}\n",
        "\\end{equation}\n",
        "\n",
        "Where: `Labels` is matrix of our label data, `Features` is matrix of our feature data, `n` number of obeservations, and `weights` is 'M' and 'B' in a matrix"
      ],
      "metadata": {
        "id": "CFfySsYuQ2_q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Mean Squared Logarithmic Error Loss*\n",
        "\n",
        "  There may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error.\n",
        "\n",
        "Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss, or MSLE for short.\n",
        "\n",
        "It has the effect of relaxing the punishing effect of large differences in large predicted values.\n",
        "\n",
        "As a loss measure, it may be more appropriate when the model is predicting unscaled quantities directly. Nevertheless, we can demonstrate this loss function using our simple regression problem.\n",
        "\n",
        "Keras: `mean_squared_logarithmic_error`"
      ],
      "metadata": {
        "id": "s0uhvs0dfy00"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Mean Absolute Error Loss*\n",
        "\n",
        "On some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value.\n",
        "\n",
        "The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is **more robust to outliers**. It is calculated as the average of the absolute difference between the actual and predicted values.\n",
        "\n",
        "keras: `mean_absolute_error`"
      ],
      "metadata": {
        "id": "L9OFgTGggPew"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Binary Classification Loss Functions**"
      ],
      "metadata": {
        "id": "_urvie6nhgbR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Binary Cross-Entropy Loss*\n",
        "\n",
        "Cross-entropy is the default loss function to use for binary classification problems.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set {0, 1}.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for predicting class 1. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `binary_crossentropy`\n",
        "\n",
        "Requires an output activation layer of `sigmoid`"
      ],
      "metadata": {
        "id": "KJcaCabCkRz8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\begin{equation}\n",
        "-\\left(\\frac{1}{n}\\right)\\sum^n_{i=0} \\text{Actual} \\cdot \\log(\\text{Guess}) + (1 - \\text{Actual}) \\cdot \\log(1 - \\text{Guess})\n",
        "\\end{equation}\n",
        "\n",
        "Where Actual is the encoded label value, Guess is the prediction value of sigmoid function, and n is the number of observations "
      ],
      "metadata": {
        "id": "-YKpN-qTz4Vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vectorized Equation for Binary Cross Entropy:*\n",
        "\n",
        "\\begin{equation}\n",
        "-\\frac{1}{n} \\cdot \\left(\\text{Actual}^T \\cdot \\log(\\text{Guess}) + (1 - \\text{Actual})^T \\cdot log(1-\\text{Guess})\\right)\n",
        "\\end{equation}\n",
        "\n",
        "Where: `Actual` is matrix of our label data, `Guess` is matrix of prediction, `n` number of obeservations"
      ],
      "metadata": {
        "id": "S6glSnV78oMF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vectorized Equation for the derivative of Binary Cross Entropy:*\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\text{Features}^T * (\\text{Softmax}((\\text{Features} * \\text{Weights})) - \\text{Labels})}{n}\n",
        "\\end{equation}\n",
        "\n",
        "Where: `Labels` is matrix of our label data, `Features` is matrix of our feature data, `n` number of obeservations, and `weights` is 'M' and 'B' in a matrix"
      ],
      "metadata": {
        "id": "MUV2DzzC1akw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Hinge Loss*\n",
        "An alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models.\n",
        "\n",
        "It is intended for use with binary classification where the target values are in the set **{-1, 1}**.\n",
        "\n",
        "The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.\n",
        "\n",
        "Reports of performance with the hinge loss are mixed, sometimes resulting in better performance than cross-entropy on binary classification problems.\n",
        "\n",
        "keras: `hinge`\n",
        "\n",
        "The output layer of the network must be configured to have a single node with a hyperbolic tangent activation function (keras: `tanh`) capable of outputting a single value in the range [-1, 1]."
      ],
      "metadata": {
        "id": "n7sxDb-dmA4v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Squared Hinge Loss*\n",
        "The hinge loss function has many extensions, often the subject of investigation with SVM models.\n",
        "\n",
        "A popular extension is called the squared hinge loss that simply calculates the square of the score hinge loss. It has the effect of **smoothing the surface** of the error function and making it numerically easier to work with.\n",
        "\n",
        "If using a hinge loss does result in better performance on a given binary classification problem, is likely that a squared hinge loss may be appropriate.\n",
        "\n",
        "As with using the hinge loss function, the target variable must be modified to have values in the set {-1, 1}.\n",
        "\n",
        "keras: `squared_hinge`\n",
        "\n",
        "The output layer must use a single node with a hyperbolic tangent activation function (keras: `tanh`) capable of outputting continuous values in the range [-1, 1]."
      ],
      "metadata": {
        "id": "fu8QrIP4rQO8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Multi-Class Classification Loss Functions**"
      ],
      "metadata": {
        "id": "0A9XLcElsZUr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Multi-Class Cross-Entropy Loss*\n",
        "Cross-entropy is the default loss function to use for multi-class classification problems.\n",
        "\n",
        "In this case, it is intended for use with multi-class classification where the target values are in the set {0, 1, 3, …, n}, where each class is assigned a unique integer value.\n",
        "\n",
        "Mathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.\n",
        "\n",
        "Cross-entropy will calculate a score that summarizes the average difference between the actual and predicted probability distributions for all classes in the problem. The score is minimized and a perfect cross-entropy value is 0.\n",
        "\n",
        "keras: `categorical_crossentropy`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a `softmax` activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "X61eXdGDx2fP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Vectorized Equation for the derivative of Multi Nominal Cross Entropy:*\n",
        "\n",
        "\\begin{equation}\n",
        "\\frac{\\text{Features}^T * (\\text{Sigmoid}((\\text{Features} * \\text{Weights})) - \\text{Labels})}{n}\n",
        "\\end{equation}\n",
        "\n",
        "Where: `Labels` is matrix of combined labels, `Features` is matrix of feature data, `n` number of obeservations, and `weights` is 'M' and 'B' for each category in a matrix"
      ],
      "metadata": {
        "id": "2El8DnZZGPol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Sparse Multiclass Cross-Entropy Loss*\n",
        "A possible cause of frustration when using cross-entropy with classification problems with a **large number of labels** is the one hot encoding process.\n",
        "\n",
        "For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory.\n",
        "\n",
        "Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n",
        "\n",
        "keras: `sparse_categorical_crossentropy`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a `softmax` activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "bXr8oJ9x0C_Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Kullback Leibler Divergence Loss*\n",
        "Kullback Leibler Divergence, or KL Divergence for short, is a measure of how one probability distribution differs from a baseline distribution.\n",
        "\n",
        "A KL divergence loss of 0 suggests the distributions are identical. In practice, the behavior of KL Divergence is very similar to cross-entropy. It calculates how much information is lost (in terms of bits) if the predicted probability distribution is used to approximate the desired target probability distribution.\n",
        "\n",
        "As such, the KL divergence loss function is more commonly used **when using models that learn to approximate a more complex function** than simply multi-class classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input. In this case, KL divergence loss would be preferred. Nevertheless, it can be used for multi-class classification, in which case it is functionally equivalent to multi-class cross-entropy.\n",
        "\n",
        "Keras: `kullback_leibler_divergence`\n",
        "\n",
        "The function requires that the output layer is configured with an n nodes (one for each class) and a softmax activation in order to predict the probability for each class."
      ],
      "metadata": {
        "id": "GNMDVkZQ0sZz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics\n",
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "1yisTFEg5veU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **$R^2$**\n",
        "\n",
        "$R^2$ is the coefficient of determination. 1 means it's 100% accurate, less than 0 means that it's worse than just taking the average. 0-1 means it's learning something.\n",
        "\n",
        "\\begin{equation}\n",
        "R^2 = 1 - \\frac{SS_{\\text{res}}}{SS_{\\text{tot}}}\n",
        "\\end{equation}\n",
        "\n",
        "Where $SS_{\\text{res}}$ is the sum of quares of residuals, and $SS_{\\text{tot}}$ is the total sum of squares.\n",
        "\n",
        "\\begin{equation}\n",
        "SS_{\\text{tot}} = \\sum_{i=1}^n(\\text{Actual}-\\text{Average})^2\n",
        "\\end{equation}\n",
        "\n",
        "$SS_{\\text{tot}}$ has no relation to the predictions. Acts as a baseline accuracy value.\n",
        "\n",
        "\\begin{equation}\n",
        "SS_{\\text{res}} = \\sum_{i=1}^n(\\text{Actual}-\\text{Predicted})^2\n",
        "\\end{equation}\n",
        "\n"
      ],
      "metadata": {
        "id": "4hHt0uXD5xji"
      }
    }
  ]
}
