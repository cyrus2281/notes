{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "u0ap0Y0nr8m-",
        "YmGgpU-LsG2m",
        "92HcrfMvAAnZ",
        "-ksKH_UKAIBC",
        "NZueTxW6AIFg",
        "EEA6Fa1PC-GU",
        "WZygPlVj5a9U",
        "9V1D70z63dLj",
        "uRvqO_Tc4vdm",
        "mb7_jN9K3gN1",
        "sNU8KS2131WX",
        "PtJ1MXxG64cP",
        "ZOP0Xd6045Xj",
        "3UIFNF0F47Ci",
        "v3bFS7nz7va9",
        "tMuqIIejCmgf",
        "Ll7RkCdG9ZDZ",
        "o6pyjehC5nr-",
        "3b5D-QPP587x",
        "wDyvkOc7-Vnq",
        "UcixJu9mgo3D"
      ],
      "authorship_tag": "ABX9TyOeUz+KtUXuWxpFa0DyEHiT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cyrus2281/notes/blob/main/Math/Statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Contents"
      ],
      "metadata": {
        "id": "7mH0_bADdRR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "oxtbCyieRIjF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ">[Contents](#scrollTo=7mH0_bADdRR1)\n",
        "\n",
        ">[Statistics](#scrollTo=eh--wBRornXE)\n",
        "\n",
        ">>[Descriptive Statistics](#scrollTo=u0ap0Y0nr8m-)\n",
        "\n",
        ">>[Inferential Statistics](#scrollTo=YmGgpU-LsG2m)\n",
        "\n",
        ">>[Data](#scrollTo=9V1D70z63dLj)\n",
        "\n",
        ">>>[Data Types](#scrollTo=uRvqO_Tc4vdm)\n",
        "\n",
        ">>>>[Quantitative Data](#scrollTo=mb7_jN9K3gN1)\n",
        "\n",
        ">>>>[Categorical Data](#scrollTo=sNU8KS2131WX)\n",
        "\n",
        ">>>[Binning](#scrollTo=PtJ1MXxG64cP)\n",
        "\n",
        ">>>[Charts](#scrollTo=ZOP0Xd6045Xj)\n",
        "\n",
        ">>>>[Bar Charts](#scrollTo=3UIFNF0F47Ci)\n",
        "\n",
        ">>>>[Histograms](#scrollTo=v3bFS7nz7va9)\n",
        "\n",
        ">>>>[Cumulative Frequency plots](#scrollTo=tMuqIIejCmgf)\n",
        "\n",
        ">>>>[Venn Diagram](#scrollTo=pFXEXD-FQYTM)\n",
        "\n",
        ">>>>[Dot Plot](#scrollTo=Ll7RkCdG9ZDZ)\n",
        "\n",
        ">>>>[Scatter Plot](#scrollTo=74yB7SkrmO_s)\n",
        "\n",
        ">>>>[Pie Charts](#scrollTo=o6pyjehC5nr-)\n",
        "\n",
        ">>>>[Pictographs](#scrollTo=3b5D-QPP587x)\n",
        "\n",
        ">>>>[Box and Whiskers plots](#scrollTo=wDyvkOc7-Vnq)\n",
        "\n",
        ">>[Measure of Spread](#scrollTo=e3V7c4GRy3iU)\n",
        "\n",
        ">>>[Mean (Average)](#scrollTo=92HcrfMvAAnZ)\n",
        "\n",
        ">>>>[Mean of Sum and Difference](#scrollTo=Z20LOZQoIfLh)\n",
        "\n",
        ">>>[Median](#scrollTo=-ksKH_UKAIBC)\n",
        "\n",
        ">>>[Mode](#scrollTo=NZueTxW6AIFg)\n",
        "\n",
        ">>>>[Bimodal](#scrollTo=EEA6Fa1PC-GU)\n",
        "\n",
        ">>>[Frequency](#scrollTo=WZygPlVj5a9U)\n",
        "\n",
        ">>>[Range](#scrollTo=48QbOK-pi_ta)\n",
        "\n",
        ">>>>[Mid-Range](#scrollTo=DEZjmnCokHB1)\n",
        "\n",
        ">>>[InterQuartile Range](#scrollTo=amyYKSLuzUXi)\n",
        "\n",
        ">>>[Outlier](#scrollTo=8NyWTKdibNwG)\n",
        "\n",
        ">>>[Variance](#scrollTo=MV6rxxNKG3IJ)\n",
        "\n",
        ">>>>[Variance of Sum and Difference](#scrollTo=zIsr2NGlKNsg)\n",
        "\n",
        ">>>>[Weighted Variance](#scrollTo=lgcONEiS1mXS)\n",
        "\n",
        ">>>>[Expecation of variance](#scrollTo=AJa-YIpLKtHn)\n",
        "\n",
        ">>>[Standard Deviation](#scrollTo=XIeOyqVtGnwu)\n",
        "\n",
        ">>>[Mean Absolute Deviation (MAD)](#scrollTo=TOf7_s4DltUu)\n",
        "\n",
        ">>>[Z-Score](#scrollTo=2YVGHjFkYQ0N)\n",
        "\n",
        ">>>[Skewness](#scrollTo=rZQLfEBSLiq4)\n",
        "\n",
        ">>>[Kurtosis](#scrollTo=TH0fLRDWN81u)\n",
        "\n",
        ">>>[Correlation](#scrollTo=TpG6IoZcOs7b)\n",
        "\n",
        ">>>>[Correlation Coefficient](#scrollTo=MD0DuTQ7QILW)\n",
        "\n",
        ">>>>[Squared Correlation](#scrollTo=skoZ6WeyRHvH)\n",
        "\n",
        ">>>[Percentiles](#scrollTo=jsmU3S4pZKkl)\n",
        "\n",
        ">>>[Central limit theorem](#scrollTo=E7EJicljfmis)\n",
        "\n",
        ">>>[Standard Error](#scrollTo=93IIAs5lhXg_)\n",
        "\n",
        ">>>[Confidence Interval](#scrollTo=ZFwb5UDLjcgJ)\n",
        "\n",
        ">>>>[Margin of Error](#scrollTo=fY2UTQ6VrXNk)\n",
        "\n",
        ">>>>[Estimating a Population Proportion](#scrollTo=VBbWBe5VrR98)\n",
        "\n",
        ">>>[T-Statistics](#scrollTo=d-UOZShNzCYD)\n",
        "\n",
        ">>>>[Degree of Freedom](#scrollTo=01Q9YfBz50up)\n",
        "\n",
        ">>>[Z-Statistics](#scrollTo=VGUGmQWjjKpJ)\n",
        "\n",
        ">>[Regression](#scrollTo=eFjGrbkyZcQI)\n",
        "\n",
        ">>>[Linear Regression](#scrollTo=boPRLYHzYfGV)\n",
        "\n",
        ">>>>[Residuals](#scrollTo=28mC-AnxaFuo)\n",
        "\n",
        ">>>>[Least Squares Regression](#scrollTo=kjJPE3sZaAFz)\n",
        "\n",
        ">>>>>[Regression line formula](#scrollTo=16xum4XdPYia)\n",
        "\n",
        ">>>>[Coefficient of Determination - $r^2$](#scrollTo=wOcgjM2hiKRH)\n",
        "\n",
        ">>>>[Linear Regression using Correlation Coefficient](#scrollTo=n6hgPb3UeO_x)\n",
        "\n",
        ">>>[Covariance and Regression Line](#scrollTo=H-S1GZzIURaW)\n",
        "\n",
        ">>[Anova](#scrollTo=ebIr32MH-CvW)\n",
        "\n",
        ">>>[F-Statistic](#scrollTo=w3xTNg84A29H)\n",
        "\n",
        ">>>[Factorial Anova](#scrollTo=5wdkbWX1-GfB)\n",
        "\n",
        ">>[Distributions](#scrollTo=eerCb1gSBSKR)\n",
        "\n",
        ">>>[Density Curves](#scrollTo=WYCo0cAfN-uE)\n",
        "\n",
        ">>>[Normal Distribution](#scrollTo=P82w9NlgBVFU)\n",
        "\n",
        ">>>>[Probabilty Distribution](#scrollTo=YvHFr7i3a7An)\n",
        "\n",
        ">>>>>[Relative Frequency](#scrollTo=nD_mS4Q7c7cr)\n",
        "\n",
        ">>>>[T-Distribution](#scrollTo=oDO2sbURncmf)\n",
        "\n",
        ">>>[Bimodal Distribution](#scrollTo=aPn4ViLtMN_8)\n",
        "\n",
        ">>>[Binomial Distribution](#scrollTo=XFy1-nTHDcbp)\n",
        "\n",
        ">>>[Bernoulli Distribution](#scrollTo=PKyxjSRDJflN)\n",
        "\n",
        ">>>[Geometric Probability Distribution](#scrollTo=EqIkkOMv-sfB)\n",
        "\n",
        ">>>[Chi Square Distribution](#scrollTo=L20IGOPFQ4v7)\n",
        "\n",
        ">>>>[Pearson's Chi Square Test](#scrollTo=9yCMUuQS7Hal)\n",
        "\n",
        ">[Probability](#scrollTo=yCOk3GoCbD0z)\n",
        "\n",
        ">>[Definitions](#scrollTo=ogMwtqMCz1Bi)\n",
        "\n",
        ">>>[Symbols](#scrollTo=4PXWQfSY6PJ5)\n",
        "\n",
        ">>>[Empirical Probabiliy](#scrollTo=J2EmONgIz2Nt)\n",
        "\n",
        ">>>[Theoretical Probability](#scrollTo=ECMLcAsS0Kck)\n",
        "\n",
        ">>>[Independent](#scrollTo=9hGQSk6h5JoE)\n",
        "\n",
        ">>[Rules & Theories](#scrollTo=iKhpbfAGz-ZO)\n",
        "\n",
        ">>>[Sum Rule](#scrollTo=eNBHdiWO1q5d)\n",
        "\n",
        ">>>[Multiplication Rule](#scrollTo=FVKePYUp1962)\n",
        "\n",
        ">>>[Complement Rule](#scrollTo=68f3oKdhwu6i)\n",
        "\n",
        ">>>[Conditional Probabilites](#scrollTo=iwKV13Kw5h8_)\n",
        "\n",
        ">>>[Bayes' Theorem](#scrollTo=GiDQ16Ui_M7N)\n",
        "\n",
        ">>>>[Bayesian Hypothesis Testing](#scrollTo=hYmgASo2zi-U)\n",
        "\n",
        ">>>>[Bayes Factor](#scrollTo=hZN_do7T0Wnn)\n",
        "\n",
        ">>>[Permutations](#scrollTo=j5kfhAusoyPw)\n",
        "\n",
        ">>>[Combination](#scrollTo=_nEw-yi9m1RD)\n",
        "\n",
        ">>>>[Binomial Coefficient](#scrollTo=qMHYTPUclbej)\n",
        "\n",
        ">>>>>[Binomial Variables](#scrollTo=cGuvRlZYkUsF)\n",
        "\n",
        ">>>[Cumulative Beometric Probability](#scrollTo=KZWR5OFbBUY7)\n",
        "\n",
        ">>>[Poisson Process](#scrollTo=YScKQNEnUrW_)\n",
        "\n",
        ">>[Inference](#scrollTo=3AxJ-s56Qv0-)\n",
        "\n",
        ">>>[Significance Test](#scrollTo=HWdjGr73vT4E)\n",
        "\n",
        ">>>>[Null Hypothesis Significance Testing (NHST)](#scrollTo=jr4zuaIZRssn)\n",
        "\n",
        ">>>>[P-Value](#scrollTo=1Ape6Y7UTPwa)\n",
        "\n",
        ">>>>[Significance Test Errors](#scrollTo=46DKNQU5ixzr)\n",
        "\n",
        ">>>>[Power](#scrollTo=_JbEfKUipz5Q)\n",
        "\n",
        ">>[Bias](#scrollTo=C3uKTXKggmmN)\n",
        "\n",
        ">>>[Allocation Bias](#scrollTo=UcixJu9mgo3D)\n",
        "\n",
        ">>>[Selection Bias](#scrollTo=MT5GYoF9gs-5)\n",
        "\n",
        ">[Resources](#scrollTo=hO6aVIMt58u7)\n",
        "\n"
      ],
      "metadata": {
        "colab_type": "toc",
        "id": "T35rJt36B2wN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Statistics"
      ],
      "metadata": {
        "id": "eh--wBRornXE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Descriptive Statistics\n",
        "\n",
        "> they describe what the data show! Descriptive statistics usually include things like where the middle of the data is, what statisticians call measures of central tendency, and measures of how spread out the data are.\n",
        ">\n",
        "> Describing the data with a smaller set of numbers."
      ],
      "metadata": {
        "id": "u0ap0Y0nr8m-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inferential Statistics\n",
        "\n",
        "> Inferential statistics allows us to make inferences. Inferential statistics allow us to make conclusions that extend beyond the data we have in hand.\n",
        ">\n",
        "> We ask inferential statistics to do all sorts of much more complicated work for us. Inferential statistics let us test an idea or a hypothesis."
      ],
      "metadata": {
        "id": "YmGgpU-LsG2m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "9V1D70z63dLj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Types\n",
        "\n"
      ],
      "metadata": {
        "id": "uRvqO_Tc4vdm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantitative Data\n",
        "\n",
        "> Quantits; Numbers that have both order and consistent spacing"
      ],
      "metadata": {
        "id": "mb7_jN9K3gN1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Categorical Data\n",
        "> Doesn't have a meaningful order or consistent spacing."
      ],
      "metadata": {
        "id": "sNU8KS2131WX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binning\n",
        "> Takes a quantitative variable and bins it into categories that are either pre-existing or made up. For example giving values like low, normal, and high to quantitative values."
      ],
      "metadata": {
        "id": "PtJ1MXxG64cP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Charts"
      ],
      "metadata": {
        "id": "ZOP0Xd6045Xj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bar Charts\n",
        "> Bar charts uses the frequencies\n",
        "\n",
        "![Bar Charts](https://www.mathsisfun.com/data/images/bar-chart-movies.svg)"
      ],
      "metadata": {
        "id": "3UIFNF0F47Ci"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Histograms\n",
        "> Squished together bar charts. The data are 'continuous' which means the values in one bar flow into the next bar. There's no separation like in our categorical bar charts.\n",
        ">\n",
        "> A histogram displays numerical data by grouping data into \"bins\" of equal width. Each bin is plotted as a bar whose height corresponds to how many data points are in that bin.\n",
        "Bins are also sometimes called \"intervals\", \"classes\", or \"buckets\".\n",
        "\n",
        "![Histograms](https://www.mathsisfun.com/data/images/histogram-heights.svg)"
      ],
      "metadata": {
        "id": "v3bFS7nz7va9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Cumulative Frequency plots\n",
        "> Cumulative frequency plots are like histogram but instead of the height of a bar telling you how much of data is in that specific bin, it tells you how much data is in that bin and all previous bins. That's why it's called \"cumulative\".\n",
        "\n",
        "![Cumulative Frequency plots](https://www150.statcan.gc.ca/n1/edu/power-pouvoir/ch10/img/5214862_02-eng.jpg)0"
      ],
      "metadata": {
        "id": "tMuqIIejCmgf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Venn Diagram\n",
        "\n",
        "A Venn diagram is an illustration that uses circles to show the relationships among things or finite groups of things. Circles that overlap have a commonality while circles that do not overlap do not share those traits. Venn diagrams help to visually represent the similarities and differences between two concepts.\n",
        "\n",
        "![Venn Diagram](https://upload.wikimedia.org/wikipedia/commons/e/e4/Venn_diagram_gr_la_ru.svg)"
      ],
      "metadata": {
        "id": "pFXEXD-FQYTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dot Plot\n",
        "> A dotplot takes a histogram and replaces the solid bars which use their height to show their frequency with dots. There's one dot for each data point contained in the bar, so we can just count the number of dots to find out how many there are.\n",
        "\n",
        "![Dot Plot](https://www.mathsisfun.com/data/images/dot-plot-b.svg)"
      ],
      "metadata": {
        "id": "Ll7RkCdG9ZDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Scatter Plot\n",
        "\n",
        "> A scatter plot (aka scatter chart, scatter graph) uses dots to represent values for two different numeric variables. The position of each dot on the horizontal and vertical axis indicates values for an individual data point. Scatter plots are used to observe relationships between variables.\n",
        "\n",
        "![Scatter Plot](https://upload.wikimedia.org/wikipedia/commons/a/af/Scatter_diagram_for_quality_characteristic_XXX.svg)\n",
        "\n",
        "Sometimes the data points in a scatter plot form distinct groups. These groups are called **clusters**."
      ],
      "metadata": {
        "id": "74yB7SkrmO_s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pie Charts\n",
        "> Pie charts are used for categorical data\n",
        "\n",
        "![pie charts](https://www.mathsisfun.com/data/images/pie-chart-movies.svg)"
      ],
      "metadata": {
        "id": "o6pyjehC5nr-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pictographs\n",
        "> Pictographs represent frequency with pictures\n",
        "\n",
        "![Pictographs](https://www.mathsisfun.com/data/images/pictograph.svg)"
      ],
      "metadata": {
        "id": "3b5D-QPP587x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Box and Whiskers plots\n",
        "> Box plots use some the our measures of central tendency and spread to visually display our data\n",
        "\n",
        "Whiskers show the minimum and maximum, the box starts with $Q_1$ and ends with $Q_3$ and has a line in middle at median.\n",
        "\n",
        "![Box and Whiskers](https://www150.statcan.gc.ca/edu/power-pouvoir/fig/fig04-5-2-1-eng.png)\n",
        "\n",
        "![Box plots](https://www.simplypsychology.org/wp-content/uploads/compare-boxplots.jpg)\n",
        "\n",
        "![Box plots and skew](https://www.simplypsychology.org/wp-content/uploads/bloxplots-skewed.jpg)"
      ],
      "metadata": {
        "id": "wDyvkOc7-Vnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Measure of Spread\n",
        "\n"
      ],
      "metadata": {
        "id": "e3V7c4GRy3iU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean (Average)\n",
        "\n",
        "> The mean (or average, or expectation) takes the sum of all the numbers in a data set, and divides by the number of data points.\n",
        ">\n",
        "> Mean is the the balancing point of a dataset.\n",
        "\n",
        "$$\n",
        "\\mu = \\bar{x} = \\frac{1}{n}\\sum_{i=1}^{n}x_i\n",
        "$$\n",
        "\n",
        "Also notated with $\\mu$.\n",
        "\n",
        "Means changes linearly as the dataset is shifted or scaled."
      ],
      "metadata": {
        "id": "92HcrfMvAAnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mean of Sum and Difference\n",
        "\n",
        "$x$ and $y$ are indepenent values.\n",
        "\n",
        "$$\n",
        "E(x) = \\mu_x \\\\\n",
        "E(y) = \\mu_y \\\\\n",
        "E(x+y) = \\mu_x + \\mu_y \\\\\n",
        "E(x-y) = \\mu_x - \\mu_y \\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "Z20LOZQoIfLh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Median\n",
        "\n",
        "> The median is the middle number if we lined up our data from smallest to largest.\n",
        "\n",
        "$$\n",
        "\\text{Median} =\n",
        "\\begin{cases}\n",
        "\\text{middle value}, & \\text{if the number of data points is odd} \\\\\n",
        "\\frac{\\text{sum of the two middle values}}{2}, & \\text{if the number of data points is even}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Median changes linearly as the dataset is shifted or scaled."
      ],
      "metadata": {
        "id": "-ksKH_UKAIBC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mode\n",
        "\n",
        "> The value that appears most (the most frequent number) in our data set.\n",
        "\n",
        "$$\n",
        "\\text{Mode} = \\text{value(s) with the highest frequency}\n",
        "$$\n",
        "\n",
        "The mode is most useful when you have a relatively large sample so that you have a large number of the popular values."
      ],
      "metadata": {
        "id": "NZueTxW6AIFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bimodal\n",
        "\n",
        "> Bimodal data is an example of “Multimodal” data which has many values that are similarly common. Usually multimodal data results from two or more underlying groups all being measured together."
      ],
      "metadata": {
        "id": "EEA6Fa1PC-GU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Frequency\n",
        "> The frequency (f) of a particular value is the number of times the value occurs in the data. The distribution of a variable is the pattern of frequencies, meaning the set of all possible values and the frequencies associated with these values.\n",
        "\n",
        "$$\n",
        "f = \\frac{n}{N}\n",
        "$$"
      ],
      "metadata": {
        "id": "WZygPlVj5a9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Range\n",
        "\n",
        "> Range tells us how spread apart the numbers are. The range is equal to the difference between the largest and smallest data points in a set of numerical data. The smaller the range is, the tighter the spread is.\n",
        "$$\n",
        "\\text{range}(D) = \\max(D) - \\min(D)\n",
        "$$"
      ],
      "metadata": {
        "id": "48QbOK-pi_ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Mid-Range\n",
        "\n",
        "> \"mid-range\" is the average of the biggest number (maximum) and the smallest number (minimum). It's the \"middle of the range\" if all of the numbers were evenly distributed from smallest to largest. mid-range is a measure of central tendency.\n",
        "$$\n",
        "\\text{mid-range}(D) = \\frac{\\max(D) + \\min(D)}{2}\n",
        "$$"
      ],
      "metadata": {
        "id": "DEZjmnCokHB1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### InterQuartile Range\n",
        "\n",
        "> InterQuartile Range (or IQR) doesn’t consider extreme values. The IQR looks at the spread of the middle 50% of your data.\n",
        "For example, in terms of audience, the would be the primary group\n",
        "\n",
        "$$\n",
        "\\text{IQR} = Q_3 - Q_1\n",
        "$$\n",
        "\n",
        "![iqr](https://cdn1.byjus.com/wp-content/uploads/2021/03/interquartile-range.png)\n",
        "\n",
        "\n",
        "IQR remains the same if the dataset is shifted, and it changes linearly as the dataset is scaled."
      ],
      "metadata": {
        "id": "amyYKSLuzUXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier\n",
        "\n",
        "> An outlier is a data point that's way off of where the other data points are. It's way larger or way smaller than where all of the other data points are.\n",
        "\n",
        "$$\n",
        "\\text{outliers} < Q_1 - 1.5 \\cdot \\text{IQR} \\quad \\quad\n",
        "\\text{outliers} > Q_3 + 1.5 \\cdot \\text{IQR}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "8NyWTKdibNwG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variance\n",
        "\n",
        "> The average of the squared differences from the Mean. Gives us a sense of how spread out the whole dataset is.\n",
        "\n",
        "To calculate the variance follow these steps (On total population)\n",
        "\n",
        "1. Work out the Mean (the simple average of the numbers)\n",
        "2. Then for each number: subtract the Mean and square the result (the squared difference).\n",
        "3. Then work out the average of those squared differences.\n",
        "\n",
        "\n",
        "If measuring on a sample of all data, variance is devided by N-1 while calculating the average\n",
        "\n",
        "**Population** variance:\n",
        "$$\n",
        "\\sigma^2 =\\frac{1}{N}\\sum_{i=1}^{N}(x_{i}-\\mu)^{2} \\\\\n",
        "$$\n",
        "\n",
        "**Sample** variance: (Unbiased)\n",
        "$$\n",
        "\\sigma^2 = \\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\mu)^{2} \\\\\n",
        "$$\n",
        "\n",
        "Since sample is an estimate of the whole population, we'd have a more accurate result if we divide by one less that the number of the population.\n",
        "\n",
        "Dividing a sample set by n would result in a biased result since variance is not a linear function.\n",
        "\n",
        "Variance remains the same if the dataset is shifted, and it changes linearly as the dataset is scaled."
      ],
      "metadata": {
        "id": "MV6rxxNKG3IJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Variance of Sum and Difference\n",
        "\n",
        "$x$ and $y$ are indepenent values.\n",
        "\n",
        "$$\n",
        "\\text{Var}(X) = \\sigma^2_X \\\\\n",
        "\\text{Var}(Y) = \\sigma^2_Y \\\\\n",
        "\\text{Var}(X+Y) = \\sigma^2_{X + Y} =\\sigma^2_X + \\sigma^2_Y \\\\\n",
        "\\text{Var}(X-Y) = \\sigma^2_{X - Y} = \\sigma^2_X + \\sigma^2_Y \\\\\n",
        "$$\n",
        "\n",
        "Even when we subtract two random variables, we still add their variances; subtracting two variables increases the overall variability in the outcomes.\n",
        "\n",
        "\\\n",
        "\n",
        "We can find the standard deviation of the combined distributions by taking the square root of the combined variances."
      ],
      "metadata": {
        "id": "zIsr2NGlKNsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Weighted Variance\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{Weighted Variance} = \\frac{\\sum_{i=1}^{n} w_i \\cdot (x_i - \\bar{x})^2}{\\sum_{i=1}^{n} w_i}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $n$ is the number of data points.\n",
        "- $x_i$ represents the ith data point.\n",
        "- $w_i$ represents the weight associated with the ith data point.\n",
        "- $\\bar{x}$ is the weighted mean of the data points, calculated as $\\bar{x} = \\frac{\\sum_{i=1}^{n} w_i \\cdot x_i}{\\sum_{i=1}^{n} w_i}$.\n",
        "\n",
        "In this formula, $x_i - \\bar{x}$ represents the deviation of each data point from the weighted mean, and $w_i$ represents the weight associated with each data point.\n",
        "\n",
        "To calculate the weighted variance, substitute the values of $x_i$ and $w_i$ from your dataset into the formula above, calculate the weighted mean $\\bar{x}$ using the given weights, and then compute the summations and perform the division.\n",
        "\n",
        "Note: The weights $w_i$ must be non-negative, and the sum of the weights $(\\sum_{i=1}^{n} w_i)$ should not be zero to avoid division by zero.\n",
        "\n",
        "\\\n"
      ],
      "metadata": {
        "id": "lgcONEiS1mXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example:\n",
        "\n",
        "a dataset with values $x_1 = 2$, $x_2 = 3$, and $x_3 = 5$, and corresponding weights $w_1 = 1$, $w_2 = 2$, and $w_3 = 3$. calculate the weighted variance.\n",
        "\n",
        "\n",
        "$\\displaystyle{\n",
        "\\bar{x} = \\frac{(1 \\cdot 2) + (2 \\cdot 3) + (3 \\cdot 5)}{1 + 2 + 3} = \\frac{2 + 6 + 15}{6} = \\frac{23}{6} \\approx 3.83 \\\\[1cm]\n",
        "\\text{Weighted Variance} = \\frac{(1 \\cdot (2 - 3.83)^2) + (2 \\cdot (3 - 3.83)^2) + (3 \\cdot (5 - 3.83)^2)}{1 + 2 + 3} \\\\\n",
        "= \\frac{(1 \\cdot 3.4089) + (2 \\cdot 0.7396) + (3 \\cdot 1.4889)}{6} \\\\\n",
        "= \\frac{3.4089 + 1.4792 + 4.4667}{6} \\\\\n",
        "= \\frac{9.3548}{6} \\approx 1.5591 \\\\\n",
        "}$\n"
      ],
      "metadata": {
        "id": "VzKZowbt4aN6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Expecation of variance\n",
        "\n",
        "> Variance is also an expecation. It tells us how spread out we expect the data to be. Also known as **second moment** of data.\n",
        "\n",
        "$$\n",
        "E(x-\\mu)^2 = \\sum (x-\\mu)^2\n",
        "$$"
      ],
      "metadata": {
        "id": "AJa-YIpLKtHn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Deviation\n",
        "\n",
        "> The average amount we expect a point to differ (or deviate) from the mean. The measure of how spread out numbers are.\n",
        "\n",
        "Its symbol is $\\sigma$ (the greek letter sigma)\n",
        "\n",
        "**Population** Standard Deviation:\n",
        "$$\n",
        "\\sigma = \\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}(x_{i}-\\mu)^{2}}\n",
        "$$\n",
        "\n",
        "**Sample** Standard Deviation: (Unbiased)\n",
        "$$\n",
        "\\sigma = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(x_{i}-\\mu)^{2}}\n",
        "$$\n",
        "\n",
        "Standard Deviation remains the same if the dataset is shifted, and it changes linearly as the dataset is scaled."
      ],
      "metadata": {
        "id": "XIeOyqVtGnwu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mean Absolute Deviation (MAD)\n",
        "\n",
        "> Mean absolute deviation (MAD) of a data set is the average distance between each data value and the mean. Mean absolute deviation is a way to describe variation in a data set. Mean absolute deviation helps us get a sense of how \"spread out\" the values in a data set are.\n",
        "$$\n",
        "\\text{MAD} =\\frac{1}{N}\\sum_{i=1}^{N}|x_{i}-\\mu| \\\\\n",
        "$$"
      ],
      "metadata": {
        "id": "TOf7_s4DltUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Z-Score\n",
        "\n",
        "> The distance between the mean of a distribution and a data point in a standard deviation (scaled normalized) distribution is called the z-score. In other words, how many standard deviation away from the mean.\n",
        "$$\n",
        "z = \\frac{x - \\mu}{\\sigma} \\\\\n",
        "$$\n",
        "\n",
        "- A positive z-score means that above the mean.\n",
        "- A negative z-score means that below the mean.\n",
        "- A zero z-score means that equal to the mean."
      ],
      "metadata": {
        "id": "2YVGHjFkYQ0N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Skewness\n",
        "\n",
        "> Tells us whether there are more extreme values on one side. If there are a lot more extreme values smaller than the mean, skewness will tend to be negative. On the other hand, if there are a lot more values bigger than the mean, skewness will tend to be positive.\n",
        "\n",
        "$$\n",
        "\\tilde \\mu_3 = \\frac{\\sum^N_i (X_i-\\bar{X})^3}{(N-1) \\; \\sigma^3}\n",
        "$$\n",
        "\n",
        "\n",
        "$\\tilde {\\mu }_{3}$\t=\tskewness\n",
        "\n",
        "${N}$\t=\tnumber of variables in the distribution\n",
        "\n",
        "$X_{i}$\t=\trandom variable\n",
        "\n",
        "$\\bar{X}$\t=\tmean of the distribution\n",
        "\n",
        "$\\sigma$\t=\tstandard deviation\n"
      ],
      "metadata": {
        "id": "rZQLfEBSLiq4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Kurtosis\n",
        "> The measurement of how thick the tails on a distriubtion are. This tells how common it is to have values that are really far from the mean.\n",
        "\n",
        "$$\n",
        "\\mathrm{Kurt} =\\frac{\\mu_{4}}{\\sigma^{4}}\n",
        "$$\n",
        "\n",
        "$\\mathrm{Kurt}$\t=\tkurtosis\n",
        "\n",
        "$\\mu_{4}$\t=\tfourth central moment\n",
        "\n",
        "$\\sigma^{4}$\t=\tstandard deviation\n"
      ],
      "metadata": {
        "id": "TH0fLRDWN81u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Correlation\n",
        "> Measures the way two variables move together, both the direction and closeness of their movement. Correlation doesn't equal causation.\n",
        "\n",
        "- When the y variable tends to **increase** as the x variable **increases**, we say there is a **positive correlation** between the variables.\n",
        "\n",
        "- When the y variable tends to **decrease** as the x variable **increases**, we say there is a **negative correlation** between the variables.\n",
        "\n",
        "- When there is **no clear relationship** between the two variables, we say there is **no correlation** between the two variables."
      ],
      "metadata": {
        "id": "TpG6IoZcOs7b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Correlation Coefficient\n",
        "\n",
        "> The correlation coefficient $r$ measures the direction and strength of a linear relationship.\n",
        "\n",
        "We use standard deviations to scale the correlation so that it is always between -1 and 1. This is the correlation coefficient, $r$.\n",
        "\n",
        "$$\n",
        "r = \\frac{1}{n-1} \\sum\n",
        "\\underbrace{\\left ( \\frac{x_i - \\bar x}{S_x} \\right )}_{z_x} \\\n",
        "\\underbrace{\\left ( \\frac{y_i - \\bar y}{S_y} \\right )}_{z_y}\n",
        "$$\n",
        "\n",
        "- $S$ is the sample standard deviation\n",
        "\n",
        "\n",
        "$$\n",
        "r = \\frac{n(\\sum{xy})-(\\sum{x})(\\sum{y})}\n",
        "{\\sqrt{ [n \\sum{x^2}-(\\sum{x})^2 ][n \\sum{y^2}-(\\sum{y})^2 }]} \\\\\n",
        "$$\n",
        "\n",
        "- $r=1$ means a positive correlation (max value)\n",
        "- $r=-1$ means a negative correlation (min value)\n",
        "\n",
        "\n",
        "![correlation](https://d138zd1ktt9iqe.cloudfront.net/media/seo_landing_files/diksha-q-how-to-calculate-correlation-coefficient-01-1609233340.png)"
      ],
      "metadata": {
        "id": "MD0DuTQ7QILW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Squared Correlation\n",
        "\n",
        "> $r^2$ is always between 0 and 1, and tells us, in decimal form, how much of the variance in one variable is predicted by the other. A measure of how accurate our prediction would be.\n",
        "\n",
        "$$\n",
        "R^2 = \\frac{\\text{SSR}}{\\text{SST}} = \\frac{\\sum (\\hat{y_i}-\\bar{y})^2}{\\sum (y_i-\\bar{y})^2}\n",
        "$$\n",
        "\n",
        "$\\hat{y}$ represents the prediction or a point on the regression line, $\\bar{y}$ represents the mean of all the values and $y_i$ represents the actual values or the points."
      ],
      "metadata": {
        "id": "skoZ6WeyRHvH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Percentiles\n",
        "\n",
        "> Percentiles tell you what percentage of the populcation is at or lower than a score.\n",
        ">\n",
        "> A percentile is a term that describes how a score compares to other scores from the same set.\n",
        "\n",
        "\n",
        "Example:\n",
        "What percentil of the values are below or at 7.\n",
        "\n",
        "$\n",
        "D = \\{ 1, 3, 4, 5, 5, 7, 9, 10, 12, 13 \\} \\\\\n",
        "\\text{percentil}_7 = \\frac{6}{10} = 60^{\\text{th}}\n",
        "$"
      ],
      "metadata": {
        "id": "jsmU3S4pZKkl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Central limit theorem\n",
        "\n",
        "> The distribution of sample means for an independent, random variable, will get closer and closer to a normal distribution as the size of the samples gets bigger and bigger, even if the original population distribution isn't normal itself."
      ],
      "metadata": {
        "id": "E7EJicljfmis"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\sigma_{\\text{sampling distribution}} = \\frac{\\sigma_{\\text{population}}}{\\sqrt{n}}\n",
        "$$"
      ],
      "metadata": {
        "id": "Fhd4GLirh5QZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard Error\n",
        "> Standard error tells us the average distance between a sample mean and the true mean of a distribution.\n",
        "\n",
        "$$\n",
        "\\sigma^2_{\\bar x} = \\frac{\\sigma^2}{n}\n",
        "$$\n",
        "\n",
        "- $\\sigma^2 _{\\bar x}$: Variance of the sampling distribution of the sample mean\n",
        "- $\\sigma^2$: Variance of the original distribution\n",
        "- $n$: number of samples\n",
        "\n",
        "The standard deviation of the sampling distribution of the sample mean is often called the standard deviation of the mean or the standard error of the mean\n",
        "\n",
        "$$\n",
        "\\sigma_{\\bar x} = \\frac{\\sigma}{\\sqrt{n}} = \\sqrt{\\frac{\\hat p (1- \\hat p)}{n}}\n",
        "$$"
      ],
      "metadata": {
        "id": "93IIAs5lhXg_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confidence Interval\n",
        "\n",
        "> A confidence interval is used to estimate an unknown population parameter, such as the mean, proportion, or variance, based on a sample from the population. It provides a range of values that is likely to contain the true population parameter, along with a level of confidence associated with this interval.\n",
        "\n",
        "The confidence level refers to the long-term success rate of the method, that is, how often this type of interval will capture the parameter of interest.\n",
        "\n",
        "A specific confidence interval gives a range of plausible values for the parameter of interest.\n",
        "\n",
        "$$\n",
        "\\text{Confidence Interval}=\\text{Sample Mean} \\pm (\\text{Critical Value}\\times \\text{Standard Error}) \\\\\n",
        "\\text{Confidence Interval}=\\hat p \\pm z^* \\cdot \\sqrt{\\frac{\\hat p (1- \\hat p)}{n}}\n",
        "$$\n",
        "\n",
        "where the critical value is obtained from a standard normal distribution table based on the chosen confidence level, and the standard error is the standard deviation of the sample divided by the square root of the sample size.\n",
        "\n",
        "\\\n",
        "\n",
        "**Mean Confidence Interval**\n",
        "\n",
        "An estimated range of values that seem reasonable based on what we've observed. It's center is still the sample mean, but we've got some room on either side for our uncertainity.\n",
        "Confidence intervals quantify our uncertainity. They also demonstrate the tradeoff of accuracy for precision.\n",
        "\n",
        "The 95% in a 95% confidence interval tells us that if we calculated a confidence interval from 100 different samples, about 95 of them would contain the true population mean.\n",
        "\n",
        "Our \"confidence\" is in the fact that the procedure of calculating this confidence interval will only exlcude the population mean 5% of the time."
      ],
      "metadata": {
        "id": "ZFwb5UDLjcgJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Margin of Error\n",
        "\n",
        "> The margin or error is usually telling you how far the bounds of the confidence interval are from the mean, and is represented by this part of the confidence interval formula:\n",
        "\n",
        "The margin of error, just like a confidence interval, reflects the uncertainty that surrounds sample estimates of parameters like the mean or a proportion.\n",
        "\n",
        "$$\n",
        "\\text{MOE}_{\\gamma}=z_{\\gamma} \\times  \\text{Standard error} \\\\\n",
        "\\text{MOE}_{\\gamma}=z_{\\gamma} \\times \\sqrt{\\frac{\\sigma^{2}}{n}}\n",
        "$$\n",
        "\n",
        "- $\\text{MOE}$\t=\tmargin of error\n",
        "- $\\gamma$\t=\tconfidence level\n",
        "- $z_{\\gamma}$\t=\tquantile\n",
        "- $\\sigma$\t=\tstandard deviation\n",
        "- ${n}$\t=\tsample size\n"
      ],
      "metadata": {
        "id": "fY2UTQ6VrXNk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Estimating a Population Proportion\n",
        "\n",
        "When we want to carry out inferences on one proportion (build a confidence interval or do a significance test), the accuracy of our methods depend on a few conditions.\n",
        "\n",
        "The conditions we need for inference on one proportion are:\n",
        "\n",
        "- **Random**: The data needs to come from a random sample or randomized experiment.\n",
        "\n",
        "- **Normal**: The sampling distribution of\n",
        "$\\hat p$ needs to be approximately normal — needs at least $10$ expected successes and $10$ expected failures.\n",
        "\n",
        "- **Independent**: Individual observations need to be independent. If sampling without replacement, our sample size shouldn't be more than $10\\%$ of the population.\n",
        "\n",
        "\\\n"
      ],
      "metadata": {
        "id": "VBbWBe5VrR98"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**The random condition**\n",
        "\n",
        "Random samples give us unbiased data from a population. When samples aren't randomly selected, the data usually has some form of bias, so using data that wasn't randomly selected to make inferences about its population can be risky.\n",
        "\n",
        "More specifically, sample proportions are unbiased estimators of their population proportion.\n",
        "\n",
        "Biased samples lead to inaccurate results, so they shouldn't be used to create confidence intervals or carry out significance tests.\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "**The normal condition**\n",
        "\n",
        "The sampling distribution of $\\hat p$ is approximately normal as long as the expected number of successes and failures are both at least $10$. This happens when our sample size $n$ is reasonably large.\n",
        "\n",
        "So we need:\n",
        "\n",
        "$$\\begin{aligned}\n",
        "&\\text{expected successes: } np \\geq 10 \\\\\n",
        "&\\text{expected failures: } n(1-p) \\geq 10\n",
        "\\end{aligned}$$\n",
        "\n",
        "If we are building a confidence interval, we don't have a value of\n",
        "$p$ to plug in, so we instead count the observed number of successes and failures in the sample data to make sure they are both at least $10$. If we are doing a significance test, we use our sample size $n$ and the hypothesized value of $p$ to calculate our expected numbers of successes and failures.\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "**The independence condition**\n",
        "To use the formula for standard deviation of $\\hat p$, we need individual observations to be independent. When we are sampling without replacement, individual observations aren't technically independent since removing each item changes the population.\n",
        "But the $10\\%$ condition says that if we sample $10\\%$ or less of the population, we can treat individual observations as independent since removing each observation doesn't significantly change the population as we sample.\n",
        "This allows us to use the formula for standard deviation of $\\hat p$:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\hat p}=\\sqrt{\\frac{p(1-p)}{n}}\n",
        "$$\n",
        "\n",
        "If we are building a confidence interval for $p$, we don't actually know what $p$ is, so we substitute $\\hat p$ as an estimate for $p$. When we do this, we call it the standard error of $\\hat p$ to distinguish it from the standard deviation.\n",
        "So our formula for standard error of $\\hat p$ is:\n",
        "\n",
        "$$\n",
        "\\sigma_{\\hat p}\\approx\\sqrt{\\frac{\\hat p(1-\\hat p)}{n}}\n",
        "$$"
      ],
      "metadata": {
        "id": "zLM1RyTlsw6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T-Statistics\n",
        "\n",
        "T-statistics is used in hypothesis testing when the population standard deviation is unknown and the sample size is relatively small. It's a way to test if the means of two groups are significantly different from each other.\n",
        "\n",
        "$$\n",
        "\\text{mean} \\pm t \\text{-value} \\times \\text{standard error} \\\\\n",
        "\\mu = \\bar x \\pm t^* \\cdot \\frac{s}{\\sqrt{n}}\\\\\n",
        "t = \\frac{\\bar{x} - \\mu}{\\frac{s}{\\sqrt{n}}}\n",
        "$$\n",
        "\n",
        "- $\\bar{x}$ is the sample mean.\n",
        "- $s$ is the sample standard deviation.\n",
        "- $n$ is the sample size.\n",
        "- $\\mu$  is the population mean.\n",
        "\n",
        "The t-statistic measures how many standard errors the sample mean is away from the population mean. A large absolute value of t suggests that the sample mean is far from the population mean, and this is used to assess whether the difference is statistically significant.\n",
        "\n",
        "![t-table](https://www.tdistributiontable.com/wp-content/uploads/2020/08/t-table.png)"
      ],
      "metadata": {
        "id": "d-UOZShNzCYD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The conditions we need for inference on a mean are:\n",
        "\n",
        "- **Random**: A random sample or randomized experiment should be used to obtain the data.\n",
        "\n",
        "- **Normal**: The sampling distribution of $\\bar x$ (the sample mean) needs to be approximately normal. This is true if our parent population is normal or if our sample is reasonably large $(n \\geq 30)$.\n",
        "\n",
        "- **Independent**: Individual observations need to be independent. If sampling without replacement, our sample size shouldn't be more than $10\\%$ of the population.\n",
        "\n",
        "> Assuming independence between observations allows us to use this formula for standard deviation of\n",
        "$\\bar x$ when we're making confidence intervals or doing significance tests:\n",
        "$$\n",
        "\\sigma_{\\bar x}=\\frac{\\sigma}{\\sqrt n}\n",
        "$$\n",
        "We usually don't know the population standard deviation $\\sigma$, so we substitute the sample standard deviation $s_x$ as an estimate for $\\sigma$. When we do this, we call it the standard error of $\\bar x$ to distinguish it from the standard deviation.\n",
        "So our formula for standard error of $\\bar x$ is:\n",
        "$$\n",
        "\\sigma_{\\bar x}\\approx\\frac{s_x}{\\sqrt n}\n",
        "$$"
      ],
      "metadata": {
        "id": "dkkXSYjM3_09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Degree of Freedom\n",
        "\n",
        "> The degrees of freedom ( $\\text{df}$ ) represent the number of values in a statistical calculation that are free to vary. In the case of the t-distribution, the degrees of freedom are $N-1$ as one degree of freedom is reserved for estimating the mean, and $N-1$ degrees remain for estimating the variability.\n",
        "$$\n",
        "\\text{df} = N - 1\n",
        "$$"
      ],
      "metadata": {
        "id": "01Q9YfBz50up"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Z-Statistics\n",
        "\n",
        "**Population Standard Deviation Known:**\n",
        "Z-statistics are used when the population standard deviation $\\sigma$ is known. In real-world scenarios, however, the population standard deviation is often unknown. Z-statistics are based on the standard normal distribution, where the critical values (or cutoff points) are fixed.\n",
        "\n",
        "$$\n",
        "Z = \\frac{\\bar{x} - \\mu}{\\frac{\\sigma}{\\sqrt{n}}}\n",
        "$$\n",
        "\n",
        "- $\\bar{x}$ is the sample mean.\n",
        "- $\\mu$ is the population mean.\n",
        "- $\\sigma$ is the population standard deviation.\n",
        "- $n$ is the sample size.\n",
        "\n",
        "**Z-statistics vs. T-statistics:**\n",
        "\n",
        "1. **Assumption about Population Standard Deviation:**\n",
        "   - Z-statistics: Assumes the population standard deviation is known.\n",
        "   - T-statistics: Does not assume knowledge of the population standard deviation; it estimates it from the sample.\n",
        "\n",
        "2. **Distribution:**\n",
        "   - Z-statistics: Follows the standard normal distribution.\n",
        "   - T-statistics: Follows the t-distribution, which varies based on sample size (degrees of freedom).\n",
        "\n",
        "3. **Sample Size:**\n",
        "   - Z-statistics: Suitable for large sample sizes (typically $n > 30$).\n",
        "   - T-statistics: Suitable for small sample sizes, especially when the population standard deviation is unknown.\n",
        "n from the sample.\n",
        "\n",
        "\n",
        "![z-table](https://cdn1.byjus.com/wp-content/uploads/2017/09/word-image3.png)"
      ],
      "metadata": {
        "id": "VGUGmQWjjKpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Regression"
      ],
      "metadata": {
        "id": "eFjGrbkyZcQI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "When we see a relationship in a scatterplot, we can use a line to summarize the relationship in the data. We can also use that line to make predictions in the data. This process is called linear regression.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "boPRLYHzYfGV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Residuals\n",
        "\n",
        "> The residual for each observation is the difference between predicted values of y (dependent variable) and observed values of y .\n",
        "\n",
        "$$\n",
        "\\text{Residual}=\\text{actual }y - \\text{predicted }y \\\\\n",
        " r_i=y_i-\\hat y_i\n",
        "$$"
      ],
      "metadata": {
        "id": "28mC-AnxaFuo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Least Squares Regression\n",
        "\n",
        "Finding an $m$ and a $b$ for the line equation $y=mx+b$ that produces the least sum of square residuals.\n",
        "\n",
        "$r^2$ measures how much prediction error is eliminated when we use least-squares regression.\n",
        "\n",
        "$$\n",
        "\\sum (r_n)^2\n",
        "$$"
      ],
      "metadata": {
        "id": "kjJPE3sZaAFz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$\n",
        "D = \\{(x_1,y_1), (x_2, y_2), \\cdots, (x_, y_n) \\} \\\\[.5cm]\n",
        "\\text{SE}_\\text{ line} = \\sum (y_i - (mx_i + b))^2\n",
        "$$\n",
        "\n",
        "- $\\text{SE}_\\text{ line}$: squared error line\n",
        "\n",
        "\\\n",
        "\n",
        "How much (what %) of the total variation in $y$ is described by the variation in $x$. (variance)\n",
        "\n",
        "$$\n",
        "\\text{total variation in } y = \\sum (y_i - \\bar y)^2 = \\text{SE}_{\\bar y}\n",
        "$$\n",
        "\n",
        "> note: The total variance in the $y$ value is actually the maximum possible error you can ever have. If $\\text{SE}_\\text{ line}$ is equal to $\\text{SE}_{\\bar y}$, it means that there is absolutely no $y$ value that is dependent on an $x$ value. And you can see that in the formula used to calculate the total $y$ variance. In that formula, there is no $x$ variable. Also, the $\\bar y$ line has a slope of 0, so its equation is $\\bar y = 0\\times x + b$. You can see that $\\bar y$ will stay the same, no matter what value $x$ is. It is completely independent of $x$. So the $\\text{SE}_\\text{ line}$ will never be higher than the biggest ever possible error ( the total $y$ variance). And the whole logic behind the fraction $\\frac{\\text{SE}_\\text{ line}}{\\text{SE}_{\\bar y}}$ is to see what part of the maximum possible error $\\text{SE}_{\\bar y}$ is the error of the regression line $\\text{SE}_\\text{ line}$. Then after you know what percentage of the maximum error possible, represents the error of the regression line, you can find what percentage is good.\n",
        "\n",
        "\\\n",
        "\n",
        "\n",
        "How much of the total variation is _not_ described by the regression line?\n",
        "\n",
        "$$\n",
        "\\frac{\\text{SE}_\\text{ line}}{\\text{SE}_{\\bar y}}\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "What percentage of total varaition is described by the variation in $x$\n",
        "\n",
        "$$\n",
        "r^2 = 1-\\frac{\\text{SE}_\\text{ line}}{\\text{SE}_{\\bar y}}\n",
        "$$\n",
        "\n",
        "$\\uparrow$ this is the Coefficient of Determination"
      ],
      "metadata": {
        "id": "UmmcwM4NlQmW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Regression line formula\n",
        "\n",
        "$$\n",
        "\\text{SE}_{\\text{ line}} = n \\bar y^2 - 2mn \\bar{xy} - 2bn\\bar y + m^2 n\\bar x^2 + 2mbn \\bar x + nb^2 \\\\[1cm]\n",
        "m = \\frac{\\bar x\\bar y - \\bar{xy}}{(\\bar x)^2 - \\bar x^2} \\\\[1cm]\n",
        "b = \\bar y - m \\bar x \\\\[1cm]\n",
        "y = mx + b\n",
        "$$"
      ],
      "metadata": {
        "id": "16xum4XdPYia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Coefficient of Determination - $r^2$\n",
        "\n",
        "**R-squared** tells us what percent of the prediction error in the $y$ variable is eliminated when we use least-squares regression on the $x$ variable.\n",
        "\n",
        "As a result, $r^2$ is also called the **coefficient of determination**.\n",
        "\n",
        "$r^2$ tells us what percent of the variability in the $y$ variable is accounted for by the regression on the $x$ variable."
      ],
      "metadata": {
        "id": "wOcgjM2hiKRH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear Regression using Correlation Coefficient\n",
        "\n",
        "The slope for a linear regression model can be calculated using the correlation coefficient through:\n",
        "\n",
        "$$\n",
        "m = r \\frac{S_y}{S_x} \\\\\n",
        "\\hat y = mx + b\n",
        "$$\n",
        "\n",
        "- S  is the sample standard deviation\n",
        "- r is correlation coefficient\n"
      ],
      "metadata": {
        "id": "n6hgPb3UeO_x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Covariance and Regression Line\n",
        "\n",
        "Covariance measures the direction of the relationship between two variables. A positive covariance means that both variables tend to be high or low at the same time. A negative covariance means that when one variable is high, the other tends to be low.\n",
        "\n",
        "\\\n",
        "\n",
        "$$\n",
        "\\text{Cov}(x,y) = E[(x-E[x])(y-E[y])] \\\\\n",
        "= E[xy - xE[y] - E[x]y + E[x]E[y]] \\\\\n",
        "= E[xy] - E[y]E[x] - E[x]E[y] + E[x]E[y] \\\\[1cm]\n",
        "= E[xy] - E[y]E[x] \\\\[1cm]\n",
        "E[xy] \\approx \\bar{xy} \\quad E[y]= \\bar y \\quad E[x] = \\bar x \\\\[1cm]\n",
        "\\text{Cov}(x,y) =\\bar{xy} - \\bar y \\bar x\n",
        "$$"
      ],
      "metadata": {
        "id": "H-S1GZzIURaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Anova\n",
        "\n",
        "> **An**alysis **o**f **Va**riance. It's similar to regression expect we're using a categorical variable to predict a continuous one. Like regression, the anova builds a model of how the world works. Anovas allow us to analyze the effect of variables with two or more groups on continuous variables.\n",
        "\n",
        "SST = N x Variance\n",
        "\n",
        "SST = SSR + SSE\n",
        "\n",
        "Data = Model + Error\n",
        "\n",
        "\n",
        "Sums of squares for error: The amount of information that our model doesn't explain."
      ],
      "metadata": {
        "id": "ebIr32MH-CvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### F-Statistic\n",
        "\n",
        "$$\n",
        "\\text{F-Statistic} : \\frac{\\frac{\\text{SSM}}{df_m}}{\\frac{\\text{SS}}{df_E}} = \\frac{\\text{MSE}_{model}}{\\text{MSE}_{error}}\n",
        "$$"
      ],
      "metadata": {
        "id": "w3xTNg84A29H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Factorial Anova\n",
        "\n",
        "> A factorial anova does almost exactly what a regular anova does; It takes the overall variation - or sums of squares - and portions it out into different categories."
      ],
      "metadata": {
        "id": "5wdkbWX1-GfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Distributions\n",
        "\n",
        "> A distribution represents all possible values for a set of data, and how often those values occur."
      ],
      "metadata": {
        "id": "eerCb1gSBSKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Density Curves\n",
        "\n",
        "> A density curve is a graphical representation of a numerical distribution where the outcomes are continuous. In other words, a density curve is the graph of a continuous distribution.\n",
        "\n",
        "Area under the curve is always 1.\n",
        "\n",
        "![Density Curves](https://dr282zn36sxxg.cloudfront.net/datastreams/f-d%3A4c3eb9d37178d02b8da3d7974d8b57cdf8d1f9d7ad723363e52e9a37%2BIMAGE_TINY%2BIMAGE_TINY.1)"
      ],
      "metadata": {
        "id": "WYCo0cAfN-uE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Normal Distribution\n",
        "\n",
        "> “Normal” means a distribution of data that has roughly the same amount of data on either side of the middle, and has its most common values around the middle of the data. Data that are distributed normally will have a symmetrical bell shape.\n",
        "The fact that the median and mean are the same tells us that the distribution is symmetric: there’s equal amounts of data on either side of the median, and equal amounts on either side of the mean."
      ],
      "metadata": {
        "id": "P82w9NlgBVFU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is often called a \"Bell Curve\" because it looks like a bell. And also known as a Gaussion curve\n",
        "\n",
        "* mean = median = mode\n",
        "* symmetry about the centre\n",
        "* 50% of values less than the mean and 50% greater than the mean\n",
        "\n",
        "It is good to know the standard deviation, because we can say that any value is:\n",
        "\n",
        "* likely to be within 1 standard deviation (68 out of 100 should be)\n",
        "* very likely to be within 2 standard deviations (95 out of 100 should be)\n",
        "* almost certainly within 3 standard deviations (997 out of 1000 should be)\n",
        "\n",
        "The number of standard deviations from the mean is also called the \"Standard Score\", \"sigma\" or \"z-score\".\n",
        "\n",
        "To convert a value to a Standard Score (\"z-score\") (Standardize):\n",
        "1. first subtract the mean,\n",
        "2. then divide by the Standard Deviation\n",
        "$$\n",
        "z = \\frac{x - \\mu }{\\sigma }\n",
        "$$\n",
        "\n",
        "* z is the \"z-score\" (Standard Score)\n",
        "* x is the value to be standardised\n",
        "* μ ('mu\") is the mean\n",
        "* σ (\"sigma\") is the standard deviation\n",
        "\n",
        "\\\n",
        "\n",
        "\\\n",
        "\n",
        "Combining two or more indepenet normal distributions will also result in a normal distribution."
      ],
      "metadata": {
        "id": "p8WqfAO_PdI1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![normal distribution](https://www.mathsisfun.com/data/images/normal-distrubution-large.svg)\n",
        "\n",
        "Skew\n",
        "![skew](https://upload.wikimedia.org/wikipedia/commons/c/cc/Relationship_between_mean_and_median_under_different_skewness.png)"
      ],
      "metadata": {
        "id": "RKn-k1o1W0I-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Probabilty Distribution\n",
        "\n",
        "This function produces a normal distribution with any given mean and standard deviation.\n",
        "\n",
        "$$\n",
        "p(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2}\n",
        "$$"
      ],
      "metadata": {
        "id": "YvHFr7i3a7An"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Relative Frequency\n",
        "\n",
        "Relative frequency or the cumulative distribution function is the area under the normal distribution. It give the probablity of the x landing in the given interval.\n",
        "\n",
        "$$\n",
        "\\text{CDF} = \\int_a^b p(x) \\ dx\n",
        "$$"
      ],
      "metadata": {
        "id": "nD_mS4Q7c7cr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### T-Distribution\n",
        "\n",
        "> A continuous probability distribution that's unimodal; It's a useful way to represent sampling distriubtions.\n",
        "The t-distribution changes its shape according to how much information there is. With small sample sizes there’s less information so the t-distribution has thicker tails to represent that our estimates are more uncertain when there’s not much data. However as we get more and more data, the t-distribution becomes identical to the z-distribution.\n",
        "\n",
        "Generally, sample sizes that are greater than 30 are considered “large enough” because scientists generally believe that sampling distributions where the sample is 30+ are close enough to normal...though 30 is an arbitrary cutoff just like 0.05.\n",
        "\n",
        "To convert to a raw score from a t-score:\n",
        "$$\n",
        "\\text{mean} \\pm t \\text{-value} \\times \\text{standard error}\n",
        "$$\n",
        "\n",
        "T-Statistics tells us how many standard errros away from the mean our observed difference is.\n",
        "\n",
        "![t-distribution](https://media.geeksforgeeks.org/wp-content/uploads/20230518132346/8-(1)-min.png)"
      ],
      "metadata": {
        "id": "oDO2sbURncmf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bimodal Distribution\n",
        "\n",
        "> A distribution with 2 peeks.\n",
        "\n",
        "Bimodal distributions could be a result of 2 combined unimodals.\n",
        "\n",
        "![bimodal distribution](https://upload.wikimedia.org/wikipedia/commons/e/e2/Bimodal.png)"
      ],
      "metadata": {
        "id": "aPn4ViLtMN_8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binomial Distribution\n",
        "\n",
        "> Binomial distribution is a statistical probability distribution that states the likelihood that a value will take one of two independent values under a given set of parameters or assumptions.\n",
        "\n",
        "Binomial distribution looks like a district normal distribution\n",
        "\n",
        "**Probability of a binomial coefficient**\n",
        "\n",
        "$$\n",
        "\\binom{n}{k} = _nC_k (p) ^k (1-p)^{n-k}\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Example: Getting 3 heads on 5 flip of a coin\n",
        "\n",
        "$\n",
        "\\binom{5}{3} = _5C_3 (0.5) ^3 (1-0.5)^{5-3} =  _5C_3 (0.125)(0.25) = _5C_3 (0.031) = 10 (0.031) = 0.31\n",
        "$\n",
        "\n",
        "\\\n",
        "\n",
        "Example: Score 2 times in 6 shot where the chance of scoring each attempt is 70%.\n",
        "\n",
        "$\n",
        "\\binom{6}{2} = _6C_2 (0.7) ^2 (1-0.7)^{6-2} =  _6C_2 (0.7)^2(0.3)^4 =  _6C_2 (0.031) = 10 (0.031) = 0.31\n",
        "$\n"
      ],
      "metadata": {
        "id": "XFy1-nTHDcbp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bernoulli Distribution\n",
        "\n",
        "> The Bernoulli distribution is a discrete distribution having two possible outcomes labelled by n=0 and n=1 in which n=1 (\"success\") occurs with probability p and n=0 (\"failure\") occurs with probability q=1-p, where 0<p<1.\n",
        "\n",
        "$$\n",
        "P(n)=p^n(1-p)^{1-n}\n",
        "$$\n",
        "\n",
        "![Bernoulli Distribution](https://mathworld.wolfram.com/images/eps-svg/BernoulliDistribution_1000.svg)"
      ],
      "metadata": {
        "id": "PKyxjSRDJflN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Geometric Probability Distribution\n",
        "\n",
        "> Geometric probabilities tell you the probability that your first success will be on your nth try.\n",
        "\n",
        "$$\n",
        "\\text{geom}(k;p) = (1-p)^{k-1}p\n",
        "$$\n",
        "\n",
        "Example: What's the probability of having a red bead on your 5th try from a bag of 5-color-beads. (the first 4 tries are other colors)\n",
        "$$\n",
        "\\text{geom}(5;0.05)=(0.95)^4(0.05)^1 \\approx 4.07\\%\n",
        "$$"
      ],
      "metadata": {
        "id": "EqIkkOMv-sfB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chi Square Distribution\n",
        "\n",
        "> In probability theory and statistics, the chi-squared distribution with k degrees of freedom is the distribution of a sum of the squares of k independent standard normal random variables.\n",
        "\n",
        "**Chi Square:**\n",
        "\n",
        "$$\n",
        "\\chi^2 = \\sum \\frac{(O_i-E_i)^2}{E_i}\n",
        "$$\n",
        "\n",
        "- E = Expected value\n",
        "- O = Observed value\n",
        "\n",
        "degree of freedom = $n-1$\n",
        "\n",
        "**Probability density function:**\n",
        "\n",
        "$$\n",
        "f(x) = \\begin{cases} \\frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\\Gamma(k/2)} & \\text{for}\\, x \\geq 0 \\\\ 0 & \\text{otherwise} \\end{cases}\n",
        "$$\n",
        "\n",
        "- $f(x)$\t=\tprobability density function\n",
        "- $k$\t=\tdegrees of freedom\n",
        "- $\\Gamma(k/2)$\t=\tgamma function\n",
        "\n",
        "![Chi Square Distribution](https://www.scribbr.com/wp-content/uploads/2022/05/chi-square-distribution-k-example.webp)\n",
        "\n",
        "![chi table](https://www.scribbr.com/wp-content/uploads/2022/05/chi-square-distribution-table.png)\n"
      ],
      "metadata": {
        "id": "L20IGOPFQ4v7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Pearson's Chi Square Test\n",
        "\n",
        "Pearson's chi-square test is a statistical test used to determine whether there is a significant association between categorical variables. It is a non-parametric test, meaning it does not make any assumptions about the distribution of the data.\n",
        "\n",
        "Here's how the test works:\n",
        "\n",
        "1. **Null Hypothesis ($H_0$):** Assumes that there is no significant association between the variables in the population.\n",
        "\n",
        "2. **Alternative Hypothesis ($H_1$):** Assumes that there is a significant association between the variables in the population.\n",
        "\n",
        "3. **Test Statistic:** The chi-square ($\\chi^2$) statistic is calculated based on the differences between the expected and observed frequencies of the categorical variables.\n",
        "\n",
        "4. **Degrees of Freedom:** The degrees of freedom for the chi-square test are calculated based on the number of categories in the variables being studied.\n",
        "\n",
        "5. **Critical Value or P-Value:** The chi-square statistic is compared to a critical value from the chi-square distribution table (or the p-value is calculated) to determine whether to reject the null hypothesis.\n",
        "\n",
        "6. **Interpretation:** If the chi-square statistic is greater than the critical value (or the p-value is less than a chosen significance level, often 0.05), then the null hypothesis is rejected, suggesting that there is a significant association between the categorical variables.\n"
      ],
      "metadata": {
        "id": "9yCMUuQS7Hal"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Probability"
      ],
      "metadata": {
        "id": "yCOk3GoCbD0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definitions"
      ],
      "metadata": {
        "id": "ogMwtqMCz1Bi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symbols\n",
        "\n",
        "$\n",
        "\\bullet \\quad P:  \\text{Probability} \\\\\n",
        "\\bullet \\quad \\cap:  \\text{AND} \\\\\n",
        "\\bullet \\quad \\cup:  \\text{OR} \\\\\n",
        "\\bullet \\quad \\subseteq : \\text{Subset} \\\\\n",
        "\\bullet \\quad \\supseteq : \\text{Superset} \\\\\n",
        "$"
      ],
      "metadata": {
        "id": "4PXWQfSY6PJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Empirical Probabiliy\n",
        "> Something we observe in actual data. Empirical probabilities, like sample statistics, give us a glimpse at the true theoretical probability, but they won’t always be equal to it because of the uncertainty and randomness of any sample."
      ],
      "metadata": {
        "id": "J2EmONgIz2Nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theoretical Probability\n",
        "> More of an ideal or a truth out there that we can't directly see.  The empirical probability can be a good estimation of the theoretical one, even if it’s not exact."
      ],
      "metadata": {
        "id": "ECMLcAsS0Kck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Independent\n",
        "> if the probability of one event occurring is not changed by whether or not the second event occurred"
      ],
      "metadata": {
        "id": "9hGQSk6h5JoE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rules & Theories"
      ],
      "metadata": {
        "id": "iKhpbfAGz-ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sum Rule\n",
        "$$\n",
        "P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\n",
        "$$"
      ],
      "metadata": {
        "id": "eNBHdiWO1q5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Multiplication Rule\n",
        "$$\n",
        "P(A \\cap B) = P(A) \\times P(B)\n",
        "$$"
      ],
      "metadata": {
        "id": "FVKePYUp1962"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Complement Rule\n",
        "\n",
        "Set of all things in U that aren't in A\n",
        "\n",
        "$$\n",
        "A' = U - A\n",
        "$$"
      ],
      "metadata": {
        "id": "68f3oKdhwu6i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conditional Probabilites\n",
        "\n",
        "> Conditional probabilities tell us the probability of Event 1, given that Event 2 has already happened.\n",
        "\n",
        "$$\n",
        "P(B | A) = \\frac{P( B \\cap A)}{P(A)}\n",
        "$$\n",
        "\n",
        "$A$ happens before $B$"
      ],
      "metadata": {
        "id": "iwKV13Kw5h8_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bayes' Theorem\n",
        "\n",
        "$$\n",
        "P(B|A) = \\frac{P(A|B)P(B)}{P(A)}\n",
        "$$\n",
        "\n",
        "It can also be concluded that:\n",
        "\n",
        "$$P(A \\cap B) = P(A|B)P(B)$$\n",
        "\n",
        "Bayesian statistics is all about UPDATING your beliefs based on new information"
      ],
      "metadata": {
        "id": "GiDQ16Ui_M7N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bayesian Hypothesis Testing\n",
        "\n",
        "$$\n",
        "P(H_1|X) = P(H_1) \\frac{P(X|H_1)}{P(X)}\n",
        "$$\n",
        "\n",
        "$P(H_1|X)$: Posterior\n",
        "\n",
        "$P(H_1)$: Prior\n",
        "\n",
        "$P(X|H_1)$: Likelihood\n",
        "\n",
        "$P(x)$: Normalization\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hYmgASo2zi-U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Bayes Factor\n",
        "\n",
        "$$\n",
        "\\frac{P(H_1|X)}{P(H_2|X)} = \\frac{P(H_1)}{P(H_2)} \\times \\frac{P(X|H_1)}{P(X|H_2)}\n",
        "$$\n",
        "\n",
        "$\\frac{P(H_1|X)}{P(H_2|X)}$: Posterior Odds\n",
        "\n",
        "$\\frac{P(H_1)}{P(H_2)}$: Prior Odds\n",
        "\n",
        "$\\frac{P(X|H_1)}{P(X|H_2)}$: Bayes' Factor\n"
      ],
      "metadata": {
        "id": "hZN_do7T0Wnn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Permutations\n",
        "\n",
        "Ways to arrange thing.\n",
        "\n",
        "$$\n",
        "P(n,r) = _nP_r = \\frac{n!}{(n -r)!}\n",
        "$$\n",
        "\n",
        "$n$: Total number of things\n",
        "\n",
        "$r$: Total number of possible arrangments\n",
        "\n",
        "\n",
        "Example: All possible 3 letter words (does not need to have a meaning)\n",
        "\n",
        "$\n",
        "P(26,3) = \\frac{26!}{(26-3)!} = \\frac{26!}{23!} = 26 \\times 25 \\times 24\n",
        "$"
      ],
      "metadata": {
        "id": "j5kfhAusoyPw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Combination"
      ],
      "metadata": {
        "id": "_nEw-yi9m1RD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Binomial Coefficient\n",
        "> The Binomial Coefficient formula makes it easy for us to find out how many ways a certain ratio of successes to failure, can occur. Binomial coefficients tell us how many ways there are to choose k things out of larger set\n",
        "\n",
        "$$\n",
        "_nC_k \\quad \\text{  or  } \\quad \\binom{n}{k}\n",
        "$$\n",
        "Reads $n$ choose $k$.\n",
        "$$\n",
        "\\binom{n}{k} = \\frac{n!}{k!(n-k)!}\n",
        "$$\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you have 5 elements {a, b, c, d, f}. To find out how many different subsets of 2 elements it has, look at the binomial coefficient $_5C_2$\n",
        "$$\n",
        "\\binom{5}{2} = \\frac{5!}{2!(5-2)!}\n",
        "$$"
      ],
      "metadata": {
        "id": "qMHYTPUclbej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Binomial Variables\n",
        "\n",
        "- Made up of independent\n",
        "- Each trial can be classified as either success or failure\n",
        "- Fixed number of trials\n",
        "- Probability of success on each trial is constant\n",
        "\n",
        "\n",
        "Examples: Number of heads after 10 flip of a coin"
      ],
      "metadata": {
        "id": "cGuvRlZYkUsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cumulative Beometric Probability\n",
        "\n",
        "Number of independent trials to get \"success\", where P(success) for eac trial is P\n",
        "\n",
        "$$\n",
        "P(1) + P(2) + \\cdots + P(c) \\\\\n",
        "P(n > k) = P(n \\not\\le k) \\\\\n",
        "$$\n",
        "\n",
        "\\\n",
        "\n",
        "Example: Changes of the event A happening in a series of events is 10%. What's the probability that A happens in the first 4 events.\n",
        "\n",
        "$\n",
        "\\text{A in first 5} \\\\\n",
        "P(c<5) = P(c=1) + P(c=2) + P(c=3) + P(c=4) \\\\\n",
        "\\quad\\quad\\quad = 0.1 + (0.9) 0.1 + (0.9)^2 0.1 + (0.9)^3 0.1 \\\\\n",
        "\\text{A not in first 4} \\\\\n",
        "P = 1 - (0.9)^4  = 0.3439\n",
        "$"
      ],
      "metadata": {
        "id": "KZWR5OFbBUY7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Poisson Process\n",
        "\n",
        "The probability mass function of the Poisson distribution for the number of events $ X $ occurring in a fixed interval of time $ t $ is given by:\n",
        "\n",
        "\n",
        "$$\n",
        "P(X = k) = \\frac{(\\lambda t)^k e^{-\\lambda t}}{k!}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $ k $ is the number of events that occur in the given time interval.\n",
        "- $ \\lambda $ is the average rate of events per unit time.\n",
        "- $ e $ is the base of the natural logarithm (approximately equal to 2.71828).\n",
        "\n",
        "- $t$ can be removed if equation is not time constrianed\n",
        "\n",
        "\n",
        "Poisson Process is a simplified binomial probability.\n",
        "\n",
        "\\\n",
        "\n",
        "In summary, the Poisson process provides a useful framework for modeling random events occurring at a constant rate, assuming independence and memorylessness of these events.\n",
        "\n",
        "\\\n",
        "\n",
        "Example:\n",
        "\n",
        "Suppose phone calls at a call center arrive at an average rate of 5 calls per minute ($ \\lambda = 5 $ calls/minute). The probability of receiving exactly 3 calls in the next minute is calculated using the Poisson distribution formula with $ k = 3 $ and $ \\lambda = 5 $.\n",
        "\n",
        "$\n",
        " P(X = 3) = \\frac{(5 \\times 1)^3 e^{-5 \\times 1}}{3!} \\approx 0.140\n",
        "$\n",
        "\n",
        "This means there is approximately a 14% chance of receiving exactly 3 calls in the next minute in this scenario.\n",
        "\n"
      ],
      "metadata": {
        "id": "YScKQNEnUrW_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inference\n",
        "\n",
        "> Statistical inference is the process of using data analysis to infer properties of an underlying distribution of probability."
      ],
      "metadata": {
        "id": "3AxJ-s56Qv0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Significance Test"
      ],
      "metadata": {
        "id": "HWdjGr73vT4E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Null Hypothesis Significance Testing (NHST)\n",
        "> Null Hypothesis Significance testing is a form of the reductio ad absurdum argument which tries to discredit an idea by assuming the idea is true, and then showing that if you make that assumption, something contradictory happens.\n",
        "\n",
        "In general tests of significance are base on hypothetical probabilities calculated from their null hypothese. They do not generally lead to any probability  statements about the real world, but to a rational and well-defined measure of reluctance to the acceptance of the hypotheses they test.\n",
        "\n",
        "**\"Failing to reject\"** the null hypothesis doesn't mean that there isn't an effect or relationship, it just means we didn't get enough evidence to say there definitely is one."
      ],
      "metadata": {
        "id": "jr4zuaIZRssn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### P-Value\n",
        "\n",
        "> A p-value answers the question of how “rare” your data is by telling you the probability of getting data that’s as extreme as the data you observed if the null hypothesis was true. If your p-value was 0.10 you could say that your sample is in the top 10% most extreme samples we’d expect to see based on the distribution of sample means.\n",
        "\n",
        "$$\n",
        "P ( \\text{data} | \\text{null})\n",
        "$$\n",
        "\n",
        "Probability of the data assuming that the \"null hypothesis\" is true.\n",
        "\n",
        "Arguments have been made that we can have different P-value cutoffs - the alphas - depending on the situation, and that scientists should be allowed to justify their reasosn for picking a certain cutoff.\n",
        "\n",
        "- p-value $< \\alpha \\Rightarrow$ reject null hypothesis ($H_0$)\n",
        "- p-value $\\ge \\alpha \\Rightarrow$ fail to reject null hypothesis ($H_0$)"
      ],
      "metadata": {
        "id": "1Ape6Y7UTPwa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Significance Test Errors\n",
        "\n",
        "Error Probabilities Types:\n",
        "\n",
        "|result|$H_0$ true |  $H_0$ false |\n",
        "|---|--- |--- |\n",
        "|reject $H_0$ |Type I error | correct conclusion  |\n",
        "|failed to reject $H_0$ | correct conclusion | Type II error  |\n",
        "\n",
        "\\\n",
        "\n",
        "A Type I error is when we reject a true null hypothesis. Lower values of $\\alpha$ make it harder to reject the null hypothesis, so choosing lower values for $\\alpha$ can reduce the probability of a Type I error. The consequence here is that if the null hypothesis is false, it may be more difficult to reject using a low value for $\\alpha$. So using lower values of $\\alpha$ can increase the probability of a Type II error.\n",
        "\n",
        "\\\n",
        "\n",
        "A Type II error is when we fail to reject a false null hypothesis. Higher values of $\\alpha$ make it easier to reject the null hypothesis, so choosing higher values for $\\alpha$ can reduce the probability of a Type II error. The consequence here is that if the null hypothesis is true, increasing $\\alpha$ makes it more likely that we commit a Type I error (rejecting a true null hypothesis)."
      ],
      "metadata": {
        "id": "46DKNQU5ixzr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Power\n",
        "\n",
        "Probability of not making a type II error\n",
        "\n",
        "$$\n",
        "\\text{Power} = 1- P(\\text{not rejecting } H_0 | H_0 \\text{ false} ) \\\\\n",
        "\\text{Power} = P(\\text{not making type II error} ) \\\\\n",
        "$$\n",
        "\n",
        "To increase power\n",
        "- increase alpha\n",
        "- increase sample size\n",
        "- less variability\n",
        "- true parameter far from $H_0$"
      ],
      "metadata": {
        "id": "_JbEfKUipz5Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bias"
      ],
      "metadata": {
        "id": "C3uKTXKggmmN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Allocation Bias\n",
        "\n",
        "> Allocation bias occurs when there is a systematic difference in how participants are assigned to groups in a trial. The trial is unfair because the groups are not fairly balanced from the outset."
      ],
      "metadata": {
        "id": "UcixJu9mgo3D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selection Bias\n",
        "\n",
        "> Selection bias is a distortion in a measure of association (such as a risk ratio) due to a sample selection that does not accurately reflect the target population."
      ],
      "metadata": {
        "id": "MT5GYoF9gs-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "\n",
        "# Resources\n",
        "* [Crash Course Statistics](https://youtube.com/playlist?list=PL8dPuuaLjXtNM_Y-bUAhblSAdWRnmBUcr)\n",
        "* [Math is fun](https://www.mathsisfun.com)\n",
        "* [LaTex Generator](https://latex.codecogs.com/eqneditor/editor.php)"
      ],
      "metadata": {
        "id": "hO6aVIMt58u7"
      }
    }
  ]
}